

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/tuzi.png">
  <link rel="icon" href="/img/tuzi.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Alexander Liu">
  <meta name="keywords" content="分布式系统,后端研发,数据协同">
  
    <meta name="description" content="学点新东西，拖了很久的Recommended System，启动！">
<meta property="og:type" content="article">
<meta property="og:title" content="Recommendation System">
<meta property="og:url" content="https://alexanderliu-creator.github.io/2024/08/16/recommendation-system/index.html">
<meta property="og:site_name" content="兔の博客">
<meta property="og:description" content="学点新东西，拖了很久的Recommended System，启动！">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409041705783.png">
<meta property="article:published_time" content="2024-08-16T08:05:18.000Z">
<meta property="article:modified_time" content="2024-12-06T09:18:16.974Z">
<meta property="article:author" content="Alexander Liu">
<meta property="article:tag" content="自学">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409041705783.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>Recommendation System - 兔の博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"alexanderliu-creator.github.io","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":1},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="兔の博客" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>兔的博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/background_post.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Recommendation System"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Alexander Liu
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-08-16 16:05" pubdate>
          2024年8月16日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          65k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          543 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Recommendation System</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：2 个月前
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <p>学点新东西，拖了很久的Recommended System，启动！</p>
<span id="more"></span>

<h1 id="Shuseng-Wang的公开课"><a href="#Shuseng-Wang的公开课" class="headerlink" title="Shuseng Wang的公开课"></a>Shuseng Wang的公开课</h1><h2 id="Basic-Recommendation-System-Knowledge"><a href="#Basic-Recommendation-System-Knowledge" class="headerlink" title="Basic Recommendation System Knowledge"></a>Basic Recommendation System Knowledge</h2><h3 id="Conversion-process-indicators-and-experiment-process"><a href="#Conversion-process-indicators-and-experiment-process" class="headerlink" title="Conversion process, indicators and experiment process"></a>Conversion process, indicators and experiment process</h3><ul>
<li><p>用户在小红书中，推荐算法帮助用户进行的转化流程</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408261731997.png" srcset="/img/loading.gif" lazyload alt="image-20240826173111801"></p>
<ul>
<li>不同系统不一样，淘宝/Youtube前两步都是Impression等，但是抖音就不一样。</li>
</ul>
</li>
<li><p>重要指标：</p>
<ul>
<li><p>短期消费指标</p>
<ul>
<li>点击率 = 点击次数/曝光次数</li>
<li>点赞率 = 点赞次数/点击次数</li>
<li>收藏率 = 收藏次数/点击次数</li>
<li>转发率 = 转发次数/点击次数</li>
<li>阅读完成率 = 滑动到底次数/点击次数×f(笔记长度)</li>
</ul>
</li>
<li><p>长期消费指标</p>
<ul>
<li>多样性等 -&gt; 用户粘性</li>
</ul>
</li>
<li><p>最重要的指标—北极星指标</p>
<ul>
<li>用户规模：日活用户数(DAU)、月活用户数(MAU)。</li>
<li>消费：人均使用推荐的时长、人均阅读笔记的数量。</li>
<li>发布：发布渗透率、人均发布量。</li>
</ul>
<blockquote>
<p>北极星指标比其他指标更重要！当发生冲突时，以北极星指标为准。北极星指标 都是线上指标，只能上线了才能获得。</p>
</blockquote>
</li>
</ul>
</li>
<li><p>实验流程：</p>
<ul>
<li>离线实验：收集历史数据，在历史数据上做训练、测试。算法没有部署到产品中，没有跟用户交互。</li>
<li>小流量AB测试：把算法部署到实际产品中，用户实际跟算法做交互。</li>
<li>全流量上线：小流量AB后，效果好，逐渐放量推全。</li>
</ul>
</li>
</ul>
<h3 id="Recommendation-system-workflow"><a href="#Recommendation-system-workflow" class="headerlink" title="Recommendation system workflow"></a>Recommendation system workflow</h3><ul>
<li><p>链路概览：</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408261742594.png" srcset="/img/loading.gif" lazyload alt="image-20240826174256556"></p>
<ul>
<li>召回：从数据库中，通过多条召回通道进行召回，每条召回通道取回几十到几百篇笔记，总共召回几千篇笔记。</li>
<li>粗排：用规模比较小的机器学习模型给几千篇笔记逐一打分，按照分数和排序做截断，保留分数最高的几百篇笔记。</li>
<li>精排：用大规模的深度神经网络给几百篇笔记逐一打分，分数反映出用户对笔记的兴趣。可以截断，也可以不截断（小红书就是不做截断）。带着精排的分数，进入重排。</li>
<li>重排：根据精排分数和多样性分数做随机抽样，得到几十篇内容。然后把相似内容打散，插入广告和运营内容。</li>
</ul>
</li>
<li><p>召回：</p>
<ul>
<li><p>召回通道：协同过滤、双塔模型、关注的作者、等等</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408261751566.png" srcset="/img/loading.gif" lazyload alt="image-20240826175126527"></p>
</li>
<li><p>小红书有几十个召回通道，每个通道返回几十上百篇笔记。将所有召回通道的内容融合后，会去重，并过滤（例如去掉用户不喜欢的作者的笔记，不喜欢的话题）</p>
</li>
</ul>
</li>
<li><p>粗排/精排：</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408262118372.png" srcset="/img/loading.gif" lazyload alt="image-20240826211819332"></p>
<ul>
<li><p>[几千] → 粗排 → [几百] → 精排 → [几百]</p>
</li>
<li><p>粗排模型小，速度快；精排用的模型大，计算量大，打分更可靠。</p>
</li>
<li><p>用粗排做筛选，再用精排 — 平衡计算量和准确性。</p>
</li>
<li><p>每篇笔记都有一个分数，表示用户对于笔记的兴趣有多高。（可以用于排序）</p>
</li>
<li><p>排序本质上就是输入一批特征，得到一批结果，融合得到一个最终的打分：</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408262127991.png" srcset="/img/loading.gif" lazyload alt="image-20240826212722962"></p>
</li>
<li><p>这一步骤的结果其实已经可以较好展示给用户了，但是此时结果还是不足，还有一些调整。</p>
</li>
</ul>
</li>
<li><p>重排（主要是考虑多样性）：做多样性抽样（比如 MMR、DPP），从几百篇中选出几十篇，抽样依据：精排分数、多样性。</p>
</li>
<li><p>用规则打散相似笔记，不能把内容过于相似的笔记，排在相邻的位置上。减少一个页面中的同质化内容：插入广告、运营推广内容，根据生态要求调整排序（例如：不能同时出很多美女图片）</p>
</li>
<li><p>总结</p>
<ul>
<li>召回：用多条通道，取回几千篇笔记</li>
<li>粗排：用小规模神经网络，给几千篇笔记打分，选出分数最高的几百篇</li>
<li>精排：用大规模神经网络，给几百篇笔记打分</li>
<li>重排：做多样性抽样、规则打散、插入广告和运营笔记</li>
</ul>
<blockquote>
<p>召回和粗排是个大漏斗，这俩过滤掉的页面是最多的，从所有数据 -&gt; 几千 -&gt; 几百，这直接是对数量级的降维打击。</p>
</blockquote>
</li>
</ul>
<h3 id="A-x2F-B-Test"><a href="#A-x2F-B-Test" class="headerlink" title="A/B Test"></a>A/B Test</h3><ul>
<li><p>A/B 测试举例</p>
<ul>
<li>召回团队实现了一种 GNN 召回通道，离线实验结果正向。（离线不等于上线哈）</li>
<li>线上的小流量 A/B 测试，不影响大多数用户的体验，考察新的召回通道对线上指标的影响。</li>
<li>模型中有一些参数，比如 GNN 的深度取值，需要用 A/B 测试选取最优参数。（可以同时开多组实验，查看不同参数的效果）</li>
</ul>
</li>
<li><p>随机分桶（把用户分成小份儿，方便A/B test）</p>
<ul>
<li><p>用户数量足够大的情况下，各个桶中用户的表现数据都是一样的哈（在所有的桶的环境都一样的条件下。）。</p>
</li>
<li><p>Case:</p>
<ul>
<li><p>分b=10个桶，每个桶中有10%的用户。</p>
</li>
<li><p>首先用哈希函数把用户D映射成某个区间内的整数，然后把这些整数均匀随机分成b个桶。</p>
</li>
<li><p>改变部分桶的策略，进行实验对照：</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408262139999.png" srcset="/img/loading.gif" lazyload alt="image-20240826213934890"></p>
</li>
<li><p>计算每个桶的业务指标，比如DAU、人均使用推荐的时长、点击率、等等。</p>
</li>
<li><p>如果某个实验组指标显著优于对照组，则说明对应的策略有效，值得推全。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>分层实验</p>
<ul>
<li>流量不够用怎么办？</li>
<li>信息流产品的公司有很多部门和团队，大家都需要做A/B测试。推荐系统（召回、粗排、精排、重排)，用户界面，广告等。根本就不够用呀。如果把用户随机分成10组，1组做对照，9组做实验，那么只能同时做9组实验。</li>
<li>分层实验<ul>
<li>分层实验：召回、粗排、精排、重排、用户界面、广告..（例如GNN召回通道属于召回层)</li>
<li>同层互斥：GNN实验占了召回层的4个桶，其他召回实验只能用剩余的6个桶</li>
<li>不同层正交：每一层<strong>独立随机</strong>对用户做分桶。每一层都可以独立用100%的用户做实验</li>
</ul>
</li>
<li>正交是啥？假设上层10%的用户在测试，下层也有10%的用户在测试。由于每一层都是独立随机打散的，意味着受到上层10%影响的用户，在下层也会被随机均匀打散到10个桶中，意味着1/10 * 1/10，只有1%的用户，会同时收到上层+下层的影响，通常来说用户界面实验和召回实验的效果不容易相互增强或者相互抵消，所以这种多层影响是allow的！</li>
<li>为什么不能所有都正交，同层也正交呢？<ul>
<li>如果所有实验都正交，则可以同时做无数组实验。</li>
<li>同类的策略（例如精排模型的两种结构）天然互斥，对于一个用户，只能用其中一种。</li>
<li>同类的策略（例如添加两条召回通道）效果会相互增强(1+1&gt;2)或相互抵消（1+1&lt;2)。互斥可以避免同类策略相互干扰。</li>
<li>不同类型的策略（例如添加召回通道、优化粗排模型)，通常不会相互干扰（1+1=2)，可以作为正交的两层。</li>
</ul>
</li>
</ul>
</li>
<li><p>Holdout机制（查收推荐部门，整体性能提升的时候用的，不能简单叠加各层收益昂）</p>
<ul>
<li><p>每个实验（召回、粗排、精排、重排)独立汇报对业务指标的提升。但是公司考察一个部门（比如推荐系统）在一段时间内对业务指标总体的提升。因此需要Holdout机制来考察。</p>
</li>
<li><p>取 10% 的用户作为holdout桶，推荐系统使用剩余 90% 的用户做实验，两者互斥。10% holdout 桶 vs 90% 实验桶的diff(需要归一化)为整个部门的业务指标收益。</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408262311641.png" srcset="/img/loading.gif" lazyload alt="image-20240826231108596"></p>
</li>
<li><p>每个考核周期结束之后，清除holdout桶，让推全实验从 90% 用户扩大到 100% 用户。</p>
</li>
<li><p>重新随机划分用户，得到holdout桶和实验桶开始下一轮考核周期。新的holdout桶与实验桶各种业务指标的diff接近0，随着召回、粗排、精排、重排实验上线和推全，diff会逐渐扩大。</p>
</li>
</ul>
</li>
<li><p>推全</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408270024241.png" srcset="/img/loading.gif" lazyload alt="image-20240827002432208"></p>
<ul>
<li>例如在重排中，如果在10%的用户上，可以提高1%Holdout和实验桶的diff。推全后，这个diff就会扩大至九倍。和A/B test得到的结论一致。注意这里是新建了一层，新的这层会与其他层正交昂！！！</li>
</ul>
</li>
<li><p>反转实验</p>
<ul>
<li><p>有的指标（，点击、交互)立刻收到新策略影响，有的指标（留存）有滞后性，需要长期观测。</p>
</li>
<li><p>实验观测到显著收益后尽快推全新策略。目的是腾出桶供其他实验使用，或需要基于新策略做后续的开发。</p>
</li>
<li><p>用反转实验解决上述矛盾，既可以尽快推全，也可以长期观测实验指标在推全的新层中开一个旧策略的桶，长期观测实验指标。</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408270018324.png" srcset="/img/loading.gif" lazyload alt="image-20240827001839280"></p>
</li>
<li><p>在推全的过程中，留一部分用户保持原样用于长期对比。可以在类似于清除holdout桶的时候，完成反转桶的清理。</p>
</li>
</ul>
</li>
<li><p>总结</p>
<ul>
<li>分层实验：同层互斥，不同层正交。把容易相互增强（或抵消）的实验放在同一层，让它们的用户互斥</li>
<li>Holdout: 保留10%的用户，完全不受实验影响，可以考察整个部门对业务指标的贡献。</li>
<li>实验推全：新建一个推全层，与其他层正交</li>
<li>反转实验：在新的推全层上，保留一个小的反转桶，使用引旧策略。长期观测新旧策略的dff</li>
</ul>
</li>
</ul>
<h2 id="Recall"><a href="#Recall" class="headerlink" title="Recall"></a>Recall</h2><h3 id="ItemCF"><a href="#ItemCF" class="headerlink" title="ItemCF"></a>ItemCF</h3><h4 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h4><ul>
<li>思想：<ul>
<li>I love A.</li>
<li>A is similar with B(I haven’t watched B before.)</li>
<li>I love B.</li>
</ul>
</li>
<li>一个case：</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271114384.png" srcset="/img/loading.gif" lazyload alt="image-20240827111407332"></p>
<blockquote>
<p>在上面这幅图中，就是每条路径上的like和similarity相乘，再把不同路径的值都想加。</p>
</blockquote>
<ul>
<li><p>物品相似度如何判断？</p>
<ul>
<li><p>两个物品的受众重合度越高，两个物品越相以。例如：喜欢《射雕英雄传》和《神雕侠侣》的读者重合度很高，可以认为《射雕英雄传》和《神雕侠侣》相似。本质上就是沟通Item -&gt; User的倒排关系，然后根据不同的Item的User集合的关系，判断“受众重合度”。</p>
</li>
<li><p>计算相似度公式：</p>
<ul>
<li><p>喜欢$i_1$的用户集合：$$\mathcal{W}_1$$</p>
</li>
<li><p>喜欢$i_2$的用户集合：$$\mathcal{W}_2$$</p>
</li>
<li><p>交集：$$\mathcal{V}=\mathcal{W}_1\cap\mathcal{W}_2$$</p>
</li>
<li><p>不考虑物品的喜欢程度的情况下，喜欢就是1，不喜欢就是0，公式为：$$sim(i_1,i_2)=\frac{|\mathcal{V}|}{\sqrt{|\mathcal{W}_1|·|\mathcal{W}_2|}}$$</p>
</li>
<li><p>如果考虑喜欢程度的话，我们在意的“受众重合度”，因此公式里面，用于计算的也是交集中的受众：$$sim(i_1, i_2) = \frac{\sum_{v\in{\mathcal{V}}}like(v, i_1)\ .\ like(v, i_2)}{\sqrt{\sum_{u_1\in\mathcal{W_1}}like^2(u_1, i_1)}\sqrt{\sum_{u_2\in\mathcal{W_2}}like^2(u_2, i_2)}}$$</p>
<blockquote>
<p>本质就是余弦相似度hhhh，得到一个0-1之间的分数。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p>ItemCF的基本思想：</p>
<ul>
<li>如果用户喜欢物品item1,而且物品item1与item2相似，那么用户很可能喜欢物品item2。</li>
<li>从用户历史行为记录中，我们知道用户对itemj的兴趣，还知道itemj与候选物品的相似度。</li>
<li>每个物品表示为一个稀疏向量，向量每个元素对应一个用户，相似度 sim 就是两个向量夹角的余弦。</li>
</ul>
</li>
</ul>
<h4 id="Recall-Process"><a href="#Recall-Process" class="headerlink" title="Recall Process"></a>Recall Process</h4><ul>
<li><p>建立 “用户→物品” 的索引</p>
<ul>
<li><p>记录每个用户最近点击、交互过的物品D</p>
</li>
<li><p>给定任意用户D,可以找到他近期感兴趣的物品列表</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271422077.png" srcset="/img/loading.gif" lazyload alt="image-20240827142229041"></p>
</li>
</ul>
</li>
<li><p>建立 “物品→物品” 的索引（计算量较大）</p>
<ul>
<li><p>计算物品之间两两相似度</p>
</li>
<li><p>对于每个物品，索引它最相似的k个物品</p>
</li>
<li><p>给定任意物品D,可以快速找到它最相似的k个物品</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271422848.png" srcset="/img/loading.gif" lazyload alt="image-20240827142248800"></p>
</li>
</ul>
</li>
<li><p>线上召回</p>
<ul>
<li>给定用户D，通过“用户→物品”索引，找到用户近期感兴趣的物品列表(last-n)</li>
<li>对于last-n列表中每个物品，通过“物品→物品”的索引，找到top-k相似物品</li>
<li>对于取回的相似物品（最多有k个)，用公式预估用户对物品的兴趣分数</li>
<li>返回分数最高的100个物品，作为推荐结果</li>
</ul>
</li>
<li><p>索引的意义在于<strong>避免枚举所有的物品</strong>。<strong>用索引，离线计算量大，线上计算量小</strong>。</p>
<ul>
<li>记录用户最近感兴趣的m=200个物品</li>
<li>取回每个物品最相似的k=10个物品</li>
<li>给取回的k=2000个物品打分（用户对物品的兴趣）</li>
<li>返回分数最高的100个物品作为ItemCF通道的输出</li>
</ul>
</li>
<li><p>Top-k相似：</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271426482.png" srcset="/img/loading.gif" lazyload alt="image-20240827142556072"></p>
<ul>
<li>取回的 item 中有重复的，就去重，并把分数加起来</li>
</ul>
</li>
</ul>
<h3 id="Swing"><a href="#Swing" class="headerlink" title="Swing"></a>Swing</h3><blockquote>
<p>Swing是ItemCF的一个变体，在工业界很常用。Swing和ItemCF很像，唯一区别在于如何定义相似度。</p>
</blockquote>
<ul>
<li><p>ItemCF的不足之处：如果重合的用户是一个小圈子</p>
<ul>
<li><p>两篇笔记被碰巧分享到了一个微信群里面，造成问题：两篇笔记的受众完全不同，但很多用户同时交互过这两篇笔记，导致系统错误判断两篇笔记相似度很高。</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271429361.png" srcset="/img/loading.gif" lazyload alt="image-20240827142959328"></p>
<blockquote>
<p>受众完全不一样，但是因为小圈子，导致很多相同的用户交互过这两篇完全受众不同的笔记。导致笔记的相似度被系统错判。认为两篇笔记完全相同。</p>
</blockquote>
</li>
<li><p>解决该问题，就要降低小圈子用户的权重。少量相关用户交互不同的物品，说服力较低。但大量不相关的用户同时交互两个物品，则说明两个物品的受众真的相同。</p>
</li>
</ul>
</li>
<li><p>建模：</p>
<ul>
<li>用户 $$u_1$$ 喜欢的物品记作集合 $$\mathcal{J}_1$$</li>
<li>用户 $$u_2$$ 喜欢的物品记作集合 $$\mathcal{J}_2$$</li>
<li>两个用户的重合度：$$overlap(u_1,u_2)=|\mathcal{J}_1\cap\mathcal{J}_2|$$</li>
<li>$$u_1 和 u_2 $$的重合度高，则他们可能来自一个小圈子，要降低他们的权重</li>
<li>喜欢物品 $$i_1$$ 的用户记作集合 $$\mathcal{W}_1$$</li>
<li>喜欢物品 $$i_2$$ 的用户记作集合 $$\mathcal{W}_2$$</li>
<li>定义交集 $$\mathcal{V}=\mathcal{W}_1\cap\mathcal{W}_2$$</li>
<li>两个物品的相似度：$sim(i_1,i_2)=\sum_{u_1\in \mathcal{V}}\sum_{u_2\in \mathcal{V}}\frac{1}{α+overlap(u_1,u_2)}$<ul>
<li>$α$ 是超参数</li>
<li>$overlap(u_1,u_2)$ 表示两个用户的重合度<ul>
<li>重合度高，说明两人是一个小圈子的，那么他两对物品相似度的贡献就比较小</li>
<li>重合度小，两人不是一个小圈子的，他两对物品相似度的贡献就比较大</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Summary</p>
<ul>
<li>Swing 与 ItemCF 唯一的区别在于物品相似度</li>
<li>ItemCF：两个物品重合的用户比例高，则判定两个物品相似</li>
<li>Swing：额外考虑重合的用户是否来自一个小圈子<ul>
<li>同时喜欢两个物品的用户记作集合 $\mathcal{V}$</li>
<li>对于 $\mathcal{V}$ 中的用户 $u_1$ 和 $u_2$，重合度记作 $overlap(u_1,u_2)$</li>
<li>两个用户重合度大，则可能来自一个小圈子，权重降低</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="UserCF"><a href="#UserCF" class="headerlink" title="UserCF"></a>UserCF</h3><h4 id="Concept-1"><a href="#Concept-1" class="headerlink" title="Concept"></a>Concept</h4><ul>
<li>找和我相似的用户，推荐我没看过的，他们看过的笔记。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271439465.png" srcset="/img/loading.gif" lazyload alt="image-20240827143950420"></p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271441122.png" srcset="/img/loading.gif" lazyload alt="image-20240827144111017"></p>
<ul>
<li><p>预估用户对候选物品的兴趣：$$\sum_jsim(user,user_j)×like(user_j,item)$$</p>
</li>
<li><p>如何计算用户相似度？</p>
<ul>
<li><p>用户 $u_1$ 喜欢的物品记作集合 $\mathcal{J}_1$</p>
</li>
<li><p>用户 $u_2$ 喜欢的物品记作集合 $\mathcal{J}_2$</p>
</li>
<li><p>定义交集 $I=\mathcal{J}_1\cap\mathcal{J}_2$</p>
</li>
<li><p>两个用户的相似度：$sim(u_1,u_2)=\frac{|I|}{\sqrt{|\mathcal{J}_1|·|\mathcal{J}_2|}}$</p>
</li>
<li><p>之前的公式中，交集意味着，交集中的每一个元素，都是同等的权重（为1）。降低热门物品的权重，热门的大家都喜欢看，没那么相似，如何去掉热门物品呢？</p>
<ul>
<li><p>降低热门物品权重后：$sim(u_1,u_2)=\frac{{\sum_{l\in{I}} \frac{1}{\log{(1+n_l)}}}}{\sqrt{|\mathcal{J}_1|·|\mathcal{J}_2|}}$</p>
</li>
<li><p>分子中的$n_l$代表的是喜欢商品 l 的用户数量，反映物品的热门程度。</p>
</li>
<li><p>物品越热门，$\frac{1}{\log{(1+n_l)}}$ 越小，对相似度的贡献就越小。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Recall-Process-1"><a href="#Recall-Process-1" class="headerlink" title="Recall Process"></a>Recall Process</h4><ul>
<li><p>事先做离线计算</p>
<ul>
<li><p>建立“用户 → 物品”的索引</p>
<ul>
<li><p>记录每个用户最近点击、交互过的物品 ID</p>
</li>
<li><p>给定任意用户 ID，可以找到他近期感兴趣的物品列表</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271452292.png" srcset="/img/loading.gif" lazyload alt="image-20240827145245247"></p>
</li>
</ul>
</li>
<li><p>建立“用户 → 用户”的索引（计算量大）</p>
<ul>
<li><p>对于每个用户，索引他最相似的 k 个用户</p>
</li>
<li><p>给定任意用户 ID，可以快速找到他最相似的 k 个用户</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271453402.png" srcset="/img/loading.gif" lazyload alt="image-20240827145319362"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>线上做召回</p>
<ul>
<li><p>给定用户 ID，通过“用户 → 用户”索引，找到 top-k 相似用户</p>
</li>
<li><p>对于每个 top-k 相似用户，通过“用户 → 物品”索引，找到用户近期感兴趣的物品列表（last-n）</p>
</li>
<li><p>对于取回的 $nk$ 个相似物品，用公式预估用户对每个物品的兴趣分数</p>
</li>
<li><p>返回分数最高的 100 个物品，作为召回结果</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271455610.png" srcset="/img/loading.gif" lazyload alt="image-20240827145534570"></p>
</li>
</ul>
</li>
</ul>
<h3 id="Vector-Recall-Pre-Knowledge"><a href="#Vector-Recall-Pre-Knowledge" class="headerlink" title="Vector Recall Pre Knowledge"></a>Vector Recall Pre Knowledge</h3><h4 id="Discrete-feature-processing"><a href="#Discrete-feature-processing" class="headerlink" title="Discrete feature processing"></a>Discrete feature processing</h4><ul>
<li><p>处理过程</p>
<ul>
<li>建立字典：类别 -&gt; 序号。</li>
<li>向量化：序号映射为向量。<ul>
<li>One-hot编码：把序号映射成高维稀疏向量。</li>
<li>Embedding：把序号映射成低维稠密向量。</li>
</ul>
</li>
</ul>
</li>
<li><p>One-hot：</p>
<ul>
<li><p>男女：男 -&gt; 1，女 -&gt; 2。未知 -&gt; [0, 0]，男 -&gt; [0, 1]，女 -&gt; [1, 0]。</p>
</li>
<li><p>国籍：200维表示。中国 -&gt; 1，美国 -&gt; 2，印度 -&gt; 3。未知 -&gt; 0 -&gt; [0, 0, … , 0]，中国 -&gt; 1 -&gt; [1, 0, … , 0]，美国 -&gt; 2 -&gt; [0, 1, … , 0]，印度 -&gt; 3 -&gt; [0, 0, 1, … , 0]</p>
</li>
<li><p>局限：</p>
<ul>
<li>单词编码，几万太大了。笔记有几亿篇，向量超大。类别数量太大时，通常不用one-hot编码。</li>
</ul>
</li>
</ul>
</li>
<li><p>Embedding：</p>
<ul>
<li><p>国籍：中国 -&gt; 1，美国 -&gt; 2，…。Embedding映射为低维向量，例如4 x 1。</p>
</li>
<li><p>参数数量：向量维度 x 类别数量。例如4 x 1的矩阵，总共200个国家，因此总共的参数数量就是4 x 200 = 800个向量（每个国家一个单独的嘛！）。</p>
</li>
<li><p>输入是美国的序号2，输出是一个vector，对应整个4 x 200的参数矩阵的第二列。可以使用Pytorch之类的库实现哈。</p>
</li>
<li><p>Embedding本质就是：参数矩阵 x One-hot vector，本质就是在一个大参数矩阵中，把你对应的那一列给抽取出来！</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271507256.png" srcset="/img/loading.gif" lazyload alt="image-20240827150759148"></p>
</li>
</ul>
</li>
</ul>
<h4 id="Matrix-Completion"><a href="#Matrix-Completion" class="headerlink" title="Matrix Completion"></a>Matrix Completion</h4><p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271549131.png" srcset="/img/loading.gif" lazyload alt="image-20240827154953078"></p>
<ul>
<li><p>训练（这一块儿与传统Machine Learning并无区别）</p>
<ul>
<li>基本想法<ul>
<li>用户 embedding 参数矩阵记作 $\bold{A}$。第 $u$ 号用户对应矩阵第 $u$ 列，记作向量 $\bold{a}_u$</li>
<li>物品 embedding 参数矩阵记作 $\bold{B}$。第 $i$ 号物品对应矩阵第 $i$ 列，记作向量 $\bold{b}_i$</li>
<li>内积 $\left\langle \bold{a}_u,\bold{b}_i \right\rangle$ 是第 $u$ 号用户对第 $i$ 号物品兴趣的预估值</li>
<li>训练模型的目的是学习矩阵 $\bold{A}$ 和 $\bold{B}$，使得预估值拟合真实观测的兴趣分数</li>
</ul>
</li>
<li>数据集<ul>
<li>数据集：（用户 ID，物品 ID，真实兴趣分数）的集合，记作 $\Omega={(u,i,y) }$</li>
<li>数据集中的兴趣分数是系统记录的，比如：<ul>
<li>曝光但是没有点击 → 0 分</li>
<li>点击、点赞、收藏、转发 → 各算 1 分</li>
<li>分数最低是 0，最高是 4</li>
</ul>
</li>
<li>训练的目的就是让模型的输出拟合真实兴趣分数</li>
</ul>
</li>
<li>训练<ul>
<li>把用户 ID、物品 ID 映射成向量。<ul>
<li>第 $u$ 号用户 → 向量 $\bold{a}_u$</li>
<li>第 $i$ 号物品 → 向量 $\bold{b}_i$</li>
</ul>
</li>
<li>求解优化问题，得到参数 $\bold{A}$ 和 $\bold{B}$$\min_{\bold{A},\bold{B}}\sum_{(u,i,y)\in\Omega}(y- \left\langle \bold{a}_u,\bold{b}_i \right\rangle)^2$<ul>
<li>找到使得 真实兴趣分数 $y$ 与 模型输出 $\left\langle \bold{a}_u,\bold{b}_i \right\rangle$ 间 差别 最小的 $\bold{A}$ 和 $\bold{B}$</li>
<li>求最小化常用的方法就是随机梯度下降，每次更新矩阵 $\bold{A}$ 和 $\bold{B}$ 的一列</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>为什么叫做矩阵补充？</p>
<ul>
<li>矩阵是稀疏矩阵，而拿到的神经网络在训练好了之后，则可以对任意User, Item（矩阵中的任意行、任意列做计算/补全）。</li>
<li>矩阵中只有少数位置是绿色，大多数位置是灰色（即大部分物品没有曝光给用户）而我们用绿色位置训练出的模型，可以预估所有灰色位置的输出，即把矩阵的元素补全。</li>
<li>把矩阵元素补全后，我们只需选出对应用户一行中分数较高的 物品 推荐给 用户 即可。</li>
</ul>
</li>
<li><p>生产中没有使用，效果不好</p>
<ul>
<li>缺点 1：仅用 ID embedding，没利用物品、用户属性<ul>
<li>物品属性：类目、关键词、地理位置、作者信息</li>
<li>用户属性：性别、年龄、地理定位、感兴趣的类目</li>
<li>双塔模型可以看做矩阵补充的升级版，双塔模型不仅使用 ID，还结合各种属性。</li>
</ul>
</li>
<li>缺点 2：负样本的选取方式不对<ul>
<li>样本：用户—物品的二元组，记作 $(u,i)$</li>
<li>正样本：曝光之后，有点击、交互。（正确的做法）</li>
<li>负样本：曝光之后，没有点击、交互。（错误的做法）</li>
</ul>
</li>
<li>缺点 3：做训练的方法不好<ul>
<li>内积 $\left\langle \bold{a}_u,\bold{b}_i \right\rangle$ 不如余弦相似度。工业界普遍使用余弦相似度而不是内积。</li>
<li>用平方损失（回归），不如用交叉熵损失（分类）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Online-Service"><a href="#Online-Service" class="headerlink" title="Online Service"></a>Online Service</h4><ul>
<li>模型存储<ul>
<li>训练得到矩阵 $\bold{A}$ 和 $\bold{B}$<ul>
<li>$\bold{A}$ 的每一列对应一个用户</li>
<li>$\bold{B}$ 的每一列对应一个物品</li>
</ul>
</li>
<li>把矩阵 $\bold{A}$ 的列存储到 key-value 表<ul>
<li>key 是用户 ID，value 是 $\bold{A}$ 的一列</li>
<li>给定用户 ID，返回一个向量（用户的 embedding）</li>
</ul>
</li>
<li>矩阵 $\bold{B}$ 的存储和索引比较复杂</li>
</ul>
</li>
<li>线上服务<ul>
<li>把用户 ID 作为 key，查询 key-value 表，得到该用户的 embedding 向量，记作 $\bold{a}$</li>
<li>最近邻查找：查找用户最有可能感兴趣的 $k$ 个物品，作为召回结果<ul>
<li>第 𝑖 号物品的 embedding 向量记作 $\bold{b}_i$</li>
<li>内积 $\left\langle \bold{a}_u,\bold{b}_i \right\rangle$ 是用户对第 𝑖 号物品兴趣的预估</li>
<li>返回内积最大的 $k$ 个物品</li>
</ul>
</li>
<li>如果枚举所有物品，时间复杂度正比于物品数量。</li>
<li>最近邻查找的计算量太大，不现实，下面讲解如何加速最近邻查找。</li>
</ul>
</li>
</ul>
<h4 id="Approximate-nearest-neighbor-search"><a href="#Approximate-nearest-neighbor-search" class="headerlink" title="Approximate nearest neighbor search"></a>Approximate nearest neighbor search</h4><ul>
<li><p>支持最近邻查找的系统</p>
<ul>
<li>系统：Milvus、Faiss、HnswLib、等等，快速最近邻查找的算法已经被集成到这些系统中。</li>
<li>衡量最近邻的标准：<ul>
<li>欧式距离最小（L2 距离）</li>
<li>向量内积最大（内积相似度），矩阵补充用的就是内积相似度</li>
<li>向量夹角余弦最大（cosine 相似度），最常用</li>
<li>对于不支持的系统：把所有向量作归一化（让它们的二范数等于 1），此时内积就等于余弦相似度</li>
</ul>
</li>
</ul>
</li>
<li><p>数据预处理：把数据据划分为多个区域</p>
<ul>
<li>划分后，每个区域用一个向量表示，这些向量的长度都是 1</li>
<li>例如图中蓝色区域用蓝色箭头向量表示</li>
</ul>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271544724.png" srcset="/img/loading.gif" lazyload alt="image-20240827154447667"></p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271545065.png" srcset="/img/loading.gif" lazyload alt="image-20240827154525903"></p>
<blockquote>
<p>朴素的想法哈哈哈哈，空间换时间，先定区，再去区内查找，就少了很多检索花费的时间啦！比暴力枚举比较好太多了！</p>
</blockquote>
<h3 id="Twin-tower-model"><a href="#Twin-tower-model" class="headerlink" title="Twin tower model"></a>Twin tower model</h3><h4 id="Concept-2"><a href="#Concept-2" class="headerlink" title="Concept"></a>Concept</h4><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271554903.png" srcset="/img/loading.gif" lazyload alt="image-20240827155400855" style="zoom:50%;">

<ul>
<li>用户离散特征：例如所在城市、感兴趣的话题等。<ul>
<li>对每个离散特征，单独使用一个 Embedding 层得到一个向量。</li>
<li>对于性别这种类别很少的离散特征，直接用 one-hot 编码。</li>
<li>用户连续特征：年龄、活跃程度、消费金额等，也有对应的处理方法。</li>
</ul>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271552188.png" srcset="/img/loading.gif" lazyload alt="image-20240827155217140" style="zoom:50%;">

<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271552110.png" srcset="/img/loading.gif" lazyload alt="image-20240827155247008" style="zoom:50%;">

<ul>
<li><p>双塔模型：左塔提取用户特征，右塔提取物品特征。与矩阵补充的区别在于，使用了除 ID 外的多种特征作为双塔的输入。</p>
</li>
<li><p>双塔模型的训练</p>
<ul>
<li>Pointwise：独立看待每个正样本、负样本，做简单的二元分类</li>
<li>Pairwise：每次取一个正样本、一个负样本。</li>
<li>Listwise：每次取一个正样本、多个负样本。</li>
<li>正负样本的选择<ul>
<li>正样本：用户点击的物品</li>
<li>负样本可能有很多指标和标准：<ul>
<li>没有被召回的？</li>
<li>召回但是被粗排、精排淘汰的？</li>
<li>曝光但是未点击的？</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Pointwise</p>
<ul>
<li>把召回看做二元分类任务</li>
<li>对于正样本，鼓励 $\cos{(\bold{a},\bold{b})}$ 接近 +1</li>
<li>对于负样本，鼓励 $\cos{(\bold{a},\bold{b})}$ 接近 −1</li>
<li>控制正负样本数量为 1: 2 或者 1: 3（经验hhhh）</li>
</ul>
</li>
<li><p>Pairwise</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271602798.png" srcset="/img/loading.gif" lazyload alt="image-20240827160208748"></p>
<blockquote>
<p>两个物品塔是相同的，它们共享参数。</p>
</blockquote>
<ul>
<li><p>基本想法：鼓励 $\cos{(\bold{a},\bold{b}^+)}$ 大于 $\cos{(\bold{a},\bold{b}^-)}$，两者只差越大越好！！！</p>
<ul>
<li>如果  $\cos{(\bold{a},\bold{b}^+)}$ 大于  $\cos{(\bold{a},\bold{b}^-)}+m$，则没有损失。 $m$ 是超参数，需要调</li>
<li>否则，损失等于  $\cos{(\bold{a},\bold{b}^-)}+m-\cos{(\bold{a},\bold{b}^+)}$</li>
</ul>
</li>
<li><p>Triplet hinge loss: $L(\bold{a},\bold{b}^+,\bold{b}^-)=\max{\{ 0,\cos{(\bold{a},\bold{b}^-)}+m-\cos{(\bold{a},\bold{b}^+)}\}}$</p>
</li>
<li><p>Triplet logistic loss:<br>  ○ Loss function: $L(\bold{a},\bold{b}^+,\bold{b}^-)=\log{( 1+\exp{[\sigma·(\cos{(\bold{a},\bold{b}^-)}-\cos{(\bold{a},\bold{b}^+))}])}}$<br>  ○ $\sigma$ 是大于 0 的超参数，控制损失函数的形状，需手动设置</p>
</li>
</ul>
</li>
<li><p>Listwise</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271648153.png" srcset="/img/loading.gif" lazyload alt="image-20240827164809096" style="zoom:50%;">

<ul>
<li>一条数据包含：<ul>
<li>一个用户，特征向量记作 $\bold{a}$</li>
<li>一个正样本，特征向量记作 $\bold{b}^+$</li>
<li>多个负样本，特征向量记作 $\bold{b}^-_1,…,\bold{b}^-_n$</li>
</ul>
</li>
<li>鼓励 $\cos{(\bold{a},\bold{b}^+)}$ 尽量大，鼓励 $\cos{(\bold{a},\bold{b}^-_1)},…,\cos{(\bold{a},\bold{b}^-_n)}$ 尽量小。</li>
<li>正样本 $y^+=1$，即鼓励 $s^+$ 趋于 1，负样本 $y^-_1=…=y^-_n=0$，即鼓励 $s^-_1…s^-_n$ 趋于 0，用 $y$ 和 $s$ 的交叉熵作为损失函数，意思是鼓励 $\rm Softmax$ 的输出 $s$ 接近标签 $y$。</li>
</ul>
</li>
<li><p><strong>错误的模型设计</strong>：</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408271651186.png" srcset="/img/loading.gif" lazyload alt="image-20240827165104138"></p>
<ul>
<li>用户和物品的向量在进入神经网络前就拼接起来了，和双塔模型有很大区别。</li>
<li>双塔模型是在后期输出相似度时才进行融合。</li>
<li>用户（或物品）自身特征的拼接没有影响，依然保持了用户（或物品）的独立性。而一旦用户和物品进行拼接，此时的输出就特定于该 用户（或物品）了</li>
<li>这种前期融合的模型，不适用于召回<ul>
<li>因为得在召回前，把每个用户向量对应的所有物品向量挨个拼接了送入神经网络</li>
<li>假设有一亿个物品，每给用户做一次召回，就得跑一亿遍。</li>
</ul>
</li>
<li>这种模型通常用于排序，在几千个候选物品中选出几百个</li>
<li>以后看到这种模型就要意识到 —— 这是排序模型，不是召回模型</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li>我的一些thoughts：<ul>
<li>因为<strong>如果是输入后的，保存的是embedding和对应的索引，但是，如果是输入之前的，每次都要重新计算</strong>。换句话说，在线推荐的时候，其实在模型参数固定的情况下，用户塔和物品塔拿到的Embedding结果，是可以离线算出来的。在线召回的时候只需要计算“相似度”，这个是很快很快的。但是如果“拼接 + 塞入神经网络”，每次在线召回的时候，对每个用户的所有物品，都要过一次神经网络，这个计算量是很大的。</li>
<li>以及，当上面不是神经网络的时候，可以用类似于最似近邻查找的方法。在Embedding的空间中分区分块，快速找到结果。而神经网络杜绝了这种可能性，每次都要一对一重新计算，这个计算量也是很大的。</li>
<li>综上：需要遍历的数据量增多 + 每一次计算的时间增长，使得这种模型设计不适用于在线召回。</li>
</ul>
</li>
</ul>
</blockquote>
<h4 id="Positive-and-negative-samples"><a href="#Positive-and-negative-samples" class="headerlink" title="Positive and negative samples"></a>Positive and negative samples</h4><ul>
<li><p>正样本：曝光而且有点击的用户一物品二元组。(用户对物品感兴趣)。问题：少部分物品占据大部分点击’导致正样本大多是热门物品。</p>
</li>
<li><p>解决方案：过采样冷门物品，或降采样热门物品</p>
<ul>
<li>过采样(up-sampling)：一个样本出现多次</li>
<li>降采样(down-sampling)：一些样本被抛弃</li>
</ul>
</li>
<li><p>如何选择负样本：</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408272321209.png" srcset="/img/loading.gif" lazyload alt="image-20240827232116145"></p>
</li>
<li><p>简单负样本：全体物品</p>
<ul>
<li>未被召回的物品，大概率是用户不感兴趣的。</li>
<li>未被召回的物品约等于全体物品（几亿里面抽取几千个，相当于就是全体物品用户都不care，小概率事件约等于不发生）</li>
<li>从全体物品中做抽样，作为负样本。</li>
<li>均匀抽样 or 非均匀抽样？<ul>
<li>均匀抽样：对冷门物品不公平（绝大多数商品都是冷门物品）<ul>
<li>正样本大多是热门物品。</li>
<li>如果均匀抽样产生负样本，负样本大多是冷门物品。</li>
<li>结果：热门物品更热，冷门物品更冷。</li>
</ul>
</li>
<li>非均抽采样：目的是打压热门物品<ul>
<li>负样本抽样概率与热门程度(点击次数)正相关。</li>
<li>抽样概率 正比于 $(点击次数)^{0.75}$。0.75是经验值。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>简单负样本：Batch内负样本</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408272330612.png" srcset="/img/loading.gif" lazyload alt="image-20240827233004480"></p>
<ul>
<li>用户和自己batch内，自己没有点击的物品构成负样本。一个batch内有n个正样本。一个用户和 n-1 个物品组成负样本。这个batch内一共有 n(n-1) 个负样本。</li>
<li>都是简单负样本。（因为第一个用户不喜欢第二个物品。)</li>
<li>Batch内负样本存在的问题<ul>
<li>一个物品出现在batch内的概率 正比于 点击次数</li>
<li>物品成为负样本的概率本该是正比于点击次数的0.75次方，但这里实际是点击次数。热门物品成为负样本的概率过大，即对热门物品打压太狠了，容易造成偏差。</li>
<li>下面这篇论文讲了如何修正偏差：Xinyang Yi et al. Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations. In RecSys, 2019.</li>
</ul>
</li>
<li>修正偏差：<ul>
<li>物品 i 被抽样到的概率：$p_i$ 正比于 点击次数</li>
<li>预估用户对物品 $i$ 的兴趣：$cos(a,b_i)$</li>
<li>做训练的时候，将兴趣调整为：$cos(a,b_i)一log\ p_i$<ul>
<li>这样纠偏，避免过度打压热门物品</li>
<li>训练结束后，在线上做召回时，还是用$cos(a,b_i)$作为兴趣</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>困难负样本：</p>
<ul>
<li>被粗排淘汰的物品（比较困难）<ul>
<li>这些物品被召回，说明和用户兴趣有关；又被粗排淘汰，说明用户对物品兴趣不大</li>
<li>而在对正负样本做二元分类时，这些困难样本容易被分错（被错误判定为正样本）</li>
</ul>
</li>
<li>精排分数靠后的物品（非常困难)<ul>
<li>能够进入精排，说明物品比较符合用户兴趣，但不是用户最感兴趣的</li>
</ul>
</li>
<li>对正负样本做二元分类：<ul>
<li>全体物品（简单）分类准确率高</li>
<li>被粗排淘汰的物品（比较困难）容易分错</li>
<li>精排分数靠后的物品（非常困难）更容易分错</li>
</ul>
</li>
</ul>
</li>
<li><p>训练数据</p>
<ul>
<li>混合几种负样本</li>
<li>50% 的负样本是全体物品（简单负样本）</li>
<li>50% 的负样本是没通过排序的物品（困难负样本）—&gt; 即在粗排、精排淘汰的物品</li>
</ul>
</li>
<li><p>常见错误：把曝光但是没有点击的样本作为负样本</p>
<ul>
<li><p>工业界已经踩过雷，并且作为教训。</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408272349884.png" srcset="/img/loading.gif" lazyload alt="image-20240827234953842"></p>
</li>
<li><p>训练<strong>召回模型不能</strong>用这类负样本。训练<strong>排序模型才能</strong>用这类负样本。</p>
</li>
</ul>
</li>
<li><p>选择负样本的原理：</p>
<ul>
<li>召回的目标：快速找到用户可能感兴趣的物品。即区分用户 <strong>不感兴趣</strong> 和 <strong>可能感兴趣</strong> 的物品，而不是区分 <strong>比较感兴趣</strong> 和 <strong>非常感兴趣</strong> 的物品</li>
<li>全体物品（easy ）：绝大多数是用户根本不感兴趣的。</li>
<li>被排序淘汰（hard ）：用户可能感兴趣，但是不够感兴趣。</li>
<li>有曝光没点击（没用）：用户感兴趣，可能碰巧没有点击<ul>
<li>曝光没点击的物品已经非常符合用户兴趣了，甚至可以拿来做召回的正样本。</li>
<li><strong>可以作为排序的负样本，不能作为召回的负样本</strong>。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Online-recalls-and-updates"><a href="#Online-recalls-and-updates" class="headerlink" title="Online recalls and updates"></a>Online recalls and updates</h4><ul>
<li><p>线上召回：</p>
<ul>
<li><p>双塔模型的召回</p>
<ul>
<li><p><strong>离线</strong>存储：把物品向量 $\bold{b}$ 存入向量数据库</p>
<ul>
<li><p>完成训练之后，用物品塔计算每个物品的特征向量 $\bold{b}$</p>
</li>
<li><p>把几亿个物品向量 $\bold{b}$ 存入向量数据库（比如 Milvus、Faiss、HnswLib ）</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408281123907.png" srcset="/img/loading.gif" lazyload alt="image-20240828112328846" style="zoom:50%;">
</li>
<li><p>向量数据库建索引，以便加速最近邻查找</p>
</li>
</ul>
</li>
<li><p>线上召回：查找用户最感兴趣的 $k$ 个物品</p>
<ul>
<li><p>给定用户 ID 和画像，线上用神经网络现算（实时计算）用户向量 $\bold{a}$。</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408281124181.png" srcset="/img/loading.gif" lazyload alt="image-20240828112425139" style="zoom:50%;">
</li>
<li><p>最近邻查找：</p>
<ul>
<li>把向量 $\bold{a}$ 作为 query，调用向量数据库做最近邻查找</li>
<li>返回余弦相似度最大的 $k$ 个物品，作为召回结果</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>为什么事先存储物品向量 $\bold{b}$，线上现算用户向量 $\bold{a}$？</p>
<ul>
<li>为了拿到用户的实时特征啊，离线存储可能推的就不是用户当前喜欢的了，物品特征变化很小或者不变，但是用户特征变化很快</li>
<li>每做一次召回，用到一个用户向量 $\bold{a}$，几亿物品向量 $\bold{b}$（线上算物品向量的代价过大）</li>
<li><strong>用户兴趣动态变化，而物品特征相对稳定</strong>（可以离线存储用户向量，但不利于推荐效果）</li>
</ul>
</li>
</ul>
</li>
<li><p>模型更新</p>
<ul>
<li><p>全量更新</p>
<ul>
<li>全量更新：每天凌晨，用昨天全天的数据训练模型</li>
<li>在昨天模型参数的基础上做训练（不是重新随机初始化）</li>
<li>用昨天的数据，训练 1 epoch，即每天数据只用一遍</li>
<li>发布新的 <strong>用户塔神经网络</strong> 和 <strong>物品向量</strong>，存入向量数据库并建立索引。供线上召回使用</li>
<li>全量更新对数据流、系统的要求比较低。不需要实时数据流，对数据生成速度没有要求，延迟一两个小时也没关系。只需要凌晨批处理就行。每天只发布一次，对系统要求也低。</li>
</ul>
</li>
<li><p>增量更新：</p>
<ul>
<li>做 online learning 更新模型参数，用户兴趣会随时发生变化。小时级别的增量更新，对数据流和系统要求都很高，实时收集线上数据，做流式处理，生成 TFRecord 文件。</li>
<li>对模型做 online learning，增量更新 ID Embedding 参数<strong>（不更新神经网络其他部分的参数）</strong>。即锁住全连接层的参数，只更新 Embedding 层的参数，只有全量更新才更新全连接层，这是出于工程实现的考量。</li>
<li>发布用户 ID Embedding，供用户塔在线上计算用户向量，最新的ID Embedding会捕捉到用户最新的兴趣点，对推荐很有帮助。 -&gt; 其实用户的ID Embedding变了，在Item Embedding空间中的位置就变了，最近邻搜索到的结果就会不一样昂！</li>
</ul>
</li>
<li><p>双管齐下：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408281136556.png" srcset="/img/loading.gif" lazyload alt="image-20240828113623416" style="zoom:50%;">

<ul>
<li>可以只增量，不全量吗？<ul>
<li>试过了，效果不好。小时级数据有偏；分钟级数据偏差更大。</li>
<li>增量更新：按照数据从早到晚的顺序，做1 epoch训练。</li>
<li>全量更新：random shuffle一天的数据，做1 epoch训练。（Shuffle就是为了消除偏差）<strong>随机打乱</strong>优于<strong>按顺序排列数据</strong>，全量训练优于增量训练。</li>
<li>可以试试分钟学ID，小时学全部特征嵌入，但不学神经网络，按天学神经网络。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Summmary"><a href="#Summmary" class="headerlink" title="Summmary"></a>Summmary</h4><ul>
<li><p>双塔模型</p>
<ul>
<li>用户塔、物品塔各输出一个向量，两个向量的余弦。相似度作为兴趣的预估值。</li>
<li>三种训练的方式：pointwise pairwise、listwise。</li>
<li>正样本：用户，点击过的物品。</li>
<li>负样本：全体物品（简单)、被排序淘汰的物品（困难)</li>
</ul>
</li>
<li><p>召回</p>
<ul>
<li>做完训练，把物品向量存储到向量数据库，供线上最近邻查找。</li>
<li>线上召回时，给定用户D、用户画像，调用用户塔，现算用户向量a。</li>
<li>把a作为query，查询向量数据库，找到余弦相似度最高的k个物品向量，返回k个物品ID。</li>
</ul>
</li>
<li><p>更新模型</p>
<ul>
<li>全量更新：今天凌晨，用昨天的数据训练整个神经网络，做1 epoch的随机梯度下降。</li>
<li>增量更新：用实时数据训练神经网络，只更新ID Embedding,锁住全连接层。</li>
<li>实际的系统：<ul>
<li><strong>全量更新</strong> &amp; <strong>增量更新</strong>相结合。</li>
<li>每隔几十分钟，发布最新的用户ID Embedding，供用户塔在线上计算用户向量。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Twin-Tower-Self-supervised-Learning"><a href="#Twin-Tower-Self-supervised-Learning" class="headerlink" title="Twin Tower + Self-supervised Learning"></a>Twin Tower + Self-supervised Learning</h4><ul>
<li><p>自监督学习的目的：把物品塔训练的更好。</p>
</li>
<li><p>双塔模型的问题</p>
<ul>
<li><p>推荐系统的头部效应严重：</p>
<ul>
<li>少部分物品占据大部分点击。</li>
<li>大部分物品的点击次数不高。</li>
</ul>
</li>
<li><p>高点击物品的表征学得好，长尾物品的表征学得不好。</p>
</li>
<li><p>自监督学习：做data augmentation，更好地学习长尾物品的向量表征。</p>
<blockquote>
<p>参考：Tiansheng Yao et al.Self-supervised Learning for Large-scale Item Recommendations. In CIKM,2021.</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h5 id="Listwise-Training"><a href="#Listwise-Training" class="headerlink" title="Listwise Training"></a>Listwise Training</h5><ul>
<li><p>训练方式</p>
<ul>
<li><p>Listwise上面提到过，一个Batch里面，一个用户自己感兴趣的为正样本，和其他用户感兴趣的构成负样本。</p>
<ul>
<li>一个batch包含正样本：$(a_1, b_1), (a_2, b_2), … , (a_n, b_n)$</li>
<li>负样本${(a_i, b_j)}$，对于所有的i不等于j。</li>
<li>训练的时候，鼓励$cos(a_i, b_i)$尽量大，鼓励$cos(a_i, b_j)$尽量小。</li>
</ul>
</li>
<li><p>损失函数：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408282129071.png" srcset="/img/loading.gif" lazyload alt="image-20240828212955898" style="zoom:50%;"></li>
</ul>
</li>
<li><p>纠偏：</p>
<ul>
<li><p>Batch类负样本会过度打压负样本，要纠偏。</p>
</li>
<li><p>物品j被抽样到的概率$p_j$正比于点击次数。做训练的时候，预估用户 i 对物品 j 的兴趣：$cos(a_i, b_j) - log\ p_{j}$，训练结束后，在线上做召回的时候则不用纠偏，直接用余弦相似度即可。</p>
<blockquote>
<p>参考：Xinyang Yi et al.Sampling-Bias-Corrected Neural Modeling for Large<br>Corpus Item Recommendations.In RecSys,2019.</p>
</blockquote>
</li>
</ul>
</li>
<li><p>训练双塔模型：</p>
<ul>
<li>从点击数据中随机抽取个用户一物品二元组，组成一个batch。</li>
<li>双塔损失函数：$L_{main}[i] = -log(\frac{exp(cos(a_i, b_i)-log\ p_i)}{\sum_{j=1}^{n}exp(cos(a_i, b_j))-log\ p_{j}})$，这个地方i为对应的第i个用户。</li>
<li>做梯度下降，减少损失函数：$\frac{1}{n}\sum_{i=1}^{n}L_{main}[i]$，这里指的是一个batch中，所有n个用户在双塔函数上的损失的平均值。</li>
</ul>
</li>
<li><p>同时训练用户塔和物品塔。</p>
</li>
</ul>
<h5 id="Self-supervised-Learning"><a href="#Self-supervised-Learning" class="headerlink" title="Self-supervised Learning"></a>Self-supervised Learning</h5><ul>
<li><p>用于训练物品塔。</p>
</li>
<li><p>对于不同的item，分别做多种变换，会得到多个特征，但是从广义上讲，应该满足：</p>
<ul>
<li>同一个物品即使做了不同的变换，在经过物品塔后，也应该拿到相似的Embedding，越相似越好。</li>
<li>不同的物品，不管做了什么样的变换，他们的Embedding都不应该相似，越不同越好。</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408282321098.png" srcset="/img/loading.gif" lazyload alt="image-20240828232136028" style="zoom:50%;">

<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408282322501.png" srcset="/img/loading.gif" lazyload alt="image-20240828232232458" style="zoom:50%;">

<ul>
<li>物品i的两个向量表征$b_{i}^{‘}$和$b_{i}^{‘’}$有较高的相似度，物品i和j的向量表征$b_j^{‘}$和$b_j^{‘’}$有较低的相似度。鼓励$cos(b_{i}^{‘}, b_{i}^{‘’})$尽量大，$cos(b_{i}^{‘}, b_{j}^{‘’})$尽量小。</li>
</ul>
</li>
<li><p>自监督学习常用的特征变换</p>
<ul>
<li>Random mask<ul>
<li>随机选一些离散特征（比如类目)，把它们遮住。</li>
<li>例：某物品的类目特征是U={数码，摄影}，之前有可能是分别搞出来embedding，然后加和平均之类的。现在是直接遮住！</li>
<li>Mask后的类目特征是U’={default}，意思是默认的缺失值，然后对于Default做Embedding。相当于物品的类目特征直接整个被“丢掉”了。</li>
</ul>
</li>
<li>Dropout(仅对多值离散特征生效)<ul>
<li>一个物品可以有多个类目，那么类目是一个多值离散特征。Dropout：随机丢弃特征中50%的值。</li>
<li>例：某物品的类目特征是U={美妆，摄影}。Dropout后的类目特征是U’={美妆}。</li>
</ul>
</li>
<li>互补特征(complementary)<ul>
<li>假设物品一共有4种特征：ID,类目，关键词，城市。正常做法：四个特征的值分别做Embedding，然后拼起来输入物品塔，得到物品的向量表征。</li>
<li>随机分成两组：{ID, 关键词}和{类目，城市}。</li>
<li>{ID,default,关键词，default} -&gt; 物品表征，{default,类目，default,城市} -&gt; 物品表征。由于是同一个物品的表征，鼓励上面两个向量相似，cos大。</li>
</ul>
</li>
<li>Mask一组关联的特征<ul>
<li>为啥这么做？特征之间有较强的关联，遮住一个特征并不会损失太多信息。模型可以从其他强关联特征中，学到遮住的特征。最好是把关联特征一次全都遮住。</li>
<li>e.g.<ul>
<li>受众性别：U={男，女，中性}，类目：V={美妆，数码，足球，摄影，科技，…}</li>
<li>u=女 和 v=美妆 同时出现的概率 p(u,v) 大，u=女 和 v=数码 同时出现的概率p(u,v)小。这里性别和类目的关联就很强。</li>
<li>p(u)：某特征取值为u的概率。p(男性)=20%，p(女性)=30%。p(中性)=50%</li>
<li>p(u,v)：某特征取值为u, 另一个特征取值为v, 同时发生的概率。p(女性，美妆)=3%，p(女性，数码)=0.1%</li>
<li>离线计算特征两两之间的关联，用互信息（mutual<br>information)衡量：$MI(\mathcal{U}, \mathcal{V}) = \sum_{u\in\mathcal{U}}\sum_{v\in\mathcal{V}}p(u,v)\times log\frac{p(u,v)}{p(u).p(v)}$，两种特征关联强，p(u, v)就比较大，MI就会大。</li>
</ul>
</li>
<li>设一共有k种特征。离线计算特征两两之间MI，得到k x k的矩阵。随机选一个特征作为种子，找到种子最相关的k/2种特征。Mαsk种子及其相关的k/2种特征，保留其余的k/2种特征。</li>
<li>优缺点：<ul>
<li>好处：比random mask、dropout、互补特征等方法<br>效果更好。</li>
<li>坏处：方法复杂，实现的难度大，不容易维护。每添加一个新的特征，都要重新算一遍所有特征的MI。对于业界来讲，ROI太低了！</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>训练模型：</p>
<ul>
<li><p>从全体物品中均匀抽样，得到m个物品作为一个batch。</p>
</li>
<li><p>做两类特征变换，物品塔输出两组向量：b1’, b2’, …, bm’和b1’’, b2’’, …, bm’’。</p>
</li>
<li><p>第i个item的损失函数：$L_{self}[i] = -log(\frac{exp(cos(b_i^{‘}, b_i^{‘’}))}{\sum_{j=1}^{m}exp(cos(b_i^{‘}, b_j^{‘’}))})$</p>
</li>
<li><p>示意图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408290008208.png" srcset="/img/loading.gif" lazyload alt="image-20240829000816141"></p>
</li>
<li><p>做梯度下降，减少自监督学习的损失：$\frac{1}{m}\sum_{i=1}^{m}L_{self}[i]$，算一个Batch数据Loss的平均值。</p>
</li>
</ul>
</li>
<li><p>Summary</p>
<ul>
<li><p>双塔模型学不好低曝光物品的向量表征。</p>
</li>
<li><p>自监督学习</p>
<ul>
<li>对物品做随机特征变换。</li>
<li>相同物品的特征向量相似度高，不同物品的特征向量相似度低。</li>
<li>实验效果：低曝光物品、新物品的推荐变得更准。</li>
</ul>
</li>
<li><p>训练模型：</p>
<ul>
<li><p>对点击做随机抽样，得到n对用户一物品二元组，作为batch</p>
</li>
<li><p>从全体物品中均匀抽样，得到m个物品作为一个batch</p>
</li>
<li><p>做梯度下降，使得损失减小：$\frac{1}{n}\sum_{i=1}^{n}L_{main}[i] + \alpha.\frac{1}{m}\sum_{j=1}^{m}L_{self}[j]$</p>
<blockquote>
<p>前半部分是双塔模型的损失，后半部分是自监督学习的损失。中间的$\alpha$是超参数，决定了自监督学习的作用。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Deep-Retrieval"><a href="#Deep-Retrieval" class="headerlink" title="Deep Retrieval"></a>Deep Retrieval</h3><ul>
<li>经典的双塔模型把用户、物品表示为向量，线上做最近邻查找。Deep Retrieval 把物品表征为路径(path)，线上查找用户最匹配的路径。Deep Retrieval类似于阿里的TDM。</li>
<li>Outline<ul>
<li>索引：<ul>
<li>路径 → List&lt;物品&gt;</li>
<li>物品 → List&lt;路径&gt;</li>
</ul>
</li>
<li>预估模型：神经网络预估用户对路径的兴趣</li>
<li>线上召回：用户→路径→物品。</li>
<li>训练：<ul>
<li>学习神经网络参数。</li>
<li>学习物品表征（物品 -&gt; 路径）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Concept-3"><a href="#Concept-3" class="headerlink" title="Concept"></a>Concept</h4><ul>
<li>物品可以用路径来表示：</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408281724855.png" srcset="/img/loading.gif" lazyload alt="image-20240828172418708"></p>
<ul>
<li><p>索引：</p>
<ul>
<li>item -&gt; List<path><ul>
<li>训练的时候使用。</li>
<li>一个物品多条路径</li>
<li>用三个节点表示一条路径：path = [a, b, c]</li>
</ul>
</path></li>
<li>path -&gt; List<item><ul>
<li>线上召回的时候使用。</li>
<li>一条路径对应多个物品</li>
</ul>
</item></li>
</ul>
</li>
<li><p>Deep Retrieval原理</p>
<ul>
<li><p>本质是一种神经网络，预估模型，预估用户对路径的兴趣分数。可以根据用户特征召回多条路径。</p>
</li>
<li><p>预估用户对路径的兴趣</p>
<ul>
<li><p>条件概率，朴素贝叶斯，用户喜欢a -&gt; 用户喜欢a的情况下喜欢a,b -&gt; 用户喜欢a,b的情况下喜欢a,b,c</p>
</li>
<li><p>算法流程：</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408281730760.png" srcset="/img/loading.gif" lazyload alt="image-20240828173002629"></p>
<blockquote>
<p>这里的每一层神经网络，都不共享参数的哈！论文中使用的是beam search。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p>线上召回：</p>
<ul>
<li><p>用户 -&gt; 路径 -&gt; 物品，流程</p>
<ul>
<li>第一步：给定用户特征，用beam search召回一批路径。</li>
<li>第二步：利用索“path→List(item)”, 召回一批物品，每条路径对应多个物品。</li>
<li>第三步：对物品做打分和排序，选出一个子集。打分没有限制，小的排序模型就行，例如双塔模型。</li>
</ul>
</li>
<li><p>Beam Search</p>
<ul>
<li><p>假设有3层，每层K个节点，那么一共有K^3条路径。用神经网络给所有K3条路径打分，计算量太大。</p>
</li>
<li><p>用beam search,可以减小计算量。需要设置超参数beam size。Beam_size = 1就是贪心，越大，全局效果越好！一个Demo：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408281737951.png" srcset="/img/loading.gif" lazyload alt="image-20240828173731824" style="zoom:50%;"></li>
</ul>
</li>
</ul>
</li>
<li><p>训练：</p>
<ul>
<li><p>同时学习神经网络参数和物品表征。</p>
<ul>
<li>神经网络p(a,b,c | x)预估用户对路径[a,b,c]的兴趣</li>
<li>把一个物品表征为多条路径{[a,b,c]},建立索引<ul>
<li>item -&gt; List(path),</li>
<li>path -&gt; List(item)。</li>
</ul>
</li>
</ul>
</li>
<li><p>训练只用正样本(user, item): click(user, item) = 1，用户点过就算正样本。</p>
</li>
<li><p>神经网络参数：</p>
<ul>
<li>这个神经网络用来表征用户对于物品多感兴趣，物品表征为J条路径：[a1, b1, c1], …, [aj, bj, cj]</li>
<li>用户对路径a, b, c感兴趣。如果用户点击过物品，说明用户对这个物品的所有J条路径感兴趣，这时候就应该让$\sum_{j=1}^{J}p(a_j, b_j, c_j\ | \ x)$变大。</li>
<li>损失函数：$loss = -log(\sum_{j=1}^{J}p(a_j, b_j, c_j\ | \ x))$</li>
</ul>
</li>
<li><p>学习物品表征：</p>
<ul>
<li><p>用户user对路径path=[a,b,c]的兴趣记作 $p(path | user)=p(a,b,c | x)$.</p>
</li>
<li><p>item与path的相关性：$score(item, path) = \sum_{user}p(path|user)\times click(user, item)$，第一项是用户对于路径的兴趣，神经网络预估，click点击了就是1，没点击就是0。</p>
</li>
<li><p>根据Score选出J条路径，作为item的表征。</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408281751022.png" srcset="/img/loading.gif" lazyload alt="image-20240828175121870" style="zoom:50%;">

<blockquote>
<p>通过用户为“中介”，串联路径和物品的关系。</p>
</blockquote>
</li>
<li><p>选出J条路径$\pi = {path1,…,path}$,作为物品的表征。</p>
</li>
<li><p>损失函数（选择与item高度相关的path)：<br>$loss(item,\pi)=-log(\sum_{j=1}^{J}score(item,path_{j}))$</p>
</li>
<li><p>正则项（避免过多的item集中在一条path上)：<br>$reg(path_j)=(number\ of\ items\ on\ path )^4$</p>
</li>
<li><p>贪心算法更新路径：每次固定j-1条路径，并从从来没有被选中的路径中，选出一条新作为新的path，$argmin_{path_l}loss(item, \pi) + \alpha\times{reg(path_l)}$</p>
<blockquote>
<p>较高的分数score，而且路径上的物品不会太多。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><ul>
<li><p>更新神经网络</p>
<ul>
<li>神经网络判断<strong>用户对路径</strong>的兴趣<br>$p(path \ | \ x)$.</li>
<li>训练所需的数据<ul>
<li>“物品→路径”的索引</li>
<li>用户点击过的物品。</li>
</ul>
</li>
<li>如果用户点击过物品，且物品对应路径path，则更新神经网络参<br>数使$p(path|x)$变大</li>
</ul>
</li>
<li><p>更新物品表征</p>
<ul>
<li>判断物品与路径的相关性：<ul>
<li>物品 ← 用户 → 路径</li>
<li>用户点击过物品（物品 ← 用户）</li>
<li>神经网络的打分（用户 → 路径）</li>
</ul>
</li>
<li>让每个物品关联J条路径<ul>
<li>物品和路径要有很高的相关性。</li>
<li>一条路径上不能有过多的物品。</li>
</ul>
</li>
</ul>
</li>
<li><p>在线召回：用户 -&gt; 路径 -&gt; 物品</p>
<ul>
<li>根据神经网络，给定用户的特征，可以算出对于路径的感兴趣程度。</li>
<li>Beam search召回最高的 s 条路径。</li>
<li>根据上面物品表征过程中建立的索引，根据路径，查找到路径上的n个物品。</li>
<li>一共召回 s x n 个物品，排序，返回分数高的若干物品。</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>Deep Retrieval本质：路径作为用户和物品的中介。双塔本质：用向量表征作为用户和物品的中介。</strong></p>
</blockquote>
<ul>
<li>离线训练<ul>
<li>同时学习 用户—路径，物品—路径的关系。</li>
<li>用户—路径：<ul>
<li>一个物品被表征为J条路径，存储在物品 -&gt; 路径的索引上。（现有数据：物品 -&gt; 路径）</li>
<li>如果用户点击过物品，则更新神经网路参数，分数变大。（现有数据：用户 -&gt; 物品）</li>
<li>可以训练模型，学习：用户 -&gt; 路径的兴趣，使得用户对于感兴趣的物品，对应的路径的加和增大。</li>
</ul>
</li>
<li>物品—路径：<ul>
<li>上面神经网络线训出来了，这个时候就可以用来判断，任意的用户对于任意的路径的感兴趣程度。（现有数据：用户 -&gt; 路径）</li>
<li>如果用户还点击过物品。（现有数据：用户 -&gt; 物品）</li>
<li>可以学习到：路径和物品的关系，感兴趣的path和实际点击过的物品，这两者关联性高。</li>
<li>寻找与Item相关的 J 条路径，且避免一条路径上的物品过多。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Other-recall-channels"><a href="#Other-recall-channels" class="headerlink" title="Other recall channels"></a>Other recall channels</h3><h4 id="GeoHash"><a href="#GeoHash" class="headerlink" title="GeoHash"></a>GeoHash</h4><ul>
<li><p>GeoHash 召回</p>
<ul>
<li><p>用户可能对附近发生的事感兴趣</p>
</li>
<li><p>GeoHash：对经纬度的编码，大致表示地图上一个长方形区域</p>
</li>
<li><p>索引：GeoHash → 优质笔记列表（按时间倒排）</p>
</li>
<li><p>这条召回通道没有个性化（正因为如此，才需要有优质笔记列表）</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408282044255.png" srcset="/img/loading.gif" lazyload alt="image-20240828204435194" style="zoom:50%;">
</li>
<li><p>根据用户定位的 GeoHash，取回该地点最新的 $k$ 篇优质笔记</p>
</li>
<li><p>同城召回</p>
<ul>
<li>用户可能对同城发生的事感兴趣</li>
<li>索引： 城市 → 优质笔记列表（按时间倒排）</li>
<li>这条召回通道没有个性化</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Author"><a href="#Author" class="headerlink" title="Author"></a>Author</h4><ul>
<li>作者召回<ul>
<li>关注作者召回<ul>
<li>用户对关注的作者发布的笔记感兴趣。索引：<ul>
<li>用户 → 关注的作者</li>
<li>作者 → 发布的笔记</li>
</ul>
</li>
<li>召回： 用户 → 关注的作者 → 最新的笔记</li>
</ul>
</li>
<li>有交互的作者召回<ul>
<li>如果用户对某笔记感兴趣（点赞、收藏、转发），那么用户可能对该作者的其他笔记感兴趣</li>
<li>索引： 用户 → 有交互的作者，作者列表需要定期更新，加入最新交互的作者，删除长期未交互的作者</li>
<li>召回： 用户 → 有交互的作者 → 最新的笔记</li>
</ul>
</li>
<li>相似作者召回<ul>
<li>如果用户喜欢某作者，那么用户喜欢相似的作者</li>
<li>索引：作者 → 相似作者（$k$ 个作者）<ul>
<li>作者相似度的计算类似于 ItemCF 中判断两个物品的相似度</li>
<li>例如两个作者的粉丝有很大重合，则认定两个作者相似</li>
</ul>
</li>
<li>召回：用户 → 感兴趣的作者 → 相似作者 → 最新的笔记<br>（$n$ 个作者）  （$nk$ 个作者）（$nk$ 篇笔记）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Cache"><a href="#Cache" class="headerlink" title="Cache"></a>Cache</h4><ul>
<li>缓存召回<ul>
<li>想法：复用前 $n$ 次推荐精排的结果</li>
<li>背景：<ul>
<li>精排输出几百篇笔记，送入重排</li>
<li>重排做多样性抽样，选出几十篇</li>
<li>精排结果一大半没有曝光，被浪费</li>
</ul>
</li>
<li>精排前 50，都是用户非常感兴趣的，但是没有曝光而已，值得再次尝试。缓存起来，作为一条召回通道。</li>
</ul>
</li>
<li>缓存大小固定，需要退场机制<ul>
<li>一旦笔记成功曝光，就从缓存退场</li>
<li>如果超出缓存大小，就移除最先进入缓存的笔记</li>
<li>笔记最多被召回 10 次，达到 10 次就退场</li>
<li>每篇笔记最多保存 3 天，达到 3 天就退场</li>
<li>上面这里介绍的规则都比较简单粗暴，还能够有更多的策略，再来细化规则。</li>
</ul>
</li>
</ul>
<h3 id="Exposure-filter-amp-Bloom-filter"><a href="#Exposure-filter-amp-Bloom-filter" class="headerlink" title="Exposure filter &amp; Bloom filter"></a>Exposure filter &amp; Bloom filter</h3><ul>
<li><p>曝光过滤问题：</p>
<ul>
<li><p>如果用户看过某个物品，则不再把该物品曝光给该用户。</p>
</li>
<li><p>对于每个用户，记录已经曝光给他的物品。（小红书只召回1个月以内的笔记，因此只需要记录每个用户最近1个月的曝光历史。)</p>
</li>
<li><p>对于每个召回的物品，判断它是否已经给该用户曝光过，排除掉曾经曝光过的物品。</p>
</li>
<li><p>一位用户看过几个物品，本次召回r个物品，如果暴力对比，需要O(nr)的时间。</p>
<blockquote>
<p>小红书为例子，用户一个月n的量级在几千，每次召回r的量级也在几千。如果nr的话，暴力对比的计算量太大了。</p>
</blockquote>
</li>
</ul>
</li>
<li><p>Bloom Filter：</p>
<ul>
<li><p>Bloom filter判断一个物品ID是否在已曝光的物品集合中。</p>
</li>
<li><p>如果判断为no，那么该物品一定不在集合中。</p>
</li>
<li><p>如果判断为yes，那么该物品很可能在集合中。（可能误伤，错误判断未曝光物品为已曝光，将其过滤掉。)</p>
</li>
<li><p>Bloom filter把物品集合表征为一个m维二进制向量。每个用户有一个曝光物品的集合，表征为一个向量，需要m bit的存储。Bloom filter有k个哈希函数，每个哈希函数把物品D映射成介于 0 和 m-1 之间的整数。</p>
</li>
<li><p>Demo</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408282103922.png" srcset="/img/loading.gif" lazyload alt="image-20240828210356743" style="zoom:50%;">

<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408282104368.png" srcset="/img/loading.gif" lazyload alt="image-20240828210456206" style="zoom:50%;">
</li>
<li><p>曝光过滤的链路：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408282108553.png" srcset="/img/loading.gif" lazyload alt="image-20240828210835389" style="zoom:50%;">

<blockquote>
<p>实时流处理要足够快哈，不然用户刷着刷着，很容易就出现重复了。这一块儿挂了/延时特别大，其实也是比较常见的情况。</p>
</blockquote>
</li>
<li><p>缺点</p>
<ul>
<li>Bloom filter只支持添加物品，不支持删除物品。从集合中移除物品，无法消除它对向量的影响。</li>
<li>每天都需要从物品集合中移除年龄大于1个月的物品(超龄物品不可能被召回，没必要把它们记录在Bloom filter,降低n可以降低误伤率。)，这种场景Bloom Filter用起来就有点困难。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Sort"><a href="#Sort" class="headerlink" title="Sort"></a>Sort</h2><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408301050257.png" srcset="/img/loading.gif" lazyload alt="image-20240830105024176" style="zoom:50%;">

<blockquote>
<ul>
<li>本节内容集中在粗排和精排，它们的原理基本相同，只是粗排模型小，用的特征少，效果差一些。</li>
<li>粗排的目的是做快速的初步筛选，<strong>如果不用粗排，直接对于几千篇笔记用很大的精排模型，计算代价承受不住</strong>。（这里还有一致性的问题，参考<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/2318414">这篇博客</a>）。</li>
<li>流程<ul>
<li>召回：很多召回通道，从几亿 -&gt; 几千。</li>
<li>粗排给笔记打分，几千 -&gt; 几百。精排对于选出的几百篇笔记打分，不做截断，让几百篇笔记带着精排的分数直接进入重排。</li>
<li>重排做多样性抽样，并且打散相似内容。最终只有几十篇笔记被选中，展示给用户。</li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="Multi-objective-ranking-model"><a href="#Multi-objective-ranking-model" class="headerlink" title="Multi-objective ranking model"></a>Multi-objective ranking model</h3><ul>
<li><p>用户—笔记的交互：</p>
<ul>
<li>排序的主要依据是用户对笔记的兴趣，而兴趣都反映在 用户—笔记 的交互中。<ul>
<li>对于每篇笔记，系统记录： <ul>
<li>曝光次数（number of impressions）</li>
<li>点击次数（number of clicks）</li>
<li>点赞次数（number of likes）</li>
<li>收藏次数（number of collects）</li>
<li>转发次数（number of shares）</li>
</ul>
</li>
<li>点击率 = 点击次数 / <strong>曝光</strong>次数 </li>
<li>点赞率 = 点赞次数 / <strong>点击</strong>次数 </li>
<li>收藏率 = 收藏次数 / <strong>点击</strong>次数 </li>
<li>转发率 = 转发次数 / <strong>点击</strong>次数 （少见行为，远少于上面的点赞和收藏，转发到外部平台可以引流！）</li>
<li>排序的依据 <ul>
<li>排序模型预估点击率、点赞率、收藏率、转发率等多种分数</li>
<li>融合这些预估分数（比如加权和）</li>
<li>根据融合的分数做排序、截断。保留分数高的笔记，淘汰分数低的笔记。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>多目标模型：</p>
<ul>
<li><p>模型结构：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408301105126.png" srcset="/img/loading.gif" lazyload alt="image-20240830110542925" style="zoom:50%;">

<blockquote>
<p>统计特征包括”用户统计特征”和”物品统计特征”，”场景特征” 是随着用户请求传过来的。</p>
</blockquote>
</li>
<li><p>损失函数：</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408301122397.png" srcset="/img/loading.gif" lazyload alt="image-20240830112209205"></p>
<blockquote>
<p>实际数据其实是0/1（二元分类），用户是否点击/点赞这样的指标，然后和我们神经网路输出的possibility做比较，得到损失，</p>
</blockquote>
</li>
<li><p>训练：让预估值接近真实目标值 </p>
<ul>
<li>总的损失函数： $\sum^4_{i=1}{α_i·\rm{CrossEntropy}(y_i,p_i)}$ </li>
<li>对损失函数求梯度，做梯度下降更新参数 </li>
<li>困难：类别不平衡，即正样本数量显著少于负样本 <ul>
<li>每 100 次曝光，约有 10 次点击、90 次无点击</li>
<li>每 100 次点击，约有 10 次收藏、90 次无收藏</li>
<li>不平衡的影响在于：太多负样本用处不大，白白浪费计算资源。</li>
</ul>
</li>
<li>解决方案：负样本降采样（down-sampling） <ul>
<li>保留一小部分负样本</li>
<li>让正负样本数量平衡，节约计算量。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>预估值校准</p>
<ul>
<li>做了降采样后训练出的预估点击率会大于真实点击率。</li>
<li>正样本、负样本数量为 $n_+$ 和 $n_-$，对负样本做降采样，抛弃一部分负样本 。</li>
<li>使用 $\alpha . n_-$ 个负样本，$\alpha$是采样率，在[0, 1] 之间。由于负样本变少，<strong>预估点击率大于真实点击率</strong>。 采样的负样本越少，对于点击率的高估越大。</li>
<li>校准公式： <ul>
<li>真实点击率：$p_{true} = \frac{n_+}{n_- +n_+}$</li>
<li>预估点击率：$p_{pred} = \frac{n_+}{n_+ +\alpha . n_-}$</li>
<li>可以得到校准公式，从pred -&gt; true: $p_{true} = \frac{\alpha.p_{pred}}{(1-p_{pred}) + \alpha.p_{pred}}$</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>参考文献：1. Xinran He et al. Practical lessons from predicting clicks on ads at Facebook. In the 8th International Workshop on Data Mining for Online Advertising.</p>
</blockquote>
<h3 id="Multi-gate-Mixture-of-Experts-MMoE"><a href="#Multi-gate-Mixture-of-Experts-MMoE" class="headerlink" title="Multi-gate Mixture-of-Experts(MMoE)"></a>Multi-gate Mixture-of-Experts(MMoE)</h3><ul>
<li><p>模型结构：</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408301154142.png" srcset="/img/loading.gif" lazyload alt="image-20240830115413945"></p>
<ul>
<li><p>三个神经网络结构相同，但是不共享参数。专家神经网络的数量是超参数，实践中通常用 4 个或 8 个。</p>
</li>
<li><p>把特征向量输入左侧的神经网络，再通过 Softmax 输出 $p_1,p_2,p_3$ 分别对应 3 个专家神经网络。用 $p_1,p_2,p_3$ 作为权重，对 3 个专家神经网络的输出 $x_1,x_2,x_3$ 进行加权平均。</p>
</li>
<li><p>采取同样方法，将特征向量输入右侧的神经网络，得到的 $q_1,q_2,q_3$ 可以与专家神经网络的输出组成另一个预估值的输入。</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408301156238.png" srcset="/img/loading.gif" lazyload alt="image-20240830115605036" style="zoom:50%;">

<blockquote>
<p>每一个指标，都会有一组专门的权重，负责处理多个Experts的输出，并最终输出这个指标的预估值。</p>
</blockquote>
</li>
</ul>
</li>
<li><p>极化现象（Polarization）</p>
<ul>
<li><p>专家神经网络在实践中的问题。</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408301158642.png" srcset="/img/loading.gif" lazyload alt="image-20240830115838439" style="zoom:50%;">

<blockquote>
<p>不同的权重，在多个指标下，使得某个神经网络没有被用到，或者用到的特别少。就相当于没有对专家做融合嘛，没有起到MoE的优势。</p>
</blockquote>
</li>
<li><p>解决极化问题的方法：</p>
<ul>
<li>如果有 $n$ 个“专家”，那么每个 softmax 的输入和输出都是 $n$ 维向量 </li>
<li>在<strong>训练</strong>时，对 softmax 的输出使用 dropout <ul>
<li>Softmax 输出的 $n$ 个数值被 mask 的概率都是 10% </li>
<li>每个“专家”被随机丢弃的概率都是 10% </li>
<li>由于每个“专家”都可能被丢弃，<strong>神经网络在训练过程中没法过度依赖某个“专家”</strong>，神经网络就会尽量避免极化的发生。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>实际应用场景中，MMoE用上了不一定会有提升。博主聊过，MMoE没效果的原因很多情况下不清楚，有可能是设计/应用场景的原因。很有可能是试过了但是不work！</p>
</li>
</ul>
<blockquote>
<p>参考文献：</p>
<ul>
<li>Jiaqi Ma et al.Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts.In KDD,2018.</li>
<li>Zhe Zhao et al.Recommending What Video to Watch Next:A Multitask Ranking System.In RecSys, 2019.</li>
</ul>
</blockquote>
<h3 id="Fusion-of-estimated-scores"><a href="#Fusion-of-estimated-scores" class="headerlink" title="Fusion of estimated scores"></a>Fusion of estimated scores</h3><ul>
<li><p>简单的加权：$p_{click}+w_1·p_{like}+w_2·p_{collect}+…$</p>
</li>
<li><p>点击率乘以其他项的加权和 $p_{click}·(1+w_1·p_{like}+w_2·p_{collect}+…)$ </p>
<ul>
<li>$p_{click}=\frac{点击}{曝光}$，$p_{like}=\frac{点赞}{点击}$</li>
<li>所以 $p_{click}·p_{like}=\frac{点赞}{曝光}$（类似于贝叶斯的想法）</li>
</ul>
</li>
<li><p>海外某短视频 APP 的融分公式 : $(1+w_{1}\cdot p_{time})^{\alpha_{1}}\ \cdot\ \ (1+w_{2}\cdot p_{like})^{\alpha_{2}}\ \cdots$，这里$\omega_1,\ \alpha_1$都是超参数。</p>
</li>
<li><p>国内某短视频 APP （老铁）的融分公式 </p>
<ul>
<li>根据预估时长 $p_{time}$，对 $n$ 篇候选视频做排序 </li>
<li>如果某视频排名第 $r_{time}$，则它得分 $\frac{1}{r^\alpha _{time} +\beta}$</li>
<li>对点击、点赞、转发、评论等预估分数做类似处理 </li>
<li>最终融合分数： $\frac{w_{1}}{r_{\mathrm{time}}^{\alpha_{1}}+\beta_{1}}\ +\ \frac{w_{2}}{r_{\mathrm{click}}^{\alpha_{2}}+\beta_{2}}\ +\ \frac{w_{3}}{r_{\mathrm{like}}^{\alpha_{3}}+\beta_{3}}\ +\ \cdots$ </li>
<li>公式特点在于 — 不是用预估的分数，而是使用预估的排名。</li>
</ul>
</li>
<li><p>某电商的融分公式 </p>
<ul>
<li>电商的转化流程：曝光 → 点击 → 加购物车 → 付款 </li>
<li>模型预估：$p_{click}$、$p_{cart}$、$p_{pay}$ </li>
<li>最终融合分数： $p_{\mathrm{cilck}}^{\alpha_{1}}\ \times\ \ p_{\mathrm{cart}}^{\alpha_{2}}\ \times\ \ p_{\mathrm{pay}}^{\alpha_{3}}\ \times\ \mathrm{price}^{\alpha_{4}}$ </li>
<li>假如 $\alpha_{1}=\alpha_{2}=\alpha_{3}=\alpha_{4}=1$ 那该公式就是电商的营收，有明确的物理意义。</li>
</ul>
</li>
</ul>
<h3 id="Video-play-modeling"><a href="#Video-play-modeling" class="headerlink" title="Video play modeling"></a>Video play modeling</h3><ul>
<li><p>视频播放时长</p>
<ul>
<li><p>图文 vs. 视频 </p>
<ul>
<li>图文笔记排序的主要依据：点击、点赞、收藏、转发、评论…… </li>
<li>视频排序的依据还有播放时长和完播 ，对于视频来说，播放时长与完播的重要性大于点击。</li>
<li>直接用回归拟合播放时长效果不好。建议用 YouTube 的时长建模</li>
</ul>
<blockquote>
<p>参考：Paul Covington, Jay Adams, &amp; Emre Sargin. Deep Neural Networks for YouTube Recommendations. In RecSys, 2016.</p>
</blockquote>
</li>
</ul>
</li>
<li><p>模型：</p>
<ul>
<li><p>模型结构：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408301718769.png" srcset="/img/loading.gif" lazyload alt="image-20240830171820544" style="zoom:50%;">
</li>
<li><p>每个全连接层对应一个目标，假设最右边的输出对应播放时长 $z$， 对 $z$ 做 Sigmoid 变换得到 $p$，然后让 $p$ 拟合 $y$，它们的交叉熵作为损失函数。 $y$ 是我们定义的，其中的 $t$ 是用户实际观看时长，$t$ 越大则 $y$ 越大。目标就是要让p逼近于y。</p>
</li>
<li><p>观察公式发现</p>
<ul>
<li><p>如果 $p=y$，那么 $\exp{(z)}=t$ ，即 $\exp{(z)}$ 就是播放时长的预估值。</p>
</li>
<li><p>总结视频播放时长建模 </p>
<ul>
<li>把最后一个全连接层的输出记作 $z$。设 $p=sigmoid(z)$ </li>
<li>实际观测的播放时长记作 $t$。（如果没有点击，则 $t = 0$） </li>
<li>做训练：最小化交叉熵损失 $-\left({\frac{t}{1+t}}\cdot\log p+{\frac{1}{1+t}}\cdot\log(1-p)\right)$</li>
<li>实践中可以去掉分母 $1+t$，就等于给损失函数做加权，权重是<strong>播放时长</strong>。</li>
<li>做推理：把 $exp(z)$ 作为播放时长的预估。最终把 $exp(z)$ 作为融分公式中的一项。</li>
</ul>
<blockquote>
<p>说白了就是想训练一个能够预估视频播放时长的模型呗。然后这个指标就成为我们预估视频质量分数的一个指标！</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p>视频完播</p>
<ul>
<li><p>回归方法 </p>
<ul>
<li>视频长度 10 分钟，实际播放 4 分钟，则实际播放率为 𝑦 = 0.4</li>
<li>让预估播放率 $p$ 拟合 $y$：$\textstyle{loss}=y\cdot\log p+(1-y)\cdot\log(1-p)$</li>
<li>线上预估完播率，模型输出 $p$ = 0.73，意思是预计播放 73%</li>
</ul>
</li>
<li><p>二元分类方法 </p>
<ul>
<li>自定义完播指标，比如完播 80% </li>
<li>例：视频长度 10 分钟，播放 &gt; 8 分钟作为正样本，播放 &lt; 8 分钟作为负样本 </li>
<li>做二元分类训练模型：播放 &gt; 80% vs 播放 &lt; 80% </li>
<li>线上预估完播率，模型输出 $p$ = 0.73，意思是$\mathbb{P}(播放&gt;80%)=0.73$</li>
</ul>
</li>
<li><p>不能直接把完播率当作指标，视频越长，完播率越低。利好与短视频，对长视频不公平！</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408301726200.png" srcset="/img/loading.gif" lazyload alt="image-20240830172618942"></p>
<blockquote>
<p>拟合完播率，统一量纲，让它make sense！</p>
</blockquote>
<ul>
<li>预估完播率，然后做调整，$p_{finish} = \frac{预估完播率}{f(视频长度)}$，这里的$f$就是上面的拟合曲线。</li>
<li>可以把这里的$p_{finish}$作为融分公式中的一项。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Characteristics-of-ranking-models"><a href="#Characteristics-of-ranking-models" class="headerlink" title="Characteristics of ranking models"></a>Characteristics of ranking models</h3><ul>
<li><p>用户画像（User Profile） </p>
<ul>
<li>用户 ID（在召回、排序中做 embedding，用户 ID 本身不携带任何信息，但模型学到的 ID embedding 对召回和排序有很重要的影响） </li>
<li>人口统计学属性：性别、年龄</li>
<li>账号信息：新老、活跃度……（模型需要专门针对 新用户 和 低活跃 用户做优化）</li>
<li>感兴趣的类目、关键词、品牌（对排序很有用）</li>
</ul>
</li>
<li><p>物品画像（Item Profile） </p>
<ul>
<li>物品 ID（在召回、排序中做 embedding，非常重要） </li>
<li>发布时间（或者年龄，越久越没价值，尤其是时效性低的信息） </li>
<li>GeoHash（经纬度编码）、所在城市 </li>
<li>标题、类目、关键词、品牌…… （通常对于这些离散的信息做Embedding）</li>
<li>字数、图片数、视频清晰度、标签数…… （笔记自带属性，反映笔记的质量）</li>
<li>内容信息量、图片美学…… （算法打的分数，事先用人工标注的数据训练 NLP 和 CV 模型，然后用模型打分）</li>
</ul>
</li>
<li><p>用户统计特征 </p>
<ul>
<li>用户最近 30 天（7 天、1 天、1 小时）的曝光数、点击数、点赞数、收藏数……(划分各种时间粒度，可以反映用户的 实时、短期、中长期 兴趣)</li>
<li>按照笔记图文/视频分桶。（比如最近 7 天，该用户对图文笔记的点击率、对视频笔记的点击率。可以反映用户对两类笔记的偏好）</li>
<li>按照笔记类目分桶。（比如最近 30 天，用户对美妆笔记的点击率、对美食笔记的点击率、对科技数码笔记的点击率，反映用户对哪个类目更感兴趣。）</li>
</ul>
</li>
<li><p>笔记统计特征 </p>
<ul>
<li>笔记最近 30 天（7 天、1 天、1 小时）的曝光数、点击数、点赞数、收藏数……（划分时间粒度，可以提前发现哪些笔记过时了）</li>
<li>按照用户性别分桶、按照用户年龄分桶…… </li>
<li>作者特征：发布笔记数、粉丝数、消费指标（曝光数、点击数、点赞数、收藏数）</li>
</ul>
</li>
<li><p>场景特征（Context） </p>
<ul>
<li>用户定位 GeoHash（经纬度编码）、城市</li>
<li>当前时刻（分段，做 embedding）<ul>
<li>一个人在同一天不同时刻的兴趣是变化的。</li>
<li>而且可以反推用户是在上班路上、公司、家里</li>
</ul>
</li>
<li>是否是周末、是否是节假日 </li>
<li>手机品牌、手机型号、操作系统 <ul>
<li>安卓用户和苹果用户的 点击率、点赞率 等数据差异很大</li>
</ul>
</li>
</ul>
</li>
<li><p>特征处理 </p>
<ul>
<li>离散特征：做 embedding <ul>
<li>用户 ID、笔记 ID、作者 ID</li>
<li>类目、关键词、城市、手机品牌</li>
</ul>
</li>
<li>连续特征： 做分桶，变成离散特。年龄、笔记字数、视频长度。</li>
</ul>
</li>
<li><p>其他变换 </p>
<ul>
<li>曝光数、点击数、点赞数等数值具有长尾效应，做 $\log{(1+x)}$。</li>
<li>转化为点击率、点赞率等值，并做平滑。</li>
</ul>
</li>
<li><p>特征覆盖率</p>
<ul>
<li>很多特征无法覆盖 100% 样本 </li>
<li>例：很多用户不填年龄，因此用户年龄特征的覆盖率远小于 100% </li>
<li>例：很多用户设置隐私权限，APP 不能获得用户地理定位，因此场景特征有缺失 </li>
<li>提高特征覆盖率，可以让精排模型更准 </li>
<li>想各种办法提高特征覆盖率，并考虑特征缺失时默认值如何设置</li>
</ul>
</li>
<li><p>数据服务：</p>
<ul>
<li><p>三个数据源：用户画像（User Profile）、物品画像（Item Profile）、统计数据。三个数据源都存储在内存数据库中。</p>
</li>
<li><p>线上服务的时候，排序服务器会从三个数据源取回所需要的数据，然后把读取的数据做处理，作为特征喂给模型。模型就能够预估出点击率、点赞率等指标。</p>
</li>
<li><p>整体流程图：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409031945827.png" srcset="/img/loading.gif" lazyload alt="image-20240903194553567" style="zoom:50%;">
</li>
<li><p>用户画像数据库压力小（每次只读 1 个用户），物品画像数据库压力非常大（每次读几千个物品） 。工程实现时，用户画像中的特征可以很多很大，但尽量不往物品画像中塞很大的向量。</p>
</li>
<li><p>由于用户和物品画像较为静态，甚至可以把用户和物品画像缓存在排序服务器本地，加速读取 </p>
</li>
<li><p>收集了排序所需特征后，将特征打包发给 TF Serving，Tensorflow 给笔记打分并把分数返回排序服务器 </p>
</li>
<li><p>排序服务器依据融合的分数、多样性分数、业务规则等给笔记排序，并把排名最高的几十篇返回主服务器</p>
</li>
</ul>
</li>
</ul>
<h3 id="Rough-Sort"><a href="#Rough-Sort" class="headerlink" title="Rough Sort"></a>Rough Sort</h3><blockquote>
<p>上面这些精排用的多，下面讲讲粗排</p>
</blockquote>
<ul>
<li><p>粗排 vs 粗排：</p>
<ul>
<li><p>粗排：给几千篇笔记打分，单次推理代价必须小，预估的准确性不高。</p>
</li>
<li><p>精排：给几百篇笔记打分，单次推理代价很大，预估的准确性更高。</p>
</li>
</ul>
</li>
<li><p>模型简介：</p>
<ul>
<li><p>精排模型：</p>
<ul>
<li><p>模型结构：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409041540091.png" srcset="/img/loading.gif" lazyload alt="image-20240904154000877" style="zoom:50%;">
</li>
<li><p>直接对各种特征做concatenation，塞入shared bottom中，意思是它被多个任务共享。再接上多个头，得到每个指标。</p>
</li>
<li><p>开销主要在shared button，它很大，模型很复杂。</p>
</li>
<li><p>前期融合：先对所有特征做 concatenation，再输入神经网络。</p>
</li>
<li><p>线上推理代价大：如果有 $n$ 篇候选笔记，整个大模型要做 $n$ 次推理。</p>
</li>
</ul>
</li>
<li><p>双塔模型：</p>
<ul>
<li><p>模型结构：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409041543695.png" srcset="/img/loading.gif" lazyload alt="image-20240904154333541" style="zoom:50%;">
</li>
<li><p>物品塔在训练结束后，物品其实就可以直接过物品塔，然后存储到向量数据库中了。线上不需要用物品塔做计算。</p>
</li>
<li><p>线上只需要用到用户塔，每做一次推荐，用户塔只做一次推理，得到一个向量。而且本身塔的结构较为简单，计算代价很小，适合做召回。</p>
</li>
<li><p>后期融合：把用户、物品特征分别输入不同的神经网络，不对用户、物品特征做融合 。</p>
</li>
<li><p>线上计算量小： </p>
<ul>
<li>用户塔只需要做一次线上推理，计算用户表征 $\bold{a}$</li>
<li>物品表征 $\bold{b}$ 事先储存在向量数据库中，物品塔在线上不做推理</li>
</ul>
</li>
</ul>
</li>
<li><p>后期融合模型不如前期融合模型准确 </p>
<ul>
<li>预估准确性不如精排模型</li>
<li><strong>后期融合模型用于召回，前期融合模型用于精排</strong>。</li>
</ul>
</li>
</ul>
</li>
<li><p>粗排的三塔模型：</p>
<ul>
<li><p>小红书粗排用的三塔模型，效果介于双塔和精排之间。[参考文献](参考文献：Zhe Wang et al. COLD: Towards the Next Generation of Pre-Ranking System. In DLP-KDD, 2020.)</p>
</li>
<li><p>模型结构：<br><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409041625217.png" srcset="/img/loading.gif" lazyload alt="image-20240904162557057"></p>
</li>
<li><p>交叉特征：用户特征与物品特征做交叉。</p>
</li>
<li><p>对 3 个塔输出的向量做Concatenation 和 Cross（交叉）得到 1 个向量。</p>
</li>
<li><p>与前期融合在最开始对各类特征做融合不同，三塔模型在塔输出的位置做融合。</p>
</li>
<li><p>模型各个部分的计算量：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409041628118.png" srcset="/img/loading.gif" lazyload alt="image-20240904162827961" style="zoom:50%;">

<ul>
<li>通常来说，交叉塔必须足够小（通常只有一层，参数也小），速度够快。</li>
</ul>
</li>
<li><p>有 $n$ 个物品，模型上层需要做 $n$ 次推理，粗排推理的大部分计算量在模型上层：这个环节无法利用缓存节省计算量，三塔模型节省的是对物品推理的计算量。</p>
</li>
<li><p>三塔模型的推理 </p>
<ul>
<li>从多个数据源取特征：1 个用户的画像、统计特征。$n$ 个物品的画像、统计特征</li>
<li>用户塔：只做 1 次推理 </li>
<li>物品塔：未命中缓存时需要做推理 。（最坏情况下，需要做n次推理，但实际上缓存命中率非常高，99%都能命中缓存，不需要做推理。给几千个候选物品做粗排，物品塔只需要做几十次推理。）</li>
<li>交叉塔：必须做 $n$ 次推理 </li>
<li>上层网络做 $n$ 次推理，给 $n$ 个物品打分 </li>
<li><strong>粗排模型的设计理念就是尽量减少推理的计算量，使得模型可以线上对几千篇笔记打分</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Feature-Crossover"><a href="#Feature-Crossover" class="headerlink" title="Feature Crossover"></a>Feature Crossover</h2><h3 id="Factorized-Machine-FM"><a href="#Factorized-Machine-FM" class="headerlink" title="Factorized Machine (FM)"></a>Factorized Machine (FM)</h3><blockquote>
<p>FM 以前在召回和排序都很常用，现在用得少了，主要了解一下思想。特征交叉有必要，可以让模型变得很准确！</p>
</blockquote>
<ul>
<li><p>线性模型</p>
<ul>
<li>有 $d$ 个特征，记作 $\mathbf{x}=\left[x_{1},\cdot\cdot\cdot,x_{d}\right]$ </li>
<li>线性模型：$p\ =\ b\ +\textstyle{\sum_{i=1}^{d}}w_{i}x_{i}$ ，$b$ 是偏移项 bias</li>
<li>模型有 $d+1$ 个参数：${\bf w}=\left[{\bf w}<em>{1},\cdot\cdot\cdot,{\bf w}</em>{d}\right]$ 和 $b$ </li>
<li>预测是特征的加权和（只有加，没有乘） ，即特征之间没有交叉</li>
</ul>
</li>
<li><p>二阶交叉特征</p>
<ul>
<li><p>有 $d$ 个特征，记作 $\mathbf{x}=\left[x_{1},\cdot\cdot\cdot,x_{d}\right]$ </p>
</li>
<li><p>线性模型 + 二阶交叉特征： $p=b+\sum_{i=1}^{d},w_{i}x_{i}+\sum_{i=1}^{d}\sum_{j=i+1}^{d}u_{ij}x_ix_j$ ，公式中的 $x_ix_j$ 就是两个特征的交叉，$u_{ij}$ 是权重。（例如：房子大小和每平米价格，交叉明显对于价格的预估更准确！）</p>
</li>
<li><p>模型有 $O(d^2)$ 个参数 ，$d$ 较大时，参数数量过多，且容易出现过拟合。减少参数！</p>
</li>
<li><p>减少参数的方式：</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409051338394.png" srcset="/img/loading.gif" lazyload alt="image-20240905133843250"></p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409051340493.png" srcset="/img/loading.gif" lazyload alt="image-20240905134019447"></p>
<ul>
<li>Factorized Machine (FM)： $p=b+\sum_{i=1}^{d},w_{i}x_{i}+\sum_{i=1}^{d}\sum_{j=i+1}^{d}(\bold{v}_i^T\bold{v}_j)x_ix_j$ </li>
<li>使用矩阵 $\bold{V}\bold{V}^T$ 来近似矩阵 $\bold{U}$</li>
<li>FM 模型有 $O(kd)$ 个参数（$k\ll d$，是一个超参数），参数量从 $O(d^2)$ 降低到了 $O(kd)$ 。</li>
</ul>
</li>
</ul>
</li>
<li><p>总结：</p>
<ul>
<li>FM 是线性模型的替代品，能用线性回归、逻辑回归的场景，都可以用 FM。</li>
<li>FM 使用二阶交叉特征，表达能力比线性模型更强</li>
<li>通过做近似 $u_{ij}\approx\bold{v}_i^T\bold{v}_j$，FM 把二阶交叉权重的数量从 $O(d^2)$ 降低到 $O(kd)$</li>
<li>早期的推荐系统通常使用LR，在LR中用FM做特征交叉，流行了很长一段时间。但是现在工业界里面，FM的这种做法已经过时了。小红书里面早都下掉了FM。</li>
</ul>
</li>
</ul>
<blockquote>
<p>参考文献：Steffen Rendle.Factorization machines. In ICDM, 2010.</p>
</blockquote>
<h3 id="Deep-amp-Cross-Networks-DCN"><a href="#Deep-amp-Cross-Networks-DCN" class="headerlink" title="Deep &amp; Cross Networks (DCN)"></a>Deep &amp; Cross Networks (DCN)</h3><blockquote>
<p>Deep &amp; Cross Networks (DCN) 译作“深度交叉网络”，可以用于召回双塔模型、粗排三塔模型、精排模型。DCN 由一个深度网络和一个交叉网络组成，交叉网络的基本组成单元是交叉层 (Cross Layer)。这节课最重点的部分就是交叉层。</p>
</blockquote>
<ul>
<li><p>双塔/三塔的每一个塔，精排多目标排序模型的shared bottom，排序MMoE模型的每一个神经专家网络。都可以使用任意的网络结构，最简单就是全连接，但是也可以用更加复杂/有效的网络结构。</p>
</li>
<li><p>交叉层（Cross Layer）</p>
<ul>
<li><p>网络结构：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409051351856.png" srcset="/img/loading.gif" lazyload alt="image-20240905135123684" style="zoom:50%;">

<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409051354349.png" srcset="/img/loading.gif" lazyload alt="image-20240905135414247" style="zoom:50%;">
</li>
<li><p>Hadmamard Product是逐元素相乘，要求矩阵的shape完全一致！有点类似于 ResNet 中的 跳跃链接 Skip Connection，防止梯度消失。每个交叉层的输入和输出都是向量，并且形状相同。</p>
</li>
</ul>
</li>
<li><p>交叉网络（Cross Network）</p>
<ul>
<li><p>交叉网络由多个交叉层堆叠而来：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409051356638.png" srcset="/img/loading.gif" lazyload alt="image-20240905135628439" style="zoom:50%;">
</li>
<li><p>参考文献：这节课介绍的是 Cross Network V2，于2021年提出。老版本的 Cross Network是在2017年提出的。参考文献：</p>
<ul>
<li>Ruoxi Wang et al. DCN V2: Improved Deep &amp; Cross Network and Practical Lessons for Web-scale Learning to Rank Systems. In WWW, 2021.</li>
<li>Ruoxi Wang et al. Deep &amp; Cross Network for Ad Click Predictions. In ADKDD, 2017.</li>
</ul>
</li>
</ul>
</li>
<li><p>深度交叉网络DCN</p>
<ul>
<li><p>网络结构：</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409051359502.png" srcset="/img/loading.gif" lazyload alt="image-20240905135929323"></p>
</li>
<li><p>把 全连接网络、交叉网络、全连接层 拼到一起，就是 深度交叉网络 DCN。</p>
</li>
<li><p>比简单的全连接神经网络效果更好，被业界普遍接受，可以用于召回或者是排序。</p>
</li>
<li><p>召回、排序模型中的各种 塔、神经网络、专家网络 都可以是 DCN。</p>
</li>
</ul>
</li>
</ul>
<h3 id="LHUC-PPNet"><a href="#LHUC-PPNet" class="headerlink" title="LHUC(PPNet)"></a>LHUC(PPNet)</h3><blockquote>
<p>LHUC 这种神经网络结构，可以用于<strong>精排</strong>。LHUC 的起源是语音识别，后来被应用到推荐系统，快手将其称为 PPNet，现在已经在业界广泛落地。视频中遗漏一个细节：将LHUC用于推荐系统，门控神经网络（2 x sigmoid）的梯度不要传递到用户ID embedding特征，需要对其做 stop gradient。</p>
</blockquote>
<ul>
<li><p>参考文献：</p>
<ul>
<li>Pawel Swietojanski, Jinyu Li, &amp; Steve Renals. Learning hidden unit contributions for unsupervised acoustic model adaptation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016.</li>
<li>快手落地万亿参数推荐精排模型，2021。链接：<a target="_blank" rel="noopener" href="https://ai.51cto.com/art/202102/644214.html">https://ai.51cto.com/art/202102/644214.html</a></li>
</ul>
</li>
<li><p>说话者的特征：例如说话者的 ID，年龄，性别 等</p>
</li>
<li><p>处理说话者的特征的神经网络</p>
<ul>
<li><p>网络结构：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409051413260.png" srcset="/img/loading.gif" lazyload alt="image-20240905141342199" style="zoom:50%;">
</li>
<li><p>内部包含多个全连接层</p>
</li>
<li><p>下半部分的神经网络是由多个全连接层组成，激活函数是 Sigmoid 乘以 2，Sigmoid作用于每个元素上，Sigmoid输出的每个元素都介于[0, 2]。部分值大于 1，部分值小于 1，就可以拿去作为放缩因子，放大某些特征，缩小某些特征，实现个性化。</p>
</li>
</ul>
</li>
<li><p>在推荐系统中：</p>
<ul>
<li><p>模型结构：</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409051424520.png" srcset="/img/loading.gif" lazyload alt="image-20240905142412351"></p>
</li>
<li><p>只是简单的把输入的特征活成了用户特征&amp;物品特征。</p>
</li>
</ul>
</li>
</ul>
<h3 id="SENet-amp-Bilinear-Cross"><a href="#SENet-amp-Bilinear-Cross" class="headerlink" title="SENet &amp; Bilinear Cross"></a>SENet &amp; Bilinear Cross</h3><blockquote>
<p>参考文献：</p>
<ol>
<li>Jie Hu, Li Shen, and Gang Sun. Squeeze-and-Excitation Networks. In CVPR, 2018.</li>
<li>Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction. In RecSys, 2019.</li>
</ol>
</blockquote>
<ul>
<li><p>SENet</p>
<ul>
<li><p>首先，把所有的m维离散的特征，做Embedding（假设dimension为k）会得到很多的向量(m x k)，接下来SENet如何对这些Embedding做操作呢？</p>
</li>
<li><p>第一部分：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409051437184.png" srcset="/img/loading.gif" lazyload alt="image-20240905143711999" style="zoom:50%;">

<ul>
<li>$r$ 是压缩比列，设置为大于 1 的数。再用FC + Sigmoid，恢复出m维的向量。这个向量的元素都介于0和1之间。</li>
<li>原文中是按照特征通道的数量来进行压缩的，压缩后每个多维的特征通道变成一个实数，这个实数某种程度上具有全局的感受野。</li>
<li>给我的感觉其实就是attention哈哈哈哈哈，本质就是提取出里面的特征，再去算出来对每一个部分的“重视程度”。</li>
</ul>
</li>
<li><p>第二部分：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409051446616.png" srcset="/img/loading.gif" lazyload alt="image-20240905144602553" style="zoom:50%;">

<ul>
<li>每个特征乘上对应这个特征的权重，拿到一个m x k的output。</li>
<li>使用右侧的 $m$ 维向量对左边矩阵的行做加权，赋予原始的Embedding对应的权重。</li>
<li>例如：学习发现物品 ID 对任务的重要性不高，就给物品 ID embedding 降权。</li>
</ul>
</li>
<li><p>整体框架：</p>
<ul>
<li><p>其实对于这m个特征，他们的Embedding向量维度可以不同，不会影响SENet。</p>
</li>
<li><p>框架图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409051449747.png" srcset="/img/loading.gif" lazyload alt="image-20240905144934594"></p>
</li>
<li><p>红色和蓝色的向量形状完全相同。</p>
</li>
</ul>
</li>
<li><p>$m$ 个离散特征的 Embedding 向量维度可以不同。</p>
</li>
<li><p>SENet 本质是对离散特征做 field-wise 加权。Field 举例： </p>
<ul>
<li>用户 ID Embedding 是 64 维向量。64 个元素算一个 field，获得相同的权重。</li>
<li>对于每一个Embedding，都对应了一个val（m x 1）权重，这个权重会作用于这个Embedding的每一个元素上，因此是field-wise的！</li>
</ul>
</li>
<li><p>如果有 $m$ 个 fields，那么权重向量是 $m$ 维。</p>
</li>
</ul>
</li>
<li><p>Field间特征交叉</p>
<ul>
<li><p>最简单的特征交叉方法：</p>
<ul>
<li>内积：假设m个fields，m x m，共得到$m ^ 2$个实数。$f_{ij} = x_{i}^{T} \times x_j$，还好，因为m就几十个。顶多产出几百个/几千个数字。</li>
<li>哈达玛乘积：假设m个fields，m x m，共得到$m ^ 2$个向量。$f_{ij} = x_{i} \bigodot x_j$，不太行，m就几十个。会产出几百个/几千个向量，这就有点夸张了。</li>
<li>两者都需要每个特征的Embedding向量维度一样，都是k维向量，不然算不了。</li>
<li>像哈达玛这种，就不能这么搞了，参数量太大。如果要用它做特征交叉，必须人工选择 tier 做交叉。</li>
</ul>
</li>
<li><p>Bilinear Cross（内积）</p>
<ul>
<li><p>更加先进的特征交叉方法。有内积和哈达玛乘积两种方式。</p>
</li>
<li><p>内积方式：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409051510780.png" srcset="/img/loading.gif" lazyload alt="image-20240905151005604" style="zoom:50%;">
</li>
<li><p>不要求特征形状相同 </p>
</li>
<li><p>$m$ fields → $m^2$ 个交叉特征（实数） ，$m$ fields → $m^2/2$ 个参数矩阵。</p>
</li>
<li><p>参数矩阵过多，需要人工指定某些相关且重要的特征做交叉，不能对所有特征做两两交叉。</p>
</li>
</ul>
</li>
<li><p>Bilinear Cross（哈达玛乘积）</p>
<ul>
<li><p>哈达玛方式，和上面累死，只不过把点乘那里改为了哈达玛乘积：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409051514787.png" srcset="/img/loading.gif" lazyload alt="image-20240905151418742" style="zoom:50%;">
</li>
<li><p>$m$ fields → $m^2$ 个向量</p>
</li>
<li><p>在实践中依然需要人工指定某些特征做交叉</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>小结 </p>
<ul>
<li>SENet 对离散特征做 field-wise 加权 </li>
<li>Field 间特征交叉： <ul>
<li>向量内积</li>
<li>哈达玛乘积</li>
<li>Bilinear Cross</li>
</ul>
</li>
</ul>
</li>
<li><p>FiBiNet</p>
<ul>
<li><p>模型结构</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409051519034.png" srcset="/img/loading.gif" lazyload alt="image-20240905151930979" style="zoom:50%;">
</li>
<li><p>以前我们就直接把离散特征的 Embedding 拼接后再与连续特征拼接 </p>
</li>
<li><p>FiBiNet 多做了一些工作 </p>
<ul>
<li>对离散特征的 Embedding 做 Bilinear Cross，得到交叉特征并拼接为 1 个向量 </li>
<li>用 SENet 对 Embedding 做加权，再对这些向量做 Biliner Cross，得到交叉特征并拼接为 1 个向量。<strong>王老师个人认为这里的两条路径中，下面那条SENet -&gt; Biliner Cross 多余了，小红书没有用，而是直接做了拼接。</strong></li>
</ul>
</li>
<li><p>小红书借鉴了 FiBiNet </p>
<ul>
<li>SENet 和 Bilinear Cross 在精排模型上确实有效</li>
<li>模型结构以及 Bilinear Cross 的实现都与原文区别很大</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Last-N"><a href="#Last-N" class="headerlink" title="Last N"></a>Last N</h2><h3 id="User-Last-N-Modeling"><a href="#User-Last-N-Modeling" class="headerlink" title="User Last N Modeling"></a>User Last N Modeling</h3><ul>
<li><p>用户行为序列简称为 LastN，即用户最后交互的 $n$ 个物品，LastN 可以反映用户对什么样的物品感兴趣。</p>
</li>
<li><p>适用于召回双塔模型、粗排三塔模型、精排模型，都可以用Last N。很有效！用于召回和排序，指标都会大涨！</p>
</li>
<li><p>本节课后面重点关注 用户特征 中的 LastN 行为序列。Last N记录是用户最近交互过的N个物品的ID。</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409081934720.png" srcset="/img/loading.gif" lazyload alt="image-20240908193402534" style="zoom:50%;">

<ul>
<li><p>LastN 特征 </p>
<ul>
<li>LastN：用户最近的 $n$ 次交互（点击、点赞等）的物品 ID</li>
<li>对 LastN 物品 ID 做 embedding，得到 $n$ 个向量</li>
<li>把 $n$ 个向量取平均，作为用户的一种特征/</li>
<li>适用于召回双塔模型、粗排三塔模型、精排模型</li>
</ul>
<blockquote>
<p>参考文献：Covington, Adams, and Sargin. Deep neural networks for YouTube recommendations. In ACM Conference on Recommender Systems, 2016.</p>
</blockquote>
</li>
</ul>
</li>
<li><p>小红书召回实践</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409081937230.png" srcset="/img/loading.gif" lazyload alt="image-20240908193748179" style="zoom:50%;">

<ul>
<li>取平均是早期的用法，现在也很常用，效果不错。现在更多的是用 Attention，效果更好，计算量更大。</li>
<li>上面得到的多个向量拼接起来，作为一种用户特征，传到召回或排序模型中。</li>
<li>实际用Last N的时候，Embedding 不只有物品 ID，还会有物品类别等特征，把ID Embedding和其他的Em bedding拼在一起，效果更好。</li>
</ul>
</li>
</ul>
<h3 id="DIN-Model-Deep-Interest-Network"><a href="#DIN-Model-Deep-Interest-Network" class="headerlink" title="DIN Model(Deep Interest Network)"></a>DIN Model(Deep Interest Network)</h3><ul>
<li><p>DIN 模型</p>
<ul>
<li>DIN 用加权平均代替平均，即注意力机制（attention）</li>
<li>权重：候选物品与用户 LastN 物品的相似度。哪个Last N和候选物品更相似，它的权重就越高。</li>
</ul>
<blockquote>
<p>参考文献：Zhou et al. Deep interest network for click-through rate prediction. In KDD, 2018.</p>
</blockquote>
</li>
<li><p>计算过程：</p>
<ul>
<li><p>候选物品：例如粗排选出了 500 个物品，那这 500 个就是精排的候选物品</p>
</li>
<li><p>计算相似度的方法很多，如 内积 和 余弦相似度 等</p>
</li>
<li><p>DIN 模型总结</p>
<ol>
<li>计算相似度，可能用个cos之类的东西。每个Last N中的向量，都和q单独算一个。</li>
</ol>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409081944991.png" srcset="/img/loading.gif" lazyload alt="image-20240908194443937" style="zoom:50%;">

<ol start="2">
<li>相乘再相加：</li>
</ol>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409081946995.png" srcset="/img/loading.gif" lazyload alt="image-20240908194635811" style="zoom:50%;">
</li>
<li><p>总结</p>
<ul>
<li>对于某候选物品，计算它与用户 LastN 物品的相似度</li>
<li>以相似度为权重，求用户 LastN 物品向量的加权和，结果是一个向量</li>
<li>把得到的向量<strong>作为一种用户特征</strong>，输入排序模型，预估（用户，候选物品）的点击率、点赞率等指标</li>
<li>本质是注意力机制（attention）</li>
</ul>
</li>
</ul>
</li>
<li><p>注意力机制：</p>
<ul>
<li><p>可以和Transformer的q, k, v对应上的昂！</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409081948390.png" srcset="/img/loading.gif" lazyload alt="image-20240908194801227" style="zoom:50%;">
</li>
<li><p>简单平均 v.s. 注意力机制：</p>
<ul>
<li>简单平均 和 注意力机制 都适用于精排模型</li>
<li>简单平均适用于双塔模型、三塔模型<ul>
<li>简单平均只需要用到 LastN，属于用户自身的特征，与候选物品无关</li>
<li>把 LastN 向量的平均作为用户塔的输入</li>
</ul>
</li>
<li>注意力机制不适用于双塔模型、三塔模型<ul>
<li>注意力机制 需要用到 LastN + 候选物品</li>
<li>用户塔看不到候选物品，不能把 注意力机制 用在用户塔</li>
<li>它是作为用户特征的，所以只会出现在用户这边。用双塔也就为了user_emb和item_emb，放到item tower没法跟user_tower的lastN做交叉。感觉双塔/三塔的后期融合，就杜绝了user和item在特征早期，做交叉/注意力机制的可能性。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="SIM-Model-Search-based-User-Interest-Modeling"><a href="#SIM-Model-Search-based-User-Interest-Modeling" class="headerlink" title="SIM Model(Search-based User Interest Modeling)"></a>SIM Model(Search-based User Interest Modeling)</h3><blockquote>
<p>SIM 模型的主要目的是保留用户的长期兴趣。</p>
</blockquote>
<ul>
<li><p>DIN 模型 </p>
<ul>
<li>计算用户 LastN 向量的加权平均</li>
<li>权重是候选物品与 LastN 物品的相似度</li>
</ul>
</li>
<li><p>DIN 模型的缺点 </p>
<ul>
<li>注意力层的计算量 正比于 $n$（用户行为序列的长度） </li>
<li>只能记录最近几百个物品，否则计算量太大 </li>
<li>缺点：关注短期兴趣，遗忘长期兴趣。王老师的同事做过实验，增长记录的行为序列，可以显著提升推荐系统的所有指标，但增加的计算量太大了</li>
</ul>
<blockquote>
<p>参考文献：Zhou et al. Deep interest network for click-through rate prediction. In KDD, 2018.</p>
</blockquote>
</li>
<li><p>如何改进 DIN？ </p>
<ul>
<li>目标：保留用户长期行为序列（$n$ 很大），而且计算量不会过大 </li>
<li>改进 DIN： <ul>
<li>DIN 对 LastN 向量做加权平均，权重是相似度</li>
<li>如果某 LastN 物品与候选物品差异很大，则权重接近零</li>
<li>快速排除掉与候选物品无关的 LastN 物品，降低注意力层的计算量</li>
</ul>
</li>
</ul>
</li>
<li><p>SIM模型：</p>
<ul>
<li>保留用户长期行为记录，$n$ 的大小可以是几千</li>
<li>对于每个候选物品，在用户 LastN 记录中做快速查找，找到 $k$ 个相似物品。(N = 1000，k = 100，从最近的1000个交互中，快速找到最相关的100个)</li>
<li>把 LastN 变成 TopK，然后输入到注意力层，k很小！就很厉害！</li>
<li>SIM 模型减小计算量（从 $n$ 降到 $k$）</li>
</ul>
<blockquote>
<p>参考文献：Qi et al. Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction. In CIKM, 2020.</p>
</blockquote>
</li>
<li><p>SIM步骤</p>
<ul>
<li><p>第一步：查找</p>
<ul>
<li>方法一：Hard Search —— 根据规则做筛选。<ul>
<li>根据候选物品的类目，保留 LastN 物品中类目相同的</li>
<li>简单，快速，无需训练</li>
</ul>
</li>
<li>方法二：Soft Search <ul>
<li>把物品做 embedding，变成向量</li>
<li>把候选物品向量作为 query，做 $k$ 近邻查找，保留 LastN 物品中最接近的 $k$ 个</li>
</ul>
</li>
<li>论文表明：Soft效果更好，AOC更高。编程实现更复杂，计算量也更高。</li>
<li>也要看基建，如果基建不行，Hard就可以了。</li>
</ul>
</li>
<li><p>第二步：注意力机制</p>
<ul>
<li><p>和DIM其实结构上没有什么大的不同。</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409082009835.png" srcset="/img/loading.gif" lazyload alt="image-20240908200949779" style="zoom:50%;">

<blockquote>
<p>Top N变成Top K了哈</p>
</blockquote>
</li>
<li><p>Trick：使用时间信息</p>
<ul>
<li>用户与某个 LastN 物品的交互时刻距今为 $\delta$ </li>
<li>对 $\delta$ 做离散化，再做 embedding，变成向量 $\bold{d}$ ，例如把时间离散为 1 天内、7 天内、30 天内、一年、一年以上。</li>
<li>把两个向量做 concatenation，表征一个 LastN 物品 <ul>
<li>向量 $\bold{x}$ 是物品 embedding</li>
<li>向量 $\bold{d}$ 是时间的 embedding</li>
</ul>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409082011897.png" srcset="/img/loading.gif" lazyload alt="image-20240908201157718" style="zoom:50%;"></li>
</ul>
</li>
</ul>
</li>
<li><p>为什么SIM使用时间信息？</p>
<ul>
<li><p>DIN 的序列短，记录用户近期行为</p>
</li>
<li><p>SIM 的序列长，记录用户长期行为</p>
</li>
<li><p>时间越久远，重要性越低</p>
</li>
<li><p>论文表明：使用时间信息可以带来<strong>显著</strong>的提升。</p>
</li>
</ul>
</li>
<li><p>结论：</p>
<ul>
<li>长序列（长期兴趣）优于短序列（近期兴趣）</li>
<li>注意力机制优于简单平均</li>
<li>Soft search 还是 hard search？取决于工程基建</li>
<li>使用时间信息有提升</li>
</ul>
</li>
<li><p>Ideas</p>
<ul>
<li>感觉就是Top n -&gt; Top k，多加了一层漏斗哈哈哈哈，别的没啥区别。</li>
<li>看到弹幕有人说，这个时间序列加上去，就和大模型里面，加上了Positional Embedding一样，这里的Position就是距离当前时间的位置，妙！新的思考方式，时空都可以抽象为Positional Embedding啊！</li>
</ul>
</li>
</ul>
<h2 id="Re-ranking"><a href="#Re-ranking" class="headerlink" title="Re-ranking"></a>Re-ranking</h2><h3 id="Diversity-in-recommendation-systems"><a href="#Diversity-in-recommendation-systems" class="headerlink" title="Diversity in recommendation systems"></a>Diversity in recommendation systems</h3><blockquote>
<p>多样性：推荐给用户的物品两两间不相似，则说明推荐有多样性。</p>
</blockquote>
<ul>
<li><p>物品相似度的度量</p>
<ul>
<li>相似性的度量 <ul>
<li>基于物品属性标签：类目、品牌、关键词……</li>
<li>基于物品向量表征 <ul>
<li>用召回的双塔模型学到的物品向量（不好）</li>
<li>基于内容的向量表征（好），用CV/NLP模型，提取文字/图片的特征向量。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>基于物品属性标签 ：</p>
<ul>
<li>物品属性标签：类目、品牌、关键词……，标签通常是通过 CV 或 NLP 算法通过图文推算的，不一定准确。</li>
<li>根据 <strong>一级类目、二级类目、品牌</strong> 计算相似度 <ul>
<li>物品 $i$：美妆、彩妆、香奈儿</li>
<li>物品 $j$：美妆、香水、香奈儿</li>
<li>相似度：$\rm sim_1(i,j)=1，\rm sim_2(i,j)=0，\rm sim_3(i,j)=1$</li>
<li>加权和，得到相似度总分，权重根据经验设置。</li>
</ul>
</li>
</ul>
</li>
<li><p>双塔模型的物品向量表征：</p>
<ul>
<li><p>结构：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409091526162.png" srcset="/img/loading.gif" lazyload alt="image-20240909152642045" style="zoom:50%;">
</li>
<li><p>多样性问题上，我们只需要物品塔的输出。因为我们是对于物品的相似度做判别。如果相似，则物品向量表征的内积/余弦相似度比较大。</p>
</li>
<li><p>可以将物品塔的表征用在多样性问题上！但是！效果不太好。原因：</p>
<ul>
<li>推荐系统的头部效应明显，新物品和长尾物品的曝光少。</li>
<li>双塔模型学不好 新物品 和 长尾物品 的向量表征。</li>
</ul>
</li>
</ul>
</li>
<li><p>基于图文内容的物品表征：</p>
<ul>
<li><p>最好的办法！以小红书的笔记为例子，用CNN提取图片的向量，用Bert提取文字的特征拿到另外一个向量。这俩拼起来，就是图文笔记的表征。</p>
</li>
<li><p>Bert/CNN怎么训练呢？公开的对于小红书效果不好，如果用内部的数据做训练，还要人工标注，比较麻烦。</p>
</li>
<li><p>CLIP是当前公认最有效的预训练方法。</p>
<ul>
<li>思想： 对于图片，文本二元组，预测图文是否匹配。</li>
<li>优势：无需人工标注。小红书的笔记天然包含图片 + 文字，大部分笔记图文相关</li>
<li>做预训练时：同一篇笔记的图文 作为正样本，它们的向量应该高度相似；来自不同笔记的图文作为负样本。</li>
</ul>
<blockquote>
<p>参考文献：Radford et al. Learning transferable visual models from natural language supervision. In ICML, 2021.</p>
</blockquote>
</li>
<li><p>基于图文内容的物品表征 </p>
<ul>
<li>一个 batch 内有 $m$ 对正样本</li>
<li>一张图片和 $m-1$ 条文本组成负样本，这个 batch 内一共有 $m(m-1)$ 对负样本。</li>
</ul>
</li>
</ul>
</li>
<li><p>提升多样性的方法：</p>
<ul>
<li><p>粗排和精排用多目标模型对物品做 pointwise打分，就打分，不考虑物品间的关联。</p>
</li>
<li><p>对于物品 $i$，模型输出点击率、交互率的预估，融合成分数 $reward _i$ </p>
</li>
<li><p>$reward _i$ 表示用户对物品 $i$ 的兴趣，即物品本身对用户的价值。</p>
</li>
<li><p>粗排/精排：给定 $n$ 个候选物品，排序模型打分 ${\rm{reward}} _i,…,{\rm{reward}} _n$。</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409101717773.png" srcset="/img/loading.gif" lazyload alt="image-20240910171735640" style="zoom:50%;">
</li>
<li><p>后处理：从 $n$ 个候选物品中选出 $k$ 个，既要它们的总分高，也需要它们有多样性。</p>
</li>
<li><p>增加多样性可以显著提升推荐系统指标（尤其是时长、留存率），所以工业界在后处理阶段，都是使用多样性算法。</p>
</li>
<li><p>精排的后处理通常被称为 — 重排，它决定n个物品中，哪k个曝光，以及展示给用户的顺序。粗排的后处理往往被大家忽视，也需要多样性算法，而不是简单的从几千个物品中选reward最高的几百个，也能提升业务指标！优化粗排之后在小红书获得了很大的收益，有意义的！</p>
</li>
</ul>
</li>
</ul>
<h3 id="Maximal-Marginal-Relevance-MMR"><a href="#Maximal-Marginal-Relevance-MMR" class="headerlink" title="Maximal Marginal Relevance (MMR)"></a>Maximal Marginal Relevance (MMR)</h3><blockquote>
<p>最早被用到搜索排序，然后才被用在推荐排序，沿用了MMR这个名字</p>
</blockquote>
<ul>
<li><p>多样性</p>
<ul>
<li>精排给 $n$ 个候选物品打分，融合之后的分数为 $\rm{reward} _i,…,\rm{reward} _n$，n的大小一般是几百</li>
<li>把第 $i$ 和 $j$ 个物品的相似度记作 $\rm sim(i,j)$</li>
<li>重排：从 $n$ 个物品中选出 $k$ 个，既要有高精排分数，也要有多样性</li>
</ul>
</li>
<li><p>计算集合 ${\mathcal{R}}$ 中每个物品 $i$ 的 Marginal Relevance 分数： ${{\mathrm{MR}_{i}}}={{\theta\cdot\mathrm{reward}_{i}-(1-\theta)\cdot\max_{j\in {\mathcal{S}}}{sim}(i,j)}}${#</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409111653733.png" srcset="/img/loading.gif" lazyload alt="image-20240911165341380" style="zoom:50%;">

<ul>
<li>$reward_i$：物品 $i$ 的精排分数 </li>
<li>$\max_{j\in \mathcal{S}} sim(i,j)$：物品 $i$ 的多样性分数 <ul>
<li>物品 $i$ 尚未选中，而物品 $j\in \mathcal{S}$ 都是已选中物品</li>
<li>若物品 $i$ 与已选中的某个物品 $j$ 相似度比较大，则起到抑制作用</li>
</ul>
</li>
<li>theta属于0,1之间的参数，平衡物品的价值和多样性。theta越大，则reward更重要，越小，则多样性更重要。</li>
</ul>
</li>
<li><p>Maximal Marginal Relevance (MMR)： ${\rm argmax}_{i\in {\mathcal{R}}} {\rm MR}_i$ </p>
<ul>
<li>每一轮从集合 $\mathcal{R}$ 中选择 $ {\rm MR}$ 分数最高的物品 $i$，然后将其移入集合 $\mathcal{S}$</li>
</ul>
</li>
<li><p>概括 MMR 算法流程： </p>
<ul>
<li>初始化：已选中的物品 ${\mathcal{S}}$ 初始化为空集，未选中的物品 ${\mathcal{R}}$ 初始化为全集 ${ 1,…,n}$ </li>
<li>选择精排分数 $\rm{reward} _i$ 最高的物品，从集合 ${\mathcal{R}}$ 移到 ${\mathcal{S}}$ </li>
<li>做 $k-1$ 轮循环： <ul>
<li>计算集合 ${\mathcal{R}}$ 中所有物品的分数 $&lt;!–swig￼4–&gt;}$</li>
<li>选出分数最高的物品，将其从 ${\mathcal{R}}$ 移到 ${\mathcal{S}}$</li>
</ul>
</li>
</ul>
</li>
<li><p>滑动窗口：</p>
<ul>
<li><p>MMR： $argmax_{i\in {\mathcal{R}}} \{ {{\theta\cdot\mathrm{reward}_{i}-(1-\theta)\cdot\max_{j\in {\mathcal{S}}}sim(i,j)}}\}$ {#</p>
<ul>
<li>缺点：已选中的物品越多（即集合 ${\mathcal{S}}$ 越大），越难找出物品 $i\in{\mathcal{R}}$，使得 $i$ 与 ${\mathcal{S}}$ 中的物品都不相似。</li>
<li>设 $\rm sim$ 的取值范围是 [0, 1]。当 ${\mathcal{S}}$ 很大时，多样性分数 $\max_{j\in \mathcal{S}}\rm sim(i,j)$ 总是约等于 1，导致 MMR 算法失效</li>
</ul>
</li>
<li><p>解决方案：设置一个滑动窗口 ${\mathcal{W}}$，比如最近选中的 10 个物品，用 ${\mathcal{W}}$ 代替 MMR 公式中的 ${\mathcal{S}}$ </p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409111707155.png" srcset="/img/loading.gif" lazyload alt="image-20240911170754956" style="zoom:50%;">
</li>
<li><p>标准 MMR：$argmax_{i\in {\mathcal{R}}} \{ {{\theta\cdot\mathrm{reward}_{i}-(1-\theta)\cdot\max_{j\in {\mathcal{S}}}sim(i,j)}}\}$ {#</p>
</li>
<li><p>用滑动窗口：$argmax_{i\in {\mathcal{R}}} \{ {{\theta\cdot\mathrm{reward}_{i}-(1-\theta)\cdot\max_{j\in {\mathcal{W}}}sim(i,j)}}\}$ {#</p>
</li>
<li><p>用滑动窗口的可解释性：给用户曝光的连续物品应该不相似，但没必要最新的物品和 30 个之前的物品还不相似。</p>
</li>
<li><p>没必要嘛，看到30个之后，早都忘了之前的了。核心想法：远的可以相似，但是近的不行，要有较大的差异。否则用户可以感知到多样性不好。</p>
</li>
</ul>
</li>
<li><p>总结：</p>
<ul>
<li>MMR 使用在精排的后处理（重排）阶段</li>
<li>根据精排分数和多样性分数给候选物品排序</li>
<li>MMR 决定了物品的最终曝光顺序</li>
<li>实际应用中通常带滑动窗口，这样比标准 MMR 效果更好</li>
</ul>
</li>
</ul>
<h3 id="Re-ranking-Rules"><a href="#Re-ranking-Rules" class="headerlink" title="Re-ranking Rules"></a>Re-ranking Rules</h3><blockquote>
<p>推荐系统有很多业务规则，比如不能连续出多篇某种类型的物品、某两种类型的物品笔记间隔多少。这些业务规则应用在重排阶段，可以与 MMR、DPP 等多样性算法相结合。</p>
</blockquote>
<ul>
<li><p>简介</p>
<ul>
<li><p>工业界的推荐系统一般有很多业务规则，这些规则通常是为了保护用户体验，做重排时这些规则必须被满足</p>
</li>
<li><p>下面举例重排中的部分规则，以及这些规则与 MMR 相结合</p>
</li>
<li><p><strong>规则的优先级高于多样性算法</strong></p>
</li>
</ul>
</li>
<li><p>重排的规则：</p>
<ul>
<li><p>规则：最多连续出现 $k$ 篇某种笔记 </p>
<ul>
<li>小红书推荐系统的物品分为图文笔记、视频笔记</li>
<li>最多连续出现 $k=5$ 篇图文笔记，最多连续出现 $k=5$ 篇视频笔记</li>
<li>如果排 $i$ 到 $i+4$ 的全都是图文笔记，那么排在 $i+5$ 的必须是视频笔记</li>
</ul>
</li>
<li><p>规则：每 $k$ 篇笔记最多出现 1 篇某种笔记 </p>
<ul>
<li>运营推广笔记的精排分会乘以大于 1 的系数（boost），帮助笔记获得更多曝光</li>
<li>为了防止 boost 影响体验，限制每 $k=9$ 篇笔记最多出现 1 篇运营推广笔记</li>
<li>如果排第 $i$ 位的是运营推广笔记，那么排 $i+1$ 到 $i+8$ 的不能是运营推广笔记</li>
</ul>
</li>
<li><p>规则：前 $t$ 篇笔记最多出现 $k$ 篇某种笔记 </p>
<ul>
<li>排名前 $t$ 篇笔记最容易被看到，对用户体验最重要（小红书的 top 4 为首屏） </li>
<li>小红书推荐系统有带电商卡片的笔记，过多可能会影响体验 </li>
<li>前 $t=1$ 篇笔记最多出现 $k=0$ 篇带电商卡片的笔记。即排名第一的笔记，不能是电商推广</li>
<li>前 $t=4$ 篇笔记最多出现 $k=1$ 篇带电商卡片的笔记</li>
</ul>
<blockquote>
<p>注：不是小红书的真实数据</p>
</blockquote>
</li>
</ul>
</li>
<li><p>MMR + 重排规则</p>
<ul>
<li>MMR 每一轮选出一个物品，重排要结合 MMR 与规则，在满足规则的前提下最大化 MR。</li>
<li>每一轮要先用规则，从备选集合$R$中，排除掉部分物品，得到子集$R’$</li>
<li>将MMR公式中的$R$替换为子集$R’$，选中的物品符合规则。</li>
<li>很容易的集合，公式都不变，就换个选物品的集合 -&gt; 合规的子集就可以了！</li>
</ul>
</li>
</ul>
<h3 id="DPP-Determinantal-Point-Process"><a href="#DPP-Determinantal-Point-Process" class="headerlink" title="DPP(Determinantal Point Process)"></a>DPP(Determinantal Point Process)</h3><blockquote>
<p>行列式点过程 (determinantal point process, DPP) 是一种经典的机器学习方法，在 1970’s 年代提出，在 2000 年之后有快速的发展。DPP 是目前推荐系统重排多样性公认的最好方法。</p>
<p> DPP 的数学比较复杂，这节课先介绍数学基础，下节课再介绍它在推荐系统的应用。这节课的内容主要是超平行体、超平行体的体积、行列式与体积的关系。</p>
<p> 参考文献： Chen et al. Fast greedy map inference for determinantal point process to improve recommendation diversity. In NIPS, 2018.</p>
</blockquote>
<h4 id="Math-Basics"><a href="#Math-Basics" class="headerlink" title="Math Basics"></a>Math Basics</h4><ul>
<li><p>背景：线性代数</p>
<ul>
<li><p>DPP：行列式点过程</p>
</li>
<li><p>DPP 的目标是从一个集合中选出尽量多样化的物品，契合重排的目标</p>
</li>
<li><p>它是目前推荐系统领域公认的最好多样性算法</p>
</li>
</ul>
</li>
<li><p>超平形体</p>
<ul>
<li><p>二维空间</p>
<ul>
<li><p>平行四边形</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409121107061.png" srcset="/img/loading.gif" lazyload alt="image-20240912110731478" style="zoom:50%;">
</li>
<li><p>平行四边形中的任何一个点可以表示为：$x=\alpha_{1}v_{1}+\alpha_{2}v_{2}$ </p>
</li>
<li><p>系数 $\alpha_{1}$ 和 $\alpha_{2}$ 取值范围是 [0,1]</p>
</li>
</ul>
</li>
<li><p>三维空间</p>
<ul>
<li><p>平行六面体</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409121108806.png" srcset="/img/loading.gif" lazyload alt="image-20240912110826756" style="zoom:50%;">
</li>
<li><p>平行六面体中的任何一个点可以表示为：$x=\alpha_{1}v_{1}+\alpha_{2}v_{2}+\alpha_{3}v_{3}$ </p>
</li>
<li><p>系数 $\alpha_{1},\alpha_{2},\alpha_{3}$ 取值范围是 [0,1]</p>
</li>
</ul>
</li>
<li><p>超平形体的数学定义为：</p>
<ul>
<li>一组向量 $v_{1},\cdots,v_{k}\in\mathbb{R}^{d}$ 可以确定一个 $k$ 维超平行体：${\mathcal{P}}(v_{1},\cdots,v_{k})={\alpha_{1}v_{1}+\cdots+\alpha_{k}v_{k}\mid0\leq\alpha_{1},\cdots,\alpha_{k}\leq1}$ </li>
<li>要求 $k\leq d$，比如 $d=3$ 维空间中有 $k=2$ 维平行四边形 </li>
<li>如果想让超平形体有意义，那么 $v_{1},\cdots,v_{k}$ 必须线性无关 </li>
<li>如果 $v_{1},\cdots,v_{k}$ 线性相关，则体积 ${\rm vol} ({\mathcal{P}})=0$。（例：有 $k=3$ 个向量，落在一个平面上，则平行六面体的体积为 0）</li>
</ul>
</li>
</ul>
</li>
<li><p>面积/体积计算：</p>
<ul>
<li><p>平行四边形：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409121117398.png" srcset="/img/loading.gif" lazyload alt="image-20240912111706237" style="zoom:50%;">

<ul>
<li><p>计算 $v_2$ 在 $v_1$ 上的投影： $\mathrm{Proj}<em>{v</em>{1}}(v_{2})={\frac{v_{1}^{T}v_{2}}{||v_{1}||<em>{2}^{2}}}\cdot v</em>{1}$ ，计算 $q_{2}=v_{2}-\mathrm{Proj}<em>{v</em>{1}}(v_{2})$ ，性质：底 $v_1$ 与高 $q_2$ 正交 </p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409121118983.png" srcset="/img/loading.gif" lazyload alt="image-20240912111837820" style="zoom:50%;">
</li>
<li><p>计算 $v_1$ 在 $v_2$ 上的投影： $\mathrm{Proj}<em>{v</em>{2}}(v_{1})={\frac{v_{1}^{T}v_{2}}{||v_{2}||<em>{2}^{2}}}\cdot v</em>{2}$ ，计算 $q_{1}=v_{1}-\mathrm{Proj}<em>{v</em>{2}}(v_{1})$ ，性质：底 $v_2$ 与高 $q_1$ 正交 。</p>
</li>
</ul>
</li>
<li><p>平行六面体的体积 </p>
<ul>
<li>体积 = $底面积× ||高||_2$ </li>
<li>平行四边形 $ {\mathcal{P}}(v_1,v_2)$ 是平行六面体 $ {\mathcal{P}}(v_1,v_2,v_3)$ 的底，就用上面讲的方法计算该平行四边形的面积。</li>
<li>高 $q_3$ 垂直于底 $ {\mathcal{P}}(v_1,v_2)$</li>
</ul>
</li>
<li><p>如果固定向量 $v_1,v_2,v_3$ 的长度，体积何时最大化、最小化？ </p>
<ul>
<li>设 $v_1,v_2,v_3$ 都是单位向量</li>
<li>当三个向量正交时，平行六面体为正方体，体积最大化， $\rm vol=1$</li>
<li>当三个向量线性相关时，体积最小化，$\rm vol=0$</li>
</ul>
</li>
</ul>
</li>
<li><p>衡量物品的相似性</p>
<ul>
<li><p>给定 $k$ 个物品，把它们表征为单位向量 $v_{1},\cdots,v_{k}\in\mathbb{R}^{d}$（ $d\geq k$）</p>
</li>
<li><p>用超平行体的体积衡量物品的多样性，体积介于 0 和 1 之间</p>
</li>
<li><p>如果 $v_{1},\cdots,v_{k}$ 两两正交（多样性好），则体积最大化，$vol=1$，如果 $v_{1},\cdots,v_{k}$ 线性相关（多样性差），则体积最小化，$vol=0$</p>
</li>
<li><p>给定 $k$ 个物品，把它们表征为单位向量 $v_{1},\cdots,v_{k}\in\mathbb{R}^{d}$（ $d\geq k$） </p>
</li>
<li><p>把它们作为矩阵 $V\in\mathbb{R}^{d\times k}$ 的列 </p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409121123828.png" srcset="/img/loading.gif" lazyload alt="image-20240912112343777" style="zoom:50%;">
</li>
<li><p>设 $d\geq k$</p>
<ul>
<li>行列式与体积满足： $\operatorname*{det}(V^{T}V),=,\operatorname*{vol}(\mathcal{P}(v_{1},\cdots,v_{k}))^{2}$ </li>
<li>即<strong>行列式和体积</strong>是等价的，最大化行列式等价于最大化体积。</li>
<li>因此，可以用行列式 $\operatorname*{det}(V^{T}V)$ 衡量向量 $v_{1},\cdots,v_{k}$ 的多样性</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Diversity-Algorithm"><a href="#Diversity-Algorithm" class="headerlink" title="Diversity Algorithm"></a>Diversity Algorithm</h4><ul>
<li><p>多样性问题</p>
<ul>
<li>精排给 $n$ 个物品打分：$\rm{reward} _i,…,\rm{reward} _n$ </li>
<li>$n$ 个物品的向量表征：$v_{1},\cdots,v_{k}\in\mathbb{R}^{d}$ </li>
<li>从 $n$ 个物品中选出 $k$ 个物品，组成集合 $\mathcal{S}$ <ul>
<li>价值大：分数之和  $\sum_{j\in\mathcal{S}} {\rm{reward}}_j$  越大越好</li>
<li>多样性好：$\mathcal{S}$ 中 $k$ 个向量组成的超平形体 $\mathcal{P}(\mathcal{S})$ 的体积越大越好</li>
</ul>
</li>
</ul>
</li>
<li><p>DPP过程：</p>
<ul>
<li><p>DPP 是一种传统的统计机器学习方法：$\operatorname{argmax}<em>{\mathcal{S}:|{\mathcal{S}}|=k}\log\operatorname*{det}(V</em>{\mathcal{S}}^{T},V_{\mathcal{S}})$ </p>
<ul>
<li>从集合 $\mathcal{ S}$ 中选出 $k$ 个</li>
<li>该公式严格来说叫 k-DPP</li>
</ul>
</li>
<li><p>Hulu 的论文将 DPP 应用在推荐系统： $\begin{array}{l}{{argmax_{\mathcal{S:| \mathcal{S}
     |=k}}}}\end{array}\theta\cdot\left(\sum_{j\in \mathcal{S}}\mathrm{reward}_{j}\right)+(1-\theta)\cdot\log\operatorname*{det}(V_{\mathcal{ S}}^{T}\,V_{\mathcal{ S}})$ {#</p>
<ul>
<li>前半部分计算集合中物品的价值</li>
<li>后半部分是行列式的对数，物品多样性越好，这部分越大</li>
<li>Hulu 论文的主要贡献不是提出该公式，而是快速求解该公式</li>
</ul>
<blockquote>
<p>参考文献：Chen et al. Fast greedy map inference for determinantal point process to improve recommendation diversity. In NIPS, 2018.</p>
</blockquote>
</li>
<li><p>DPP 是个组合优化问题，从集合 ${ 1,…,n}$ 中选出一个大小为 $k$ 的子集 $\mathcal{ S}$，精确 DPP 是不可能的，因为它是个 NP-hard 问题。</p>
</li>
<li><p>简单来讲，Hulu提出的算法，可以降低时间复杂度，通过矩阵运算的技巧。可以求出现有矩阵“加上一行，加上一列”，得到的结果，并且很快，不需要重新计算整个矩阵。在现有选择的物品（矩阵）上，再选择一个物品（矩阵上加一行一列），传统算法需要重新计算整个矩阵，但是Hulu提出的算法，能够在之前的运算结果上，能够快速算出新矩阵的近似准确值，从而快速得到结果。这种速度和时间复杂度上取得的提升，使DPP确实能够直接用在操作系统中。</p>
</li>
<li><p>详细算法过程可以回去看Wang老师的课程，或者精读论文和相关笔记。</p>
</li>
</ul>
</li>
<li><p>DPP拓展</p>
<ul>
<li>滑动窗口 <ul>
<li>用 S 表示已选中的物品，用 R 表示未选中的物品，DPP 的贪心算法求解： $\begin{array}{c}{{\mathrm{argmax_{i\in \mathcal{R}}}~\theta\cdot{\rm reward} }_{i}\ +\ (1-\theta)\cdot\log \mathrm{det}(\bold{ \textstyle{A}}_{\mathcal{ S}\cup\{i\}})}\end{array}$ {# </li>
<li>缺点： <ul>
<li>随着集合 $\mathcal{ S}$ 增大，其中相似物品越来越多，物品向量会趋近线性相关</li>
<li>导致行列式 ${\rm det}(\bold{ \textstyle{A}}_{\mathcal{ S}})$ 会坍缩到零，对数趋于负无穷</li>
</ul>
</li>
</ul>
</li>
<li>贪心算法： $\begin{array}{c}{{\mathrm{argmax_{i\in \mathcal{R}}}~\theta\cdot{\rm reward} }_{i}\ +\ (1-\theta)\cdot\log \mathrm{det}(\bold{ \textstyle{A}}_{\mathcal{ S}\cup\{i\}})}\end{array}$ {#</li>
<li>滑动窗口： $\begin{array}{c}{{\mathrm{argmax_{i\in \mathcal{R}}}~\theta\cdot{\rm reward} }_{i}\ +\ (1-\theta)\cdot\log \mathrm{det}(\bold{ \textstyle{A}}_{\mathcal{ W}\cup\{i\}})}\end{array}$ {#</li>
<li>规则约束 <ul>
<li>贪心算法每轮从 $\mathcal{ R}$ 中选出一个物品：$\begin{array}{c}{{\mathrm{argmax_{i\in \mathcal{R}}}~\theta\cdot{\rm reward} }_{i}\ +\ (1-\theta)\cdot\log \mathrm{det}(\bold{ \textstyle{A}}_{\mathcal{ W}\cup\{i\}})}\end{array}$ {#</li>
<li>有很多规则约束，例如最多连续出 5 篇视频笔记（如果已经连续出了 5 篇视频笔记，下一篇必须是图文笔记） </li>
<li>用规则排除掉 $\mathcal{ R}$ 中的部分物品，得到子集 $\mathcal{ R}’$，然后求解：$\begin{array}{c}{{\mathrm{argmax_{i\in \mathcal{R'}}}~\theta\cdot{\rm reward} }_{i}\ +\ (1-\theta)\cdot\log \mathrm{det}(\bold{ \textstyle{A}}_{\mathcal{ W}\cup\{i\}})}\end{array}$ {#</li>
<li>这一部分的操作，感觉和上面的做法都是很一样的，这种思想直接平迁到了DPP。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Item-Cold-Start"><a href="#Item-Cold-Start" class="headerlink" title="Item Cold Start"></a>Item Cold Start</h2><blockquote>
<p>很少有 UGC(User Generated Content) 能让冷启动达到天花板。</p>
</blockquote>
<h3 id="Optimization-goals-amp-evaluation-indicators"><a href="#Optimization-goals-amp-evaluation-indicators" class="headerlink" title="Optimization goals &amp; evaluation indicators"></a>Optimization goals &amp; evaluation indicators</h3><ul>
<li><p>物品冷启动</p>
<ul>
<li>小红书上用户新发布的笔记</li>
<li>B站上用户新上传的视频</li>
<li>今日头条上作者新发布的文章</li>
</ul>
</li>
<li><p>此处主要考虑 UGC（User Generated Content） 的物品冷启动</p>
<ul>
<li>与 UGC 相反的是 PGC（Platform Generated Content），例如网飞、腾讯视频</li>
<li>UGC 冷启比 PGC 更难，因为用户上传的内容良莠不齐，而且量大</li>
</ul>
</li>
<li><p>思考：为什么要特殊对待新笔记？新笔记需要冷启动的原因</p>
<ul>
<li>新笔记缺少与用户的交互，导致推荐的难度大、效果差</li>
<li>扶持新发布、低曝光的笔记，可以增强作者发布意愿</li>
</ul>
</li>
<li><p>优化冷启的目标</p>
<ul>
<li>精准推荐：克服冷启的困难，把新笔记推荐给合适的用户，不引起用户反感</li>
<li>激励发布：流量向低曝光新笔记倾斜，激励作者发布</li>
<li>挖掘高潜：通过初期小流量的试探，找到高质量的笔记，给与流量倾斜</li>
</ul>
</li>
<li><p>评价指标</p>
<ul>
<li>作者侧指标：反映用户的发布意愿<ul>
<li>发布渗透率、人均发布量。</li>
<li>低曝光扶持越好，作者侧指标越好。</li>
</ul>
</li>
<li>用户侧指标：反映推荐是否精准，是否会引起用户反感<ul>
<li>新笔记指标：新笔记的点击率、交互率。推荐的准，这个指标会好看。</li>
<li>大盘指标：消费时长、日活、月活。</li>
<li>冷启动不是促进消费指标增长，但是也不能显著伤害消费指标，应该尽量让消费指标持平。</li>
</ul>
</li>
<li>内容侧指标：反映冷启是否能挖掘优秀笔记<ul>
<li>类似于30天内点击超过1000的笔记</li>
<li>高热笔记占比，小红书应该也用了。</li>
</ul>
</li>
</ul>
</li>
<li><p>作者侧指标</p>
<ul>
<li>发布渗透率（penetration rate）<ul>
<li>发布渗透率 = 当日发布人数 / 日活人数，发布一篇或以上，就算一个发布人数，例：<ul>
<li>当日发布人数 = 100 万</li>
<li>日活人数 = 2000 万</li>
<li>发布渗透率 = 100 / 2000 = 5%</li>
</ul>
</li>
</ul>
</li>
<li>人均发布量<ul>
<li>人均发布量 = 当日发布笔记数 / 日活人数，例：<ul>
<li>每日发布笔记数 = 200 万</li>
<li>日活人数 = 2000 万</li>
<li>人均发布量 = 200 / 2000 = 0.1</li>
</ul>
</li>
</ul>
</li>
<li>发布渗透率、人均发布量反映出作者的发布积极性</li>
<li>冷启的重要优化目标是促进发布，增大内容池</li>
<li>新笔记获得的曝光越多，首次曝光和交互出现得越早，作者发布积极性越高</li>
</ul>
</li>
<li><p>用户侧指标</p>
<ul>
<li>新笔记的消费指标<ul>
<li>新笔记的点击率、交互率<ul>
<li>问题：曝光的基尼系数很大</li>
<li>即少量头部新笔记推送准确，但大部分新笔记的推送不准，综合下来点击率、交互率也很高</li>
</ul>
</li>
<li>少数头部新笔记占据了大部分的曝光</li>
</ul>
</li>
<li>分别考察高曝光、低曝光新笔记</li>
<li>高曝光：比如 &gt;1000 次曝光</li>
<li>低曝光：比如 &lt;1000 次曝光</li>
<li>大盘消费指标（不区分新老笔记）<ul>
<li>优化冷启时，不是为了提升大盘指标，而是确保新策略不显著伤害大盘指标</li>
<li>大盘的消费时长、日活、月活</li>
<li>新笔记扶持有跷跷板效应，大力扶持低曝光新笔记会发生什么？<ul>
<li>作者侧发布指标变好</li>
<li>用户侧大盘消费指标变差（损害用户体验）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>内容侧指标</p>
<ul>
<li>高热笔记占比</li>
<li>高热笔记：前 30 天获得 1000+ 次点击</li>
<li>高热笔记占比越高，说明冷启阶段挖掘优质笔记的能力越强</li>
</ul>
</li>
<li><p>冷启动优化点</p>
<ul>
<li>优化全链路（召回、排序等）</li>
<li>流量调控（流量如何在新老笔记中分配）</li>
</ul>
</li>
</ul>
<h3 id="Item-Cold-Start-Simple-Recall-Channel"><a href="#Item-Cold-Start-Simple-Recall-Channel" class="headerlink" title="Item Cold Start: Simple Recall Channel"></a>Item Cold Start: Simple Recall Channel</h3><ul>
<li><p>召回的依据</p>
<ul>
<li>✔ 自带图片、文字、地点</li>
<li>✔ 算法或人工标注的标签</li>
<li>❌ 没有用户点击、点赞等信息。这些信息可以反映笔记的质量，以及哪类用户喜欢该笔记</li>
<li>❌ 没有笔记 ID embedding。ID embedding 是从用户和笔记的交互中学习出来的</li>
</ul>
<blockquote>
<p>双塔模型是推荐系统中最重要的召回通道，没有之一。离开双塔模型，很难做好新笔记的推荐。同时影响召回和排序。</p>
</blockquote>
</li>
<li><p>冷启动召回的困难</p>
<ul>
<li>缺少用户交互，还没学好笔记 ID embedding，导致双塔模型效果不好。</li>
<li>缺少用户交互，导致 ItemCF 不适用。</li>
</ul>
</li>
<li><p>召回通道</p>
<ul>
<li>❌ ItemCF 召回（不适用）</li>
<li>❔ 双塔模型（改造后适用）</li>
<li>✔ 类目、关键词召回（适用）</li>
<li>✔ 聚类召回（适用）</li>
<li>✔ Look-Alike 召回（适用）</li>
</ul>
</li>
</ul>
<h4 id="ID-Embedding-Improvement"><a href="#ID-Embedding-Improvement" class="headerlink" title="ID Embedding Improvement"></a>ID Embedding Improvement</h4><ul>
<li><p>ID Embedding的改进</p>
<ul>
<li>改进方案 1：新笔记使用 default embedding<ul>
<li>物品塔做 ID embedding 时，让所有新笔记共享一个 ID，而不是用自己真正的 ID</li>
<li>Default embedding：共享的 ID 对应的 embedding 向量，学出来的 Default embedding 比随机初始化一个 ID embedding 要好。</li>
<li>到下次模型训练的时候，新笔记才有自己的 ID embedding 向量</li>
</ul>
</li>
<li>改进方案 2：利用相似笔记 embedding 向量<ul>
<li>查找 top k 内容最相似的高曝笔记</li>
<li>把 k 个高曝笔记的 embedding 向量取平均，作为新笔记的 embedding</li>
<li>之所以用高曝笔记，是因为它们的 embedding 通常学得比较好</li>
</ul>
</li>
</ul>
</li>
<li><p>多个向量召回池</p>
<ul>
<li>多个召回池，让新笔记有更多曝光机会<ul>
<li>1 小时新笔记</li>
<li>6 小时新笔记</li>
<li>24 小时新笔记</li>
<li>30 天笔记</li>
</ul>
</li>
<li>所有召回池共享同一个双塔模型，那么多个召回池不增加训练的代价。</li>
</ul>
</li>
</ul>
<h4 id="Category-and-keyword-recall"><a href="#Category-and-keyword-recall" class="headerlink" title="Category and keyword recall"></a>Category and keyword recall</h4><ul>
<li><p>类目召回</p>
<ul>
<li><p>用户画像</p>
<ul>
<li>感兴趣的类目：美食、科技数码、电影……</li>
<li>感兴趣的关键词：纽约、职场、搞笑、程序员、大学……</li>
</ul>
</li>
<li><p>基于类目的召回</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409181127341.png" srcset="/img/loading.gif" lazyload alt="image-20240918112743215" style="zoom:50%;">

<ul>
<li>系统维护类目索引：类目 → 笔记列表（按时间倒排）</li>
<li>用类目索引做召回：用户画像 → 类目 → 笔记列表</li>
<li>取回笔记列表上前 k 篇笔记（即最新的 k 篇）</li>
</ul>
</li>
</ul>
</li>
<li><p>基于关键词的召回</p>
<ul>
<li>系统维护关键词索引：关键词 → 笔记列表（按时间倒排）</li>
<li>根据用户画像上的关键词做召回</li>
<li>和上面的没啥区别，只是把关键词 -&gt; 类目。</li>
</ul>
</li>
<li><p>缺点：</p>
<ul>
<li>缺点 1：只对刚刚发布的新笔记有效</li>
<li>取回某类目/关键词下最新的 k 篇笔记。</li>
<li>发布几小时之后，就再没有机会被召回。“留给每篇笔记的窗口期很短”</li>
<li>所以应该在用户高频使用软件的时间段发布内容</li>
<li>缺点 2：弱个性化，不够精准<ul>
<li>类目/关键词还是太大了</li>
<li>比如我喜欢观赏鱼，但是我搜索的是宠物。推荐给我的新笔记大多数都是猫猫狗狗，我就不感兴趣。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Cluster-recall"><a href="#Cluster-recall" class="headerlink" title="Cluster recall"></a>Cluster recall</h4><blockquote>
<p>聚类召回是基于物品内容的召回通道。它假设如果用户喜欢一个物品，那么用户会喜欢内容相似的其他物品。使用聚类召回，需要事先训练一个多模态神经网络，将笔记图文表征为向量，并对向量做聚类，然后建索引。冷启动的时候特别有用！</p>
</blockquote>
<ul>
<li><p>聚类召回</p>
<ul>
<li>基本思想<ul>
<li>如果用户喜欢一篇笔记，那么他会喜欢内容相似的笔记</li>
<li>事先训练一个神经网络，基于笔记的类目和图文内容，把笔记映射到向量</li>
<li>对笔记向量做聚类，划分为 1000 cluster，记录每个 cluster 的中心方向（k-means 聚类，用余弦相似度）</li>
</ul>
</li>
<li>聚类索引<ul>
<li>一篇新笔记发布之后，用神经网络把它的图文内容映射到一个特征向量</li>
<li>从 1000 个向量（对应 1000 个 cluster）中找到最相似的向量，作为新笔记的 cluster</li>
<li>索引：cluster → 笔记 ID 列表（按时间倒排）</li>
</ul>
</li>
<li>线上召回<ul>
<li>给定用户 ID，找到他的 last-n 交互的笔记列表，把这些笔记作为种子笔记</li>
<li>把每篇种子笔记映射到向量，寻找最相似的 cluster（知道了用户对哪些 cluster 感兴趣）</li>
<li>从每个 cluster 的笔记列表中，取回最新的 $m$ 篇笔记</li>
<li>最多取回 $mn$ 篇新笔记</li>
</ul>
</li>
<li>感觉就是基于内容的推荐呀！聚类召回与类目召回的缺点相同：只对刚发布的新笔记有效</li>
</ul>
</li>
<li><p>内容相似度模型</p>
<ul>
<li><p>提取图文表征</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409181649283.png" srcset="/img/loading.gif" lazyload alt="image-20240918164930012" style="zoom:50%;">
</li>
<li><p>聚类召回通过内容相似度模型来把笔记映射到向量。</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409181650599.png" srcset="/img/loading.gif" lazyload alt="image-20240918165005544" style="zoom:50%;">
</li>
<li><p>左右两个用的神经网络的参数是相同的</p>
</li>
<li><p>CNN 和 BERT 可以用预训练模型，全连接层是随机初始化后训练出来的</p>
</li>
</ul>
</li>
<li><p>训练内容相似度模型</p>
<ul>
<li><p>模型结构：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409181651717.png" srcset="/img/loading.gif" lazyload alt="image-20240918165144539" style="zoom:50%;">
</li>
<li><p>给定用户 ID，找到他的 last-n 交互的笔记列表，把这些笔记作为种子笔记</p>
</li>
<li><p>基本想法：鼓励 $\cos{(\bold{a},\bold{b}^+)}$ 大于 $\cos{(\bold{a},\bold{b}^-)}$ </p>
<ul>
<li>Triplet hinge loss: $L(\bold{a},\bold{b}^+,\bold{b}^-)=\max{\{ 0,\cos{(\bold{a},\bold{b}^-)}+m-\cos{(\bold{a},\bold{b}^+)}\}}$ {#</li>
<li>Triplet logistic loss: $L(\bold{a},\bold{b}^+,\bold{b}^-)=\log{( 1+\exp{·(\cos{(\bold{a},\bold{b}^-)}-\cos{(\bold{a},\bold{b}^+))})}}$ {#</li>
</ul>
</li>
<li><p>此处的公式与召回 - 双塔模型 - Pairwise 训练中相似 </p>
</li>
<li><p>&lt; 种子笔记，正样本 &gt; </p>
<ul>
<li>方法一：人工标注二元组的相似度，人工标注代价大，不划算。</li>
<li>方法二：算法自动选正样本 <ul>
<li>筛选条件： <ul>
<li>只用高曝光笔记作为二元组（因为有充足的用户交互信息）</li>
<li>两篇笔记有相同的二级类目，比如都是“菜谱教程”（过滤掉不相似的样本，缩小查找范围）</li>
</ul>
</li>
<li>用 ItemCF 的物品相似度选正样本</li>
</ul>
</li>
</ul>
</li>
<li><p>&lt; 种子笔记，负样本 &gt;</p>
<ul>
<li>从全体笔记中随机选出满足条件的笔记：<ul>
<li>字数较多（神经网络提取的文本信息有效）</li>
<li>笔记质量高，避免图文无关</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>聚类召回总结</p>
<ul>
<li>基本思想：根据用户的点赞、收藏、转发记录，推荐内容相似的笔记</li>
<li>线下训练：多模态神经网络把图文内容映射到向量</li>
<li>线上服务：用户历史喜欢的笔记 → 特征向量 → 最近的 Cluster → 每个 Cluster 上的新笔记</li>
</ul>
</li>
</ul>
<h4 id="Look-Alike-recall"><a href="#Look-Alike-recall" class="headerlink" title="Look-Alike recall"></a>Look-Alike recall</h4><blockquote>
<p>Look-Alike 是一种召回通道，对冷启很有效。Look-Alike 适用于发布一段时间、但是点击次数不高的物品。物品从发布到热门，主要的透出渠道会经历三个阶段： </p>
<ol>
<li>类目召回、聚类召回。它们是基于内容的召回通道，适用于刚刚发布的物品。 </li>
<li>Look-Alike 召回。它适用于有点击，但是点击次数不高的物品。</li>
<li>双塔、ItemCF、Swing 等等。它们是基于用户行为的召回通道，适用于点击次数较高的物品。</li>
</ol>
</blockquote>
<ul>
<li><p>Look-Alike起源于互联网广告</p>
<ul>
<li><p>用明确的目标用户画像。</p>
</li>
<li><p>但是实际上很多用户没有那么详细的信息。</p>
</li>
<li><p>从已知种子用户 -&gt; 含有很多未知信息的用户。</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409181701435.png" srcset="/img/loading.gif" lazyload alt="image-20240918170141232" style="zoom:50%;"></li>
</ul>
</li>
<li><p>如何计算两个用户的相似度？</p>
<ul>
<li>UserCF：两个用户有共同的兴趣点</li>
<li>Embedding：两个用户向量的 cosine 较大</li>
</ul>
</li>
<li><p>Look-Alike 用于新笔记召回</p>
<ul>
<li><p>点击、点赞、收藏、转发—用户对笔记可能感兴趣</p>
</li>
<li><p>系统把新笔记推荐给了许多用户，由于是新笔记，所以受众不太精准。而跟新笔记进行了交互用户，就成了种子用户。</p>
</li>
<li><p>用双塔模型学习种子用户的向量，然后取均值，把得到的向量作为 新笔记 的表征</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409181704285.png" srcset="/img/loading.gif" lazyload alt="image-20240918170448226" style="zoom:50%;">

<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409181705439.png" srcset="/img/loading.gif" lazyload alt="image-20240918170520386" style="zoom:50%;">

<blockquote>
<p>把得到的新的向量作为用户的表征。代表什么样的用户对于笔记感兴趣。</p>
</blockquote>
</li>
<li><p>近线更新特征向量，近线：不用实时更新，做到分钟级更新即可</p>
</li>
<li><p>此处是做到每当有新交互发生后，几分钟内更新特征向量。特征向量是有交互的用户的向量的平均。</p>
</li>
<li><p>每当有用户交互该物品，更新笔记的特征向量</p>
</li>
</ul>
</li>
<li><p>交互流程</p>
<ul>
<li><p>根据上面的流程，得到很多笔记的Embedding，存在向量数据库里面。</p>
</li>
<li><p>给用户做推荐的时候，用双塔计算用户的Embedding，去向量数据库里面做最近邻查找，取回几十篇笔记。</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409181710294.png" srcset="/img/loading.gif" lazyload alt="image-20240918171000110" style="zoom:50%;">
</li>
<li><p>这个召回通道就叫 Look-Alike</p>
</li>
</ul>
</li>
<li><p>小红书的look-alike，和广告领域的look-alike原理是相同的：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409181711461.png" srcset="/img/loading.gif" lazyload alt="image-20240918171127405" style="zoom:50%;"></li>
</ul>
<h3 id="Traffic-Control"><a href="#Traffic-Control" class="headerlink" title="Traffic Control"></a>Traffic Control</h3><ul>
<li>冷启动的优化点<ul>
<li>优化全链路（包括召回和排序）</li>
<li>流量调控（流量怎么在新物品、老物品中分配）</li>
</ul>
</li>
<li>工业界的通常做法是扶持新笔记</li>
<li>扶持新笔记的目的<ul>
<li>目的 1：促进发布，增大内容池<ul>
<li>新笔记获得的曝光越多，作者创作积极性越高</li>
<li>反映在发布渗透率、人均发布量</li>
</ul>
</li>
<li>目的 2：挖掘优质笔记<ul>
<li>做探索，让每篇新笔记都能获得足够曝光</li>
<li>挖掘的能力反映在高热笔记占比</li>
</ul>
</li>
</ul>
</li>
<li>工业界的做法<ul>
<li>假设推荐系统只分发年龄 &lt;30 天的笔记。</li>
<li>假设采用自然分发，新笔记（年龄 &lt;24 小时）的曝光占比为 1/30，扶持新笔记，让新笔记的曝光占比远大于 1/30。</li>
</ul>
</li>
<li>流量调控技术的发展<ul>
<li>在推荐结果中强插新笔记（落后技术）</li>
<li>对新笔记的排序分数做提权（boost）<ul>
<li>较为复杂，难点在于调整权重</li>
<li>投入产出比很划算的策略，效果还可以。实现不难，抖音、小红书前期都这么做。</li>
</ul>
</li>
<li>通过提权，对新笔记做保量。高级技术！复杂精细化的提权，例如：1h内曝光必须最少是100。</li>
<li>差异化保量：根据内容质量，确定保量目标。质量越高，流量倾斜越多，保量目标越高。</li>
</ul>
</li>
</ul>
<h4 id="New-Notes-Boost"><a href="#New-Notes-Boost" class="headerlink" title="New Notes Boost"></a>New Notes Boost</h4><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409181803866.png" srcset="/img/loading.gif" lazyload alt="image-20240918180318811" style="zoom:50%;">

<ul>
<li>系统的链路<ul>
<li>目标：让新笔记有更多机会曝光<ul>
<li>如果做自然分发，24 小时新笔记占比为 1/30</li>
<li>做人为干涉，让新笔记占比大幅提升</li>
</ul>
</li>
<li>干涉粗排、重排环节，给新笔记提权<ul>
<li>优点：容易实现，投入产出比好</li>
<li>缺点：<ul>
<li>曝光量对提权系数很敏感</li>
<li>很难精确控制曝光量，容易过度曝光和不充分曝光</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="New-Notes-Guarantee"><a href="#New-Notes-Guarantee" class="headerlink" title="New Notes Guarantee"></a>New Notes Guarantee</h4><ul>
<li><p>保量：不论笔记质量高低，都保证 24 小时获得 100 次曝光</p>
</li>
<li><p>在原有提权系数的基础上，乘以额外的提权的系数，比如：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409181839618.png" srcset="/img/loading.gif" lazyload alt="image-20240918183947421" style="zoom:50%;">
</li>
<li><p>动态提权保量：用下面四个值计算提权系数 </p>
<ul>
<li>目标时间：比如 24 小时</li>
<li>目标曝光：比如 100 次</li>
<li>发布时间：比如笔记已经发布 12 小时</li>
<li>已有曝光：比如笔记已经获得 20 次曝光</li>
</ul>
</li>
<li><p>提权系数 =$f(\frac{发布时间}{目标时间},\frac{已有曝光}{目标曝光})=f(0.5,0.2)$ </p>
</li>
<li><p>保量的难点</p>
<ul>
<li>保量成功率远低于 100%<ul>
<li>很多笔记在 24 小时达不到 100 次曝光</li>
<li>召回、排序存在不足</li>
<li>提权系数调得不好</li>
</ul>
</li>
<li>线上环境变化会导致保量失败<ul>
<li>线上环境变化：新增召回通道、升级排序模型、改变重排打散规则……</li>
<li>线上环境变化后，需要调整提权系数</li>
</ul>
</li>
</ul>
</li>
<li><p>思考题</p>
<ul>
<li>是否可以给所有新笔记一个很大的提权系数（比如 4 倍），直到达成 100 次曝光为止，然后再把权重恢复呢？<ul>
<li>这样的保量成功率很高</li>
<li>为什么不用这种方法呢？</li>
</ul>
</li>
<li>给新笔记分数 boost 越多，对新笔记越有利？<ul>
<li>好处：分数提升越多，曝光次数越多</li>
<li>坏处：把笔记推荐给不太合适的受众<ul>
<li>点击率、点赞率等指标会偏低</li>
<li>笔记长期会受推荐系统打压，难以成长为热门笔记：即这个笔记如果推荐给合适用户，那它是不错的，但强行提高曝光次数，导致它被经常推给不适合的用户，进而交互指标偏低。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Differentiated-quality-assurance"><a href="#Differentiated-quality-assurance" class="headerlink" title="Differentiated quality assurance"></a>Differentiated quality assurance</h4><ul>
<li>保量：不论新笔记质量高低，都做扶持，在前 24 小时给 100 次曝光</li>
<li>差异化保量：不同笔记有不同保量目标，普通笔记保 100 次曝光，内容优质的笔记保 100~500 次曝光</li>
<li>基础保量：24 小时 100 次曝光</li>
<li>内容质量：用模型评价内容质量高低，给予额外保量目标，上限是加 200 次曝光</li>
<li>作者质量：根据作者历史上的笔记质量，给予额外保量目标，上限是加 200 次曝光</li>
<li>一篇笔记最少有 100 次保量，最多有 500 次保量。达到保量目标后，就会停止扶持，新笔记和老笔记公平竞争。</li>
</ul>
<h3 id="A-x2F-B-Test-1"><a href="#A-x2F-B-Test-1" class="headerlink" title="A/B Test"></a>A/B Test</h3><blockquote>
<p>这节课的内容是物品冷启动的AB测试，内容很烧脑，不建议初学者观看。推荐系统常用的A/B测试只考察用户侧消费指标，而推荐系统的AB测试还需要额外考察作者侧发布指标。后者远比前者复杂，而且所有已知的实验方案都存在缺陷。</p>
</blockquote>
<ul>
<li><p>冷启动AB测试</p>
<ul>
<li>作者侧（发布）指标：发布渗透率、人均发布量。</li>
<li>用户侧（消费）指标：<ul>
<li>对新笔记的点击率、交互率</li>
<li>大盘指标：消费时长、日活、月活。（标准AB测试）</li>
</ul>
</li>
<li>标准的 AB 测试只测大盘指标，而冷启动 AB 测试还要测上面提到的指标</li>
</ul>
</li>
<li><p>标准的A/B测试：</p>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409191209883.png" srcset="/img/loading.gif" lazyload alt="image-20240919120936788" style="zoom:50%;">

<ul>
<li><p>冷启动的A/B测试：主要测用户侧的消费指标，还有作者侧的发布指标</p>
<ul>
<li><p>用户侧实验：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409191213124.png" srcset="/img/loading.gif" lazyload alt="image-20240919121339898" style="zoom:50%;">

<ul>
<li><p>限定：保量 100 次曝光</p>
</li>
<li><p>假设：新笔记曝光越多，用户使用 APP 时长越低：这个假设很合理，因为新笔记推送不准确，过多会影响体验</p>
</li>
<li><p>新策略：把新笔记排序时的权重增大两倍</p>
</li>
<li><p>结果（只看消费指标）：</p>
<ul>
<li>AB 测试的 diff 是负数（策略组不如对照组）。</li>
<li>如果推全，diff 会缩小（比如 −2% → −1%）。新策略对用户体验的伤害，没有A/B test观测到的那么严重。Why？</li>
</ul>
</li>
<li><p>为什么会有上面的diff深入探讨：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409191414558.png" srcset="/img/loading.gif" lazyload alt="image-20240919141415073" style="zoom:50%;">

<ul>
<li>使用新策略后，实验组看到的新笔记会增多，又因为有着保量 100 次的目标，所以对照组看到的新笔记数量会减少。</li>
<li>实验组看多了新笔记，消费指标变差；对照组看少了新笔记，消费指标变好。</li>
<li>但推全后，这 100 次的保量目标会均匀的分配给所有用户，消费指标依然会下降，但不会有实验组那么差。 -&gt; GPT如是说到：在实验组中，用户被强制看到至少100次新笔记，这可能导致用户感到疲劳或者不满，因为他们可能觉得内容过多或者不感兴趣。这种强制性的展示可能会对用户体验产生负面影响，从而导致消费指标的显著下降。（100次曝光都在这一小部分人群之中）然而，在推全后，虽然所有用户都会看到更多的新笔记，但这种展示不会像实验组那样强制性和频繁。这意味着用户可能会有更多的选择和控制权，从而减少对用户体验的负面影响。因此，虽然消费指标可能会有所下降，但下降的幅度不会像实验组那样剧烈。</li>
</ul>
</li>
</ul>
</li>
<li><p>作者侧实验</p>
<ul>
<li>作者侧实验不好做，没有很完美的实验方案。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Option-1"><a href="#Option-1" class="headerlink" title="Option 1"></a>Option 1</h4><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409191502091.png" srcset="/img/loading.gif" lazyload alt="image-20240919150237022" style="zoom:50%;">

<ul>
<li><p>严重缺点：新笔记会抢夺流量。存在现象：实验观测到diff，但是推全后，发布指标没有发生任何变化。</p>
</li>
<li><p>缺点汇总：</p>
<ul>
<li><p>缺点一：新笔记之间会抢流量</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409191428016.png" srcset="/img/loading.gif" lazyload alt="image-20240919142829955" style="zoom:50%;">

<ul>
<li>设定：<ul>
<li>新老笔记走各自队列，没有竞争（完全独立）</li>
<li>重排分给新笔记 1/3 流量，分给老笔记 2/3 流量，两者曝光比例固定</li>
</ul>
</li>
<li>新策略：把新笔记重拍的权重增大两倍<ul>
<li>新笔记和老笔记公平竞争。这对新笔记来说依然是公平竞争，因为新笔记有自己的队列，只和新笔记竞争呀。而新笔记在重排中还是 1/3 流量。</li>
<li>所以新策略不会激励发布，不会改变发布侧指标。</li>
</ul>
</li>
<li>结果（只看发布指标）：<ul>
<li>AB 测试的 diff 是正数（实验组优于对照组）。为啥？因为提权后的实验组新笔记，会抢走对照组的曝光，因为实验组和对照组是公平竞争的。实验组提权后，曝光变多了，实验组指标肯定也就涨了，对照组也就跌了，这就有diff了。</li>
<li>如果推全，diff 会消失（比如 2% → 0）。在我们的<strong>设定</strong>下，新笔记权重 x 2不改变发布指标，只改变曝光量。推全后，发布侧指标就和之前一样了。</li>
<li>道理：A/B观测到的Diff是不可信的，推全之后可能会消失。</li>
</ul>
</li>
<li>给实验组新笔记提权后，同是新笔记，实验组中的能获得更多曝光，而对照组中的曝光就少了。</li>
<li>而一旦推全后，就不存在新笔记间强流量的情况，diff 就消失了。</li>
</ul>
</li>
<li><p>缺点二：新笔记和老笔记抢流量</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409191443160.png" srcset="/img/loading.gif" lazyload alt="image-20240919144313102" style="zoom:50%;">

<ul>
<li>举例说明<ul>
<li>设定：新老笔记自由竞争</li>
<li>新策略：把新笔记排序时的权重增大两倍</li>
<li>AB 测试时，50% 新笔记（带策略）跟 100% 老笔记抢流量。一份新笔记，抢两份老笔记的流量。</li>
<li>推全后，100% 新笔记（带策略）跟 100% 老笔记抢流量。一份新笔记只抢一份老笔记的流量，更难抢到流量！</li>
<li>作者侧 AB 测试结果与推全结果有些差异</li>
</ul>
</li>
<li><strong>新老笔记抢流量不是问题，问题是 AB 测试与推全后的 设定 不一致，导致 AB 测试的结果不准确</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Option-2"><a href="#Option-2" class="headerlink" title="Option 2"></a>Option 2</h4><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409191451116.png" srcset="/img/loading.gif" lazyload alt="image-20240919145123064" style="zoom:50%;">

<ul>
<li>实验组用户只能看到实验组新笔记，对照组用户同理</li>
<li>这样就避免了两组新笔记抢流量</li>
<li>方案二比方案一的优缺点<ul>
<li>优点：新笔记的两个桶不抢流量，作者侧实验结果更可信</li>
<li>相同：新笔记和老笔记抢流量，作者侧 AB 测试结果与推全结果有些差异<ul>
<li>依然是 AB 测试时 50% 新笔记跟 100% 老笔记抢流量，推全后 100% 新笔记跟 100% 老笔记抢流量</li>
</ul>
</li>
<li>缺点：新笔记池减小一半，对用户体验造成负面影响。原来100篇最好的，现在只剩50篇，还有再凑上50%没那么好的，会因为A/B test影响业务大盘，造成损失。</li>
</ul>
</li>
</ul>
<h4 id="Option-3"><a href="#Option-3" class="headerlink" title="Option 3"></a>Option 3</h4><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409191503361.png" srcset="/img/loading.gif" lazyload alt="image-20240919150314953" style="zoom:50%;">

<ul>
<li><p>相当于把小红书切成了 2 个 APP。</p>
</li>
<li><p>这样做的实验结果是最精准的。</p>
</li>
<li><p>内容池少了一半，不太可行，太损害用户体验了：但是跨国 APP 可以用这个方案。</p>
</li>
<li><p>Summary</p>
<ul>
<li>冷启的 AB 测试需要观测 <strong>作者发布指标</strong> 和 <strong>用户消费指标</strong>。</li>
<li>各种 AB 测试的方案都有缺陷（小红书有更好的方案，但也不完美）</li>
<li>设计方案的时候，问自己几个问题：<ul>
<li>实验组、对照组新笔记会不会抢流量？</li>
<li>新笔记、老笔记怎么抢流量？AB 测试时怎么抢？推全后怎么抢？</li>
<li>同时隔离笔记、用户，会不会让内容池变小？</li>
<li>如果对新笔记做保量，会发生什么？</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Indicator-Growth-Skills"><a href="#Indicator-Growth-Skills" class="headerlink" title="Indicator Growth Skills"></a>Indicator Growth Skills</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul>
<li><p>核心评价指标</p>
<ul>
<li>DAU和留存是最核心的指标，电商不太一样，营收是最核心的指标。</li>
<li>目前工业界最常用LT7和LT30衡量留存<ul>
<li>某用户今天(t0)登录APP,未来7天(t0~t6)中有4天，登录APP,那么该用户今天(t0)的LT7等于4。</li>
<li>显然有1≤LT7≤7和1≤LT30≤30。</li>
<li>算法工程师最重要的目标就是提升LT，LT增长一般都意味着用户体验的提升。（除非LT增长而DAU下降，这意味着策略赶走了不活跃的用户，不好！）</li>
<li>假设APP禁止低活用户登录，则DAU下降，LT增长。</li>
</ul>
</li>
<li>其他核心指标：用户使用时长、总阅读数（即总，点击数）、总曝光数。这些指标的重要性低于DAU和留存。<ul>
<li>时长增长通常和LT是正相关的，时长提升了，LT通常也会提升。</li>
<li>时长增长，阅读数、曝光数可能会下降。比如抖音长视频增多，短视频减少。总时长增加，曝光数减少。</li>
<li>时长关系到DAU和留存，曝光影响到广告收入，需要平衡。</li>
</ul>
</li>
<li>非核心指标：点击率、交互率、等等。核心上涨就行，不核心在核心上涨的情况下，下跌也没关系。</li>
<li>对于UGC平台，发布量和发布渗透率也是核心指标。</li>
</ul>
</li>
<li><p>涨指标的方法：</p>
<ol>
<li>改进召回模型，添加新的召回模型。</li>
<li>改进粗排和精排模型。</li>
<li>提升召回、粗排、精排中的多样性。</li>
<li>特殊对待新用户、低活用户等特殊人群。</li>
<li>利用关注、转发、评论这三种交互行为。</li>
</ol>
</li>
<li><p>前两部分很详细，有公开资料，但是后面的三个都是工业界的实际经验，不好获取。其他公开渠道也很难找到。</p>
</li>
</ul>
<h3 id="Recall-1"><a href="#Recall-1" class="headerlink" title="Recall"></a>Recall</h3><ul>
<li><p>推荐系统有几十条召回通道，它们的召回总量是固定的。总量越大，指标越好，粗排计算量越大。如果总量为5000，那这5000要给几十条召回通道分配。</p>
</li>
<li><p>双塔模型 (two-tower)和item-to-item (I2I)是最重要的两类召回模型，占据召回的大部分配额。有很多小众的模型，占据的配额很少。在召回总量不变的前提下，添加某些召回模型可以提升核心指标。这些模型的添加，不影响粗排的计算量，但是会挤兑别的模型的配额。</p>
</li>
<li><p>有很多内容池，比如30天物品、1天物品、6小时物品、新用户优质内容池、分人群内容池。后面会讲为啥需要这么多内容池。同一个模型可以用于多个内容池，得到多条召回通道。例如：只训练一个双塔模型，可以用在三个内容池上，得到三条召回通道，每条召回通道都有一定的配额。无论多少个内容池，都只训练一个双塔，不会增加训练的计算量。但是每个内容池都需要一个N索引，还需要线上做N检索，增加少量的计算成本。</p>
</li>
<li><p>双塔模型</p>
<ul>
<li><p>方向1：优化正样本、负样本</p>
<ul>
<li>简单正样本：有点击的（用户，物品)二元组。</li>
<li>简单负样本：随机组合的（用户，物品)二元组。</li>
<li>困难负样本：排序靠后的（用户，物品)二元组。被召回说明有些兴趣，排序被淘汰，则说明用户对这个物品的兴趣不大，可以作为负样本。</li>
</ul>
</li>
<li><p>方向2：改进神经网络结构</p>
<ul>
<li><p>Baseline：用户塔、物品塔分别是全连接网络，各输出一个向量，分别作为用户、物品的表征。</p>
</li>
<li><p>改进：用户塔、物品塔分别用DCN代替全连接网络。</p>
</li>
<li><p>改进：在用户塔中使用用户行为序列(last-n)。</p>
</li>
<li><p>改进：使用多向量模型代替单向量模型。（标准的双塔模型也叫单向量模型。)</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202409242058306.png" srcset="/img/loading.gif" lazyload alt="image-20240924205803168" style="zoom:50%;">

<blockquote>
<ul>
<li>物品塔是一样的，只输出一个向量。用户塔会生成多个向量，每个向量的形状和物品塔输出的向量一致，就可以和物品塔的输出，算出很多的指标。有多少个指标，用户塔就会输出多少个向量。</li>
<li>Why一个物品，多个用户？用户少，物品多，离线算出来存进向量数据库，如果每个指标，物品都要生成一个向量的话，存入对应的向量数据库的话，代价太大了（计算倍乘、存储也倍乘、向量数据库倍乘）。</li>
</ul>
</blockquote>
</li>
</ul>
</li>
<li><p>方向3：改进模型的训练方法</p>
<ul>
<li>Baseline：做二分类，让模型学会区分正样本和负样本。</li>
<li>改进：结合二分类、batch内负采样。（对于batch内负采样，需要做纠偏。)</li>
<li>改进：使用自监督学习方法，让冷门物品的embedding学得更好。</li>
</ul>
</li>
</ul>
</li>
<li><p>Item-to-Item (I2I)</p>
<ul>
<li>几乎和双塔同样重要，但是已经非常成熟了，能做的改进不太多。</li>
<li>I2I是一大类模型，基于相似物品做召回。</li>
<li>最常见的用法是U2I2I(user&gt;item&gt;item)。<ul>
<li>用户u喜欢物品i1（用户历史上交互过的物品)。</li>
<li>寻找i1的相似物品i2（即I2I)</li>
<li>将i2推荐给u</li>
</ul>
</li>
<li>方法1：ItemCF及其变体。<ul>
<li>一些用户同时喜欢物品i1和i2，则认为i1和i2相似。</li>
<li>ItemCF、Online ItemCF、Swing、Online Swing都是基于相同的思想，只不过细节有些区别。</li>
<li>四种模型的结果存在足够大的差异，所以结合起来效果更好。同时用四种比只有一两种的效果更好。</li>
<li>线上同时使用上述4种I2I模型，各分配一定配额，各自的配额需要好好调一下。</li>
</ul>
</li>
<li>方法2：基于物品向量表征，计算向量相似度。（双塔模型、图神经网络均可计算物品向量表征。)</li>
</ul>
</li>
<li><p>小众的召回模型：</p>
<ul>
<li><p>类似I2I的模型</p>
<ul>
<li>U2U2I(user -&gt; user -&gt; item)：已知用户u1与u2相似，且u2喜欢物品i, 那么给用户u1推荐物品i。</li>
<li>U2A2I(user -&gt; author -&gt; item)：已知用户u喜欢作者a,<br>且a发布物品i，那么给用户u推荐物品i。</li>
<li>U2A2A2I(user -&gt; author -&gt; author -&gt; item)：已知用户u喜欢作者a1，且a1与a2相似（不一定喜欢或者关注了a2），a2发布物品i，那么给用户u推荐物品i。</li>
</ul>
</li>
<li><p>更复杂的模型：</p>
<ul>
<li>Path-based Deep Network (PDN)[1]</li>
<li>Deep Retrieval [2]</li>
<li>Sparse-Interest Network (SINE)[3]</li>
<li>Multi-task Multi-view Graph Representation Learning (M2GRL)[4]</li>
</ul>
<blockquote>
<p>参考文献：</p>
<ol>
<li>Li et al.Path-based Deep Network for Candidate Item Matching in Recommenders.In S/G/R,2021</li>
<li>Gao et al.Learning an end-to-end structure for retrieval in large-scale recommendations.In C/KM, 2021.</li>
<li>Tan et al.Sparse-interest network for sequential recommendation.InWSDM,2021.</li>
<li>Wang et al.M2GRL:A multitask multi-view graph representation learning framework for web-scale recommender systems.In KDD,2020.</li>
</ol>
</blockquote>
</li>
<li><p>工业界实践起来确实都有效，配额都很小，但是确实都有效果。召回总量不变的情况下，添加一些这样的通道，可以涨大盘指标！</p>
</li>
</ul>
</li>
<li><p>总结：改进召回模型</p>
<ul>
<li>双塔模型：优化正负样本、改进神经网络结构、改进训练的方法。</li>
<li>I2I模型：同时使用ItemCF及其变体、使用物品向量表征计算物品相似度。</li>
<li>添加小众的召回模型，比如PDN、Deep Retrieval、SINB、M2GRL等模型。</li>
<li>在召回总量不变的前提下，仔细调整不同召回通道的配额，可以提升核心指标。（甚至可以让各用户群体用不同的配额。比如新用户和普通用户这两个群体)</li>
<li>增加召回总量显然可以提升指标，召回越多，粗排计算量越大。召回总量大到一定程度的时候，继续增加召回总量的边际效益会很小，ROI不划算。在s</li>
</ul>
</li>
</ul>
<h3 id="Sort-1"><a href="#Sort-1" class="headerlink" title="Sort"></a>Sort</h3><h4 id="Fine-Ranking"><a href="#Fine-Ranking" class="headerlink" title="Fine Ranking"></a>Fine Ranking</h4><ul>
<li><p>精排模型的大致结构：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202411041349003.png" srcset="/img/loading.gif" lazyload alt="image-20241104134949164" style="zoom:50%;">
</li>
<li><p>模型介绍：</p>
<ul>
<li><p>输入大致可以分为：离散特征和连续特征。</p>
<ul>
<li>离散特征：神经网络用Embedding层把离散特征映射到数值向量，把得到的数值向量拼接起来，得到一个几千维度的向量。再经过几个全连接层，得到一个大小是几百维的向量（上面绿色的）</li>
<li>连续特征：连续特种不多，也就是几百个（上面蓝色的）。蓝色神经网络是输入是个几百维的向量，输出也是个几百维的向量。</li>
</ul>
</li>
<li><p>算力限制，下面这俩神经网络都不会很大。CPU推理的话，算力不够，下面这里全连接网络很小，基本只有1-2层，GPU推理能力更强，一般可以有3-6层。绿色/蓝色的向量做concatenation，作为更上层的神经网络的输入。基座：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202411041356908.png" srcset="/img/loading.gif" lazyload alt="image-20241104135655861" style="zoom:50%;">

<blockquote>
<p>基座的作用就是把原始特征映射到数值向量。</p>
</blockquote>
</li>
<li><p>concetenation后的结果会同时输入到多个全连接网络</p>
<ul>
<li>这些网络一般只有两层。</li>
<li>这些网络的输出都是介于0到1之间的数值，作为各种目标的预估，比如预告点击率，点赞率，转发率，评论率等。</li>
<li>精排模型的基座和上面的多目标预估都有很多可以改进的点。</li>
</ul>
</li>
</ul>
</li>
<li><p>基座：</p>
<ul>
<li>基座的输入包括离散特征和连续特征，输出一个向量，作为多目标预估的输入。</li>
<li>改进1：基座加宽加深，计算量更大，预测更准确。<ul>
<li>精排模型的网络不够大，通常会underfit，推荐系统的参数亮很大的，通常有千亿/万亿，99%都在Embedding层，全连接参数量都很小。数据量太大，但是全连接网络不够大，加宽加深就会让预测更准。</li>
<li>加宽加深计算量也会加大，这就有ROI的问题了。</li>
<li>工业界常用的也就1-6层，取决于训练和推理的架构水平。算力强/架构先进，就可以6层，否则可能就1-2层。</li>
</ul>
</li>
<li>改进2：做自动的特征交叉，比如Bilinear[1]和LHUC[2]。<ul>
<li>参考文献:<ul>
<li>1.Huang et al.FiBiNET:combining feature importance and bilinear feature interaction<br>for click-through rate prediction.In RecSys,2019.</li>
<li>2.Swietojanski et al.Learning hidden unit contributions for unsupervised acoustic model adaptation.In WSDM,2016.</li>
</ul>
</li>
<li>特征交叉实际用起来确实可以涨指标。</li>
</ul>
</li>
<li>改进3：特征工程，比如添加统计特征、多模态内容特征。需要算法工程师根据自己的经验设计特征，判断哪些特征对预估有用，哪些特征可以做交叉。</li>
</ul>
</li>
<li><p>多目标预估：</p>
<ul>
<li>基于基座输出的向量，同时预估，点击率等多个目标</li>
<li>改进1：增加新的预估目标，并把预估结果加入融合公式。<ul>
<li>最标准的目标包括，点击率、点赞率、收藏率、转发率、评论率、关注率、完播率…</li>
<li>寻找更多目标，比如进入评论区、给他人写的评论，点赞…</li>
<li>把新的预估目标加入融合公式，排序的时候会用到这些新的目标。算法工程师会寻找用户兴趣相关的目标，添加这样的目标可以提升留存等指标。</li>
</ul>
</li>
<li>改进2：MMoE、PLE等结构可能有效，但往往无效。根据老师的经验，有些公司这样做涨了指标，但很多人试了都没有收益，无效的话不用感到惊讶。</li>
<li>改进3：纠正position bias，可能有效，但无效的可能性更大。老师的经验，观察到的数据有很强的position bias，但是很多同事花了很多精力去做也没啥用。</li>
</ul>
</li>
</ul>
<blockquote>
<p>上面的精排方法，很多可以用于粗排，例如自动特征交叉，人工特征工程，还有增加新的预估目标等。</p>
</blockquote>
<h4 id="Coarse-Ranking"><a href="#Coarse-Ranking" class="headerlink" title="Coarse Ranking"></a>Coarse Ranking</h4><ul>
<li>粗排的打分量比精排大10倍（这样才能形成漏斗），因此粗排模型必须够快。大分量 x 10，那么单个物品的计算量就要 / 10，避免消耗太大的算力。</li>
<li>粗排不用和精排一样准确，因此简单点可以：<ul>
<li>简单模型：多向量双塔模型，同时预估点击率等多个目标。</li>
<li>复杂模型：三塔模型效果好，但工程实现难度较大。</li>
</ul>
</li>
<li>粗精排一致性建模<ul>
<li>蒸馏精排训练粗排，让粗排与精排更一致。</li>
<li>方法1：pointwise蒸馏<ul>
<li>设y是用户真实行为，设p是精排的预估·</li>
<li>用$\frac{y+p}{2}$作为粗排拟合的目标。不用蒸馏的话，训练直接用y作为目标，但是用均值比直接用y效果更好哦！</li>
<li>例：<ul>
<li>对于点击率目标，用户有点击（y=1），精排预估p=0.6。</li>
<li>用$\frac{y+p}{2}=0.8$作为粗排拟合的点击率目标。</li>
</ul>
</li>
</ul>
</li>
<li>方法2：pairwise或listwise蒸馏<ul>
<li>给定k个候选物品，按照精排预估做排序。</li>
<li>做learning to rank(LTR)，让粗排拟合物品的序（而非具体的点击率数值)</li>
<li>例：<ul>
<li>对于物品i和j，精排预估点击率为$p_i&gt;p_j$</li>
<li>LTR鼓励粗排预估点击率满足$q_i &gt; q_j$，否则有惩罚</li>
<li>LTR通常使用pairwise logistic loss作为损失函数</li>
</ul>
</li>
</ul>
</li>
<li>优点：粗精排一致性建模可以提升核心指标，效果十分显著。做简单的pointwist蒸馏效果就很好了，大厂会用更复杂的pairwise/listwist。</li>
<li>缺点：用精排蒸馏粗排有缺点，系统很大很杂，如果精排出bug（很多时候bug造成的指标缓慢下降，还很难感知到），精排预估值p有偏，会污染粗排训练数据，让粗排也逐渐变差，而且不容易被察觉到。</li>
</ul>
</li>
</ul>
<h4 id="User-Behavior-Sequence-Modeling"><a href="#User-Behavior-Sequence-Modeling" class="headerlink" title="User Behavior Sequence Modeling"></a>User Behavior Sequence Modeling</h4><blockquote>
<p>排序到头了越来越难了，这时候涨指标最重要的途径就是用户行为建模</p>
</blockquote>
<ul>
<li>用户行为序列建模（前面都讲过）<ul>
<li>最简单的方法是对物品向量取平均，作为一种用户特征。</li>
<li>DIN 使用注意力机制，对物品向量做加权平均。</li>
<li>工业界目前沿着SIM的方向发展。先用类目等属性筛选物品，然后用DIN对物品向量做加权平均。</li>
</ul>
</li>
</ul>
<blockquote>
<p>参考文献</p>
<ol>
<li>Covington,Adams,and Sargin.Deep neural networks for YouTube recommendations. In RecSys,2016.</li>
<li>Zhou et al.Deep interest network for click-through rate prediction.In KDD,2018.</li>
<li>Qi et al.Search-based User Interest Modeling with Lifelong Sequential Behavior Data for Click-Through Rate Prediction.In CIKM,2020.</li>
</ol>
</blockquote>
<ul>
<li>改进方向：<ul>
<li>改进1：增加序列长度，让预测更准确，但是会增加计算成本和推理时间。难点是工程架构，工程架构弱做不到长序列。现在长序列建模做的最好的是快手，超长序列建模可以使用100w+个物品，几乎可以覆盖用户历史上的全部行为。非常难！！！算法原理简单，但是鸡架不行的话，扛不住。序列长度很长靠的还是工程架构哈！</li>
<li>改进2：筛选的方法，比如用类目、物品向量表征聚类。100w+个全塞进去不符合实际，筛选一把！着重物品向量表征聚类：<ul>
<li>离线用多模态神经网络提取物品内容特征，将物品表征为向量。用Bert或者Clip这样的模型。</li>
<li>离线将物品向量聚为1000类，每个物品有一个聚类序号。（聚类一般用层次聚类）</li>
<li>线上排序时，用户行为序列中有n=1,000,000个物品。某候选物品的聚类序号是70，对个物品做筛选，只保留聚类序号为70的物品。n个物品中只有数千个被保留下来。</li>
<li>同时有好几种筛选方法，取筛选结果的并集。</li>
<li>可能还要进一步筛选，让物品数量再降低一个数量级，然后再输入到注意力层。</li>
</ul>
</li>
<li>改进3：对用户行为序列中的物品，使用ID以外的一些特征。<ul>
<li>最简单就是ID做Embedding，作为物品的向量表征。</li>
<li>还可以再加一些别的物品特征，效果会更好。但是不能加太多，不然线上推理的时候，通信和计算都会出问题。</li>
</ul>
</li>
<li>概括：沿着SIM的方向发展，让原始的序列尽量长，然后做筛选降低序列长度，最后将筛选结果输入DIN。</li>
</ul>
</li>
</ul>
<h4 id="Online-Learning"><a href="#Online-Learning" class="headerlink" title="Online Learning"></a>Online Learning</h4><ul>
<li><p>在线学习对于推荐系统的提升非常大，但是它会消耗更多的算力</p>
<ul>
<li><p>既需要在凌晨做全量更新，也需要全天不间断做增量更新。</p>
</li>
<li><p>设在线学习需要10,000 CPU core的算力增量更新一个精排模型。推荐系统一共需要多少额外的算力给在线学习？</p>
</li>
<li><p>为了做AB测试，线上同时运行多个不同的模型。</p>
</li>
<li><p>如果线上有m个模型’则需要m套在线学习的机器。</p>
</li>
<li><p>线上有m个模型，其中1个是holdout,1个是推全的模型，m一2个测试的新模型。</p>
</li>
<li><p>以精排为例子：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202411140841427.png" srcset="/img/loading.gif" lazyload alt="image-20241114084116212" style="zoom:50%;">

<blockquote>
<p>holdout一个，推全一个，新模型俩，总共四套资源。推全和holdout的模型不一定一致，一般推全会新几个版本。而且要跑一段时间的，七日留存？三十日留存？资源会在一段时间内被占用。</p>
</blockquote>
</li>
<li><p>同样的道理，召回和粗排也需要做在线学习。每个模型都需要一套计算资源。好在召回和粗排的模型小，资源需要的少一些。</p>
</li>
</ul>
</li>
<li><p>总结：</p>
<ul>
<li>线上有m个模型，其中1个是holdout,1个是推全的模型，m一2个测试的新模型。</li>
<li>每套在线学习的机器成本都很大，因此m数量很小，制约模型开发迭代的效率。</li>
<li>在线学习对指标的提升巨大，但是会制约模型开发迭代的效率。最好在模型相对成熟之后再考虑在线学习，过早在线学习容易把模型锁死在较弱的版本，后续迭代会很慢。</li>
</ul>
</li>
</ul>
<h4 id="Old-Soup-Model"><a href="#Old-Soup-Model" class="headerlink" title="Old Soup Model"></a>Old Soup Model</h4><blockquote>
<p>算法工程师遇到的最麻烦的一个问题，解决不了的话，新模型很难超过老模型。</p>
</blockquote>
<ul>
<li><p>老汤模型是啥？</p>
<ul>
<li><p>“老汤模型”这个术语用来描述在推荐系统中，一个已经经过长时间训练和更新的模型，它已经积累了大量的数据和经验，因此很难被新模型超越，即使新模型在理论上可能更先进 。这种现象在模型更新和比较时需要特别注意，以确保公平性 。</p>
</li>
<li><p>用每天新产生的数据对模型做1 epoch的训练。</p>
</li>
<li><p>久而久之，老模型训练得非常好，很难被超过。</p>
</li>
<li><p>对模型做改进’重新训练’很难追上老模型…</p>
</li>
</ul>
</li>
<li><p>问题：</p>
<ul>
<li>问题1：如何快速判断新模型结构是否优于老模型？（不需要追上线上的老模型，只需要判断新老模型谁的结构更优。)</li>
<li>问题2：如何更快追平、超过线上的老模型？（只有几十天的数据，新模型就能追上训练上百天的老模型。)</li>
</ul>
</li>
<li><p>问题一解决方案：</p>
<ul>
<li>核心思想：“拉回同一起跑线”</li>
<li>对于新、老模型结构，都随机初始化模型全连接层。</li>
<li>Embedding层可以是随机初始化，也可以是复用老模型训练好的参数。</li>
<li>用n天的数据训练新老模型。（从旧到新，训练1 epoch)。</li>
<li>如果新模型显著优于老模型，新模型很可能更优。</li>
<li>只是比较新老模型结构谁更好，而非真正追平老模型。</li>
</ul>
</li>
<li><p>问题二解决方案：</p>
<ul>
<li>已经得出初步结论，认为新模型很可能优于老模型。用几十天的数据训练新模型’早日追平老模型。</li>
<li>方法1：尽可能多地复用老模型训练好的embedding层，避免随机初始化embedding层。(Embedding层是对用户、物品特点的“记忆”，比全连接层学得慢。)</li>
<li>方法2：用老模型做teacher，蒸馏新模型。（用户真实行为是y，老模型的预测是p，用$\frac{y+p}{2}$作为训练新模型的目标。）在训练新模型的初期，做蒸馏，可以大幅增加收敛，让新模型追的更快。</li>
</ul>
</li>
</ul>
<h4 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h4><ul>
<li>精排模型：改进模型基座（加宽加深、特征交叉、特征工程)，改进多目标预估（增加新目标、MMoE、position bias)。并非都有效，做了实验才知道。</li>
<li>粗排模型：更好的模型结构，例如三塔模型（取代多向量双塔模型)，粗精排一致性建模。</li>
<li>用户行为序列建模：沿着SIM的方向迭代升级，加长序列长度，改进筛选物品的方法。</li>
<li>在线学习：对指标提升大，但是会降低模型迭代升级效率。</li>
<li>老汤模型制约模型迭代升级效率，需要特殊技巧。</li>
</ul>
<h3 id="Diversity"><a href="#Diversity" class="headerlink" title="Diversity"></a>Diversity</h3><h4 id="Diversity-in-Fine-Ranking"><a href="#Diversity-in-Fine-Ranking" class="headerlink" title="Diversity in Fine Ranking"></a>Diversity in Fine Ranking</h4><ul>
<li>精排阶段，结合兴趣分数和多样性分数对物品$i$排序。<ul>
<li>$s_i$: 兴趣分数，即融合点击率等多个预估目标的分数。</li>
<li>$d_i$: 多样性分数，即物品$i$和已经选中的物品的差异。</li>
<li>用$s_{i} + d_{i}$对物品做排序，这个排序几乎决定了最终用户看到的推荐结果。</li>
</ul>
</li>
<li>常用MMR、DPP等方法计算多样性分数，精排使用滑动窗口，粗排不使用滑动窗口。<ul>
<li>精排决定最终的曝光，曝光页面上邻近的物品相似度应该小。所以计算精排多样性要使用滑动窗口。</li>
<li>例如：一个物品要和前五个物品有很大的差异，如果相似物品离得太近，就会影响用户体验。所以计算精排多样的时候，要使用滑动窗口，保证同一个滑动窗口内的物品要有足够大的差异。</li>
<li>粗排不决定最终曝光，只是给精排提供候选。粗排要考虑整体的多样性，而非一个滑动窗口中的多样性。</li>
</ul>
</li>
<li>除了多样性分数，精排还使用打散策略增加多样性。<ul>
<li>类目：当前选中物品$i$，之后5个位置不允许跟$i$的二级类目相同。</li>
<li>多模态：事先计算物品多模态内容向量表征，将全库物品聚为1000类；在精排阶段，如果当前选中物品$i$，之后10个位置不允许跟$i$同属一个聚类。道理：同一个聚类中的物品的图片和文字相似，应该被打散。</li>
</ul>
</li>
</ul>
<h4 id="Diversity-in-Coarse-Ranking"><a href="#Diversity-in-Coarse-Ranking" class="headerlink" title="Diversity in Coarse Ranking"></a>Diversity in Coarse Ranking</h4><blockquote>
<p>也很重要</p>
</blockquote>
<ul>
<li>粗排给5000个物品打分，选出500个物品送入精排。提示这500个多样性有助于提升推荐系统核心指标，提升粗排和精排多样性都可以提升推荐系统核心指标。</li>
<li>根据$s_i$对5000个物品排序，分数最高的200个物品送入<br>精排。（优先考虑兴趣分数，不可虑多样性分数，保证用户最感兴趣的物品可以进入精排）</li>
<li>对于剩余的4800个物品，对每个物品$i$计算兴趣分数$s_i$和多样性分数$d_i$，根据$s_i + d_i$，对于剩下4800个物品排序，分数最高的300个送入精排。（既是用户比较感兴趣的，也跟已经选中的物品有较大的差异，保证了多样性。）</li>
</ul>
<h4 id="Diversity-in-Recall"><a href="#Diversity-in-Recall" class="headerlink" title="Diversity in Recall"></a>Diversity in Recall</h4><ul>
<li><p>双塔模型：添加噪声</p>
<ul>
<li>双塔模型是最重要的召回模型，占的召回名额最多。</li>
<li>用户塔将用户特征作为输入，输出用户的向量表征；然后做ANN检索，召回向量相似度高的物品。</li>
<li>线上做召回时（在计算出用户向量之后，在做ANN检索之前)，往用户向量中添加随机噪声（比如随机高斯噪声就行）。线上真有利于推荐系统核心指标！！！</li>
<li>噪声多强取决于用户的兴趣，用户的兴趣越窄（比如用户最近交互的个物品只覆盖少数几个类目)，则添加的噪声越强。可以拿用户最后交互过的n个物品，来衡量用户的兴趣宽窄。</li>
<li>添加噪声让召回变得“不准”，但是添加噪声确实使得召回的物品更多样，可以提升推荐系统核心指标。</li>
</ul>
</li>
<li><p>双塔模型：抽样用户行为序列</p>
<ul>
<li>用户最近交互的n个物品（用户行为序列)，是用户塔的输入。保留最近的r个物品(r &lt;&lt; n)。</li>
<li>从剩余的n一T个物品中随机抽样t个物品（t &lt;&lt; n）。（可以是均匀抽样，也可以用非均匀抽样让类目平衡）</li>
<li>将得到的r+t个物品作为用户行为序列，而不是用全部个物品。（具有随机性和差异，多样性up）</li>
<li>抽样用户行为序列为什么能涨指标？<ul>
<li>一方面，注入随机性，召回结果更多样化。</li>
<li>另一方面，可以非常大，可以利用到用户很久之前的兴趣。</li>
</ul>
</li>
<li>概括一下：提升多样性 + 捕捉更久之前的兴趣点，提升指标。</li>
</ul>
</li>
<li><p>U2I2I：抽样用户行为序列</p>
<ul>
<li>U212 (user→item→item) 中的第一个item是指用户最近交互的n个物品之一，在U2I2I中叫作<strong>种子物品</strong>。</li>
<li>N个物品覆盖的类目数较少，且类目不平衡。<ul>
<li>e.g. 系统共有200个类目，某用户的个物品只覆盖15个类目。用户兴趣不是很宽泛。</li>
<li>e.g. 足球类目的物品有0.4n个，电视剧类目的物品有0.2n个，其余类目的物品数均少于0.05n个。</li>
<li>用这些种子召回出来的结果，也具有类目较少且不平衡的特性。</li>
</ul>
</li>
<li>做非均匀随机抽样，从个物品中选出t个，让类目平衡。(想法和效果与双塔中的用户行为序列抽样相似。)用抽样得到的t个物品（代替原本的n个物品）作为U2I2I的种子物品。这样召回的结果会更多样一些。</li>
<li>可以提升指标！！！Reason:<ul>
<li>一方面，抽样可以让类目更平衡，多样性更好。</li>
<li>另一方面，n可以更大（因为只从中抽一个子集出来做输入），覆盖的类目数量会大很多，多样性会更好。</li>
</ul>
</li>
</ul>
</li>
<li><p>探索流量</p>
<ul>
<li>每个用户曝光的物品中有2%是非个性化的，用作兴趣探索。</li>
<li>维护一个精选内容池，其中物品均为交互率指标高的优质物品。（内容池可以分人群，比如30~40岁男性内容池，专门推荐给30～40岁男性用户)</li>
<li>为什么要精选内容池？没有个性化 -&gt; 提高物品质量，要通过提高质量来弥补缺少个性化带来的损失。</li>
<li>从精选内容池中随机抽样几个物品，跳过排序，直接插入最终排序结果。没办法对这样的物品做公平的排序，这样的物品不匹配用户过去的兴趣点，大概率会被排序模型给淘汰掉。所以只能做提权或者强插。</li>
<li>兴趣探索一定会在短期内负向影响核心指标，但长期会产生正向影响。道理很显然：非个性化推荐用户大概率不感兴趣，点击率偏低用户不看。2%的流量大部分会被浪费掉，但是长期有利，可以发觉用户的兴趣点，更好吸引用户留存。</li>
</ul>
</li>
<li><p>总结：</p>
<ul>
<li>精排：结合兴趣分数和多样性分数做排序；做规则打散·</li>
<li>粗排：只用兴趣分数选出部分物品；结合兴趣分数和多样性分数选出部分物品。</li>
<li>召回：往双搭模型的用户向量添加噪声；对用户行为序列做非均匀随机抽样（对双塔和U2I2I都适用)</li>
<li>兴趣探索：保留少部分的流量给非个性化推荐。长期有好处。</li>
</ul>
</li>
</ul>
<h3 id="Special-treatment-for-special-groups"><a href="#Special-treatment-for-special-groups" class="headerlink" title="Special treatment for special groups"></a>Special treatment for special groups</h3><ul>
<li><p>Why</p>
<ul>
<li>新用户、低活用户的行为很少,个性化推荐不准确。</li>
<li>新用户、低活用户容易流失，要想办法促使他们留存。指标有区别，特殊策略。<ul>
<li>全体用户，很多指标：留存，时长，阅读，消费。。。</li>
<li>新用户/低活用户：留存，不用考虑别的。留下来先。</li>
</ul>
</li>
<li>特殊用户的行为（比如点击率、交互率)不同于主流用户，基于全体用户行为训练出的模型在特殊用户人群上有偏。</li>
</ul>
</li>
<li><p>涨指标的方法</p>
<ul>
<li>构造特殊内容池，用于特殊用户人群的召回。</li>
<li>使用特殊排序策略，保护特殊用户。</li>
<li>使用特殊的排序模型，消除模型预估的偏差。</li>
</ul>
</li>
<li><p>构造特殊的内容池</p>
<ul>
<li>why<ul>
<li>新用户、低活用户的行为很少，个性化召回不准确。（既然个性化不好，那么就保证内容质量好。优质内容弥补个性化的缺失)</li>
<li>针对特定人群的特点构造特殊内容池，提升用户满意度。（例如，对于喜欢留评论的中年女性，构造促评论内容池，满足这些用户的互动需求。)</li>
</ul>
</li>
<li>how<ul>
<li>方法1：根据物品获得的交互次数、交互率选择优质物品。<ul>
<li>圈定人群：只考虑特定人群，例如18~25岁一二线城市男性。</li>
<li>构造内容池：用该人群对物品的交互次数、交互率给物品打分，选出分数最高的物品进入内容池.</li>
<li>内容池有弱个性化的效果。</li>
<li>内容池定期更新，加入新物品，排除交互率低和失去时效性的老物品。</li>
<li>该内容池只对该人群生效。</li>
</ul>
</li>
<li>方法2：做因果推断，判断物品对人群留存率的贡献，根据贡献值选物品。（工业界已经在用了，但是技术还是不算成熟。）</li>
</ul>
</li>
</ul>
</li>
<li><p>特殊内容池的召回</p>
<ul>
<li>通常使用双塔模型从特殊内容池中做召回<ul>
<li>双塔模型是个性化的</li>
<li>对于新用户，双塔模型的个性化做不准。</li>
<li>靠高质量内容、弱个性化做弥补。容忍双塔的不准确。</li>
</ul>
</li>
<li>额外的训练代价？<ul>
<li>对于正常用户，不论有多少内容池，只训练一个双塔模型。</li>
<li>对于新用户，由于历史交互记录很少，需要单独训练模型。</li>
</ul>
</li>
<li>额外的推理代价？<ul>
<li>内容池定期更新，然后要更新ANN索引。</li>
<li>线上做召回时，需要做ANN检索。（少量的算力），每多一个内容池，都要多一份算力，内容池越大需要的算力也就越大。</li>
<li>特殊内容池都很小（比全量内容池小10~100倍)，所以需要的额外算力不大。</li>
</ul>
</li>
</ul>
</li>
<li><p>特殊的排序策略</p>
<ul>
<li>对于新用户、低活用户这样的特殊人群，业务上只关注留存，不在乎消费（总曝光量、广告收入、电商收入)</li>
<li>对于新用户、低活用户，少出广告、甚至不出广告。</li>
<li>新发布的物品不在新用户、低活用户上做探索。<ul>
<li>物品新发布时，推荐做得不准，会损害用户体验。</li>
<li>只在活跃的老用户上做探索，对新物品提权(boost)。</li>
<li>不在新用户、低活用户上做探索，避免伤害用户体验。</li>
</ul>
</li>
</ul>
</li>
<li><p>差异化的融分公式</p>
<ul>
<li>新用户、低活用户的，点击、交互行为不同于正常用户。</li>
<li>低活用户的人均点击量很小；没有点击就不会有进一步的交互。</li>
<li>低活用户的融分公式中，提高预估点击率的权重（相较于普通用户)，降低交互率的权重。这样更容易促使低活用户点击物品，点进去看了才有可能留存。</li>
<li>保留几个曝光坑位给预估点击率最高的几个物品。<ul>
<li>例：精排从500个物品中选50个作为推荐结果，其中3个坑位给点击率最高的物品（吸引用户点击），剩余47个坑位由融分公式决定。</li>
<li>甚至可以把，点击率最高的物品排在第一，确保用户一定能看到。（大厂实验结论：确实可以提高用户留存，有效！只对低活人群生效，用在活跃用户身上则会损害指标。）</li>
</ul>
</li>
</ul>
</li>
<li><p>特殊的排序模型</p>
<ul>
<li><p>特殊用户人群的行为不同于普通用户。新用户、低活用户的，点击率、交互率偏高或偏低。</p>
</li>
<li><p>排序模型被主流用户主导，对特殊用户做不准预估。</p>
<ul>
<li>用全体用户数据训练出的模型，给新用户做的预估有严重偏差。</li>
<li>如果一个APP的用90%是女性，用全体用户数据训练出的模型，对男性用户做的预估有偏差。</li>
</ul>
</li>
<li><p>问题：对于特殊用户，如何让排序模型预估做得准？</p>
</li>
<li><p>差异化的排序模型：</p>
<ul>
<li>方法1：大模型+小模型<ul>
<li>用全体用户行为训练大模型，大模型的预估拟合用户行为y。</li>
<li>用特殊用户的行为训练小模型，小模型的预估q，拟合大模型的残差y-p。希望小模型纠正大模型犯的错误。</li>
<li>对主流用尸只用大模型做预估p。</li>
<li>对特殊用户，结合大模型和小模型的预估<code>p + q</code>，小模型起到纠偏的作用。</li>
</ul>
</li>
<li>方法2：融合多个experts,类似MMoE。<ul>
<li>只用一个模型，模型有多个experts,各输出一个向量。</li>
<li>对experts的输出做加权平均，得到一个向量。</li>
<li>和MMOE的不同：MMOE有一个小神经网络，全部特征作为输入，输出是experts的权重。这里的区别：根据用户特征计算权重，不用其他的特征。由用户特征决定experts的权重。</li>
<li>以新用户为例，模型将用户的新老、活跃度等特征作为输入，输出权重，用于对experts做加权平均。</li>
</ul>
</li>
<li>方法3：大模型预估之后，用小模型做校准。<ul>
<li>用大模型预估点击率、交互率。大模型大量数据，在全体数据上进行训练得到的，对于大量活跃用户比较准。</li>
<li>将用户特征、大模型预估，点击率和交互率作为小模型（例如GBDT）的输入。</li>
<li>在特殊用户人群的数据上训练小模型，小模型的输出拟合用户真实行为。</li>
<li>这里的小模型：结合大模型的预估和用户特征特征，小模型再做一次预估，起到校准的作用。</li>
<li>特殊人群上，小模型的预估比大模型更加准确。直接就用小模型的预估了！</li>
</ul>
</li>
<li>注意方法1、3的区别：一个是校准，一个是纠偏。一个是并行，两个一起使用，结合起来作为最终结果。一个是串行，先小后大，作为结果。</li>
</ul>
</li>
<li><p>错误的做法</p>
<ul>
<li>每个用户人群使用一个排序模型，推荐系统同时维护多个大模型。<ul>
<li>系统有一个主模型；每个用户人群有自己的一个模型。</li>
<li>每天凌晨，用全体用户数据更新主模型。</li>
<li>基于训练好的主模型，在某特殊用户人群的数据上再训练 1 epoch,作为该用户人群的模型。</li>
</ul>
</li>
<li>短期可以提升指标；维护代价大’长期有害。<ul>
<li>起初，低活男性用户模型比主模型的AUC高0.2%。</li>
<li>主模型迭代几个版本后，AUC累计提升0.5%</li>
<li>特殊人群模型太多，长期没有人维护和更新。</li>
<li>如果把低活男性用户模型下线，换成主模型，在低活男性用户上的AUC反倒提升0.3%！</li>
<li>看似win-win，其实推荐系统前后没有任何改变啊，上线了又下线了这。。。浪费了人的时间和机器的算力，还不如直接弄主模型呢orz。</li>
</ul>
</li>
<li>不建议让每个用户人群都有自己的排序模型，最好只有一个统一的模型，最多再加几个小模型，对主模型做纠偏和校准。</li>
</ul>
</li>
</ul>
</li>
<li><p>总结：</p>
<ul>
<li>召回：针对特殊用户人群，构造特殊的内容池，增加相应的召回通道。</li>
<li>排序策略：排除低质量物品，保护新用户和低活用户；特殊用户人群使用特殊的融分公式。</li>
<li>排序模型：结合大模型和小模型，小模型拟合大模型的残差；只用一个模型，模型有多个experts；大模型预估之后用小模型做校准。</li>
</ul>
</li>
</ul>
<h3 id="Interactive-behavior"><a href="#Interactive-behavior" class="headerlink" title="Interactive behavior"></a>Interactive behavior</h3><blockquote>
<p>利用关注、转发、评论这三种交互行为。</p>
</blockquote>
<ul>
<li><p>交互行为</p>
<ul>
<li>交互行为：点赞、收藏、转发、关注、评论……</li>
<li>推荐系统如何利用交互行为？</li>
<li>最简单的方法：将模型预估的交互率用于排序。<ul>
<li>模型将交互行为当做预估的目标。</li>
<li>将预估的点击率、交互率做融合，作为排序的依据。（工业界全会这么做）</li>
</ul>
</li>
<li>其他用途？</li>
</ul>
</li>
<li><p>关注（粉丝）</p>
<ul>
<li><p>关注量对于留存的价值</p>
<ul>
<li><p>对于一位用户，他关注的作者越多，则平台对他的吸引力越强。</p>
</li>
<li><p>用户留存率（r）与他关注的作者数量（f）正相关，但不是正比。</p>
</li>
<li><p>如果某用户的 f 较小，则推荐系统要促使该用户关注更多作者。</p>
</li>
<li><p>图像大致如下：</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202412061634360.png" srcset="/img/loading.gif" lazyload alt="image-20241206163422998" style="zoom:50%;"></li>
</ul>
</li>
<li><p>如何利用关注关系提升用户留存？</p>
<ul>
<li>方法1：用排序策略提升关注量<ul>
<li>对于用户u，模型预估候选物品$i$的关注率为$p_i$</li>
<li>设用户$u$已经关注了$f$个作者。</li>
<li>我们定义单调递减函数$w(f)$，用户已经关注的作者越多，则$w(f)$越小。</li>
<li>在排序融分公式中添加$w(f)·p_{i}$用于促关注（如果f小且$p_i$大，则$w(f)·p_{i}$给物品i带来很大加分，排序靠前)。反之，如果f很大，则$w(f)$会接近0，这一项乘积就不起作用了。只有f小的时候，排序策略才会起到促关注作用。</li>
</ul>
</li>
<li>方法2：构造促关注内容池和召回通道<ul>
<li>这个内容池中物品的关注率高，可以促关注。</li>
<li>如果用户关注的作者数f较小，则对该用户使用该内容池。</li>
<li>召回配额可以固定’也可以与f负相关。</li>
</ul>
</li>
</ul>
</li>
<li><p>粉丝对于促发步的价值</p>
<ul>
<li>UGC平台将作者发布量、发布率作为核心指标，希望作者多发布。</li>
<li>作者发布的物品被平台推送给用户，会产生，点赞、评论、关注等交互。</li>
<li>交互（尤其是关注、评论）可以提升作者发布积极性。</li>
<li>作者的粉丝数越少，则每增加一个粉丝对发布积极性的提升越大（边际效应递减）。</li>
<li>用排序策略帮助低粉新作者涨粉。某作者a的粉丝数（被关注数）为$f_a$。作者α发布的物品i可能被推荐给用户u，模型预估关注率为$p_{ui}$。</li>
<li>我们定义单调递减函数$w(f_a)$作为权重，作者a的粉丝越多，则$w(f_a)$越小。在排序融分公式中添加$w(f_a)·p_{ui}$，帮助低粉作者涨粉，扶持一下，从而激发创作积极性。</li>
</ul>
</li>
<li><p>隐式关注关系</p>
<ul>
<li>召回通道U2A2I：user -&gt; author -&gt; item</li>
<li>显式关注关系：用户u关注了作者a，将a发布的物品推荐给u。（点击率、交互率指标通常高于其他召回通道。)</li>
<li>隐式关注关系：用户u喜欢看作者a发布的物品，但是u没有关注a。</li>
<li>隐式关注的作者数量远大于显式关注。挖掘隐式关注关系，构造U2A2I召回通道，可以提升推荐系统核心指标。</li>
</ul>
</li>
</ul>
</li>
<li><p>转发（分享）</p>
<ul>
<li>促转发（分享回流）<ul>
<li>A平台用户将物品转发到B平台，可以为A吸引站外流量。</li>
<li>推荐系统做促转发（也叫分享回流）可以提升DAU和消费指标。</li>
<li>简单提升转发次数是否有效呢？<ul>
<li>模型预估转发率为$p$，融分公式中有一项$w · p$，让转发率大的物品更容易获得曝光机会。</li>
<li>增大权重$w$可以促转发，吸引站外流量，但是会负面影响，点击率和其他交互率。</li>
</ul>
</li>
</ul>
</li>
<li>KOL建模<ul>
<li>目标：在不损害点击和其他交互的前提下，尽量多吸引站外流量。</li>
<li>什么样的用户的转发可以吸引大量站外流量？<ul>
<li>答案：<strong>其他平台</strong>的Key Opinion Leader(KOL)，也就是我们常说的大V。</li>
<li>用户u是我们站内的K○L,但他不是其他平台的KOL，他的转发价值大吗？No</li>
<li>用户在我们站内没有粉丝，但他是其他平台的KOL，他的转发价值大吗？Yes</li>
</ul>
</li>
<li>如何判断本平台的用户是不是其他平台的KOL？该用户历史上的转发能带来多少站外流量。</li>
</ul>
</li>
<li>促转发的策略：<ul>
<li>目标：在不损害，点击和其他交互的前提下，尽量多吸引站外流量。</li>
<li>识别出的站外KOL之后，该如何用于排序和召回？<ul>
<li>方法1：排序融分公式中添加额外的一项$k_u · p_{ui}$。<ul>
<li>$k_u$：如果用户u是站外KOL，则$k_u$大。</li>
<li>$p_{ui}$：为用户u推荐物品i, 模型预估的转发率。</li>
<li>如果u是站外KOL，利用转发的价值，则多给他曝光他可能转发的物品，不知不觉中给平台多吸引站外流量。</li>
<li>如果u不是站外KOL，$k_u$接近于0，这一项总是接近于0。不会干扰排序融分公式。</li>
<li>总而言之：只影响少数用户，对于绝大多数用户没有影响。正式通过这些少数用户吸引站外流量，提升大盘指标。</li>
</ul>
</li>
<li>方法2：构造促转发内容池和召回通道，对站外KOL生效。（和前面促关注的十分类似哈，不再重复）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>评论</p>
<ul>
<li>评论促发布<ul>
<li>UGC平台将作者发布量、发布率作为核心指标，希望作者多发布。</li>
<li>关注、评论等交互可以提升作者发布积极性。</li>
<li>如果新发布物品尚未获得很多评论，则给预估评论率提权，让物品尽快获得评论。反之，边际效益递减，就不用提权了。</li>
<li>排序融分公式中添加额外一项$W_i · P_i$。<ul>
<li>$W_i$：权重，与物品$i$已有的评论数量负相关。</li>
<li>$P_i$：为用户推荐物品i，模型预估的评论率。</li>
<li>添加这个，评论少的时候，提权。评论多的时候就没啥影响了。</li>
</ul>
</li>
</ul>
</li>
<li>评论的其他价值：<ul>
<li>有的用户喜欢留评论，喜欢跟作者、评论区用户互动。<ul>
<li>给这样的用户添加促评论的内容池，让他们更多机会参与讨论。</li>
<li>有利于提升这些用户的留存。</li>
</ul>
</li>
<li>有的用户常留高质量评论（评论的点赞量高)。<ul>
<li>高质量评论对作者、其他用户的留存有贡献。（作者、其他用户觉得这样的评论有趣或有帮助）</li>
<li>用排序和召回策略鼓励这些用户多留评论。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>总结</p>
<ul>
<li>关注：留存价值（让新用户关注更多作者，提升新用户留存)；发布价值（帮助新作者获得更多粉丝，提升作者发布积极性)；利用隐式关注关系做召回。</li>
<li>转发：判断哪些用户是站外的KOL，利用他们转发的价值，吸引站外的流量。</li>
<li>评论：发布价值（促使新物品获得评论，提升作者发布积极性）；留存价值（给喜欢讨论的用户创造更多留评机会）；鼓励高质量评论的用户多留评论。</li>
</ul>
</li>
</ul>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1PS4y1A7za/?vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd">Shusen Wang的公开课</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/wangshusen/RecommenderSystem">Shuseng Wang的笔记</a></li>
<li><a target="_blank" rel="noopener" href="https://www.yuque.com/yuejiangliu/recommended-system-in-the-industry/overview">同学对于Shuseng的课做的很全很全的笔记</a>，参考了许多内容，很有价值！</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/AI/" class="category-chain-item">AI</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E8%87%AA%E5%AD%A6/">#自学</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Recommendation System</div>
      <div>https://alexanderliu-creator.github.io/2024/08/16/recommendation-system/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Alexander Liu</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年8月16日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/08/03/huggingface-nlp-course/" title="HuggingFace-NLP-Course">
                        <span class="hidden-mobile">HuggingFace-NLP-Course</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.events.registerRefreshCallback(function() {
      if ('mermaid' in window) {
        mermaid.init();
      }
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
