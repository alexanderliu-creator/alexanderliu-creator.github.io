

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/tuzi.png">
  <link rel="icon" href="/img/tuzi.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Alexander Liu">
  <meta name="keywords" content="分布式系统,后端研发,数据协同">
  
    <meta name="description" content="新坑来自于自己的浅薄，在从洛阳回北京的动车上，李老师给我推荐了这个HuggingFace教程。尽量不大篇幅记录，记录笔记，对有感触和自己认为有价值的地方多驻足。这个项目从我的视角来看，和实践结合会更加紧密，对我来说有很多价值！">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace-NLP-Course">
<meta property="og:url" content="https://alexanderliu-creator.github.io/2024/08/03/huggingface-nlp-course/index.html">
<meta property="og:site_name" content="兔の博客">
<meta property="og:description" content="新坑来自于自己的浅薄，在从洛阳回北京的动车上，李老师给我推荐了这个HuggingFace教程。尽量不大篇幅记录，记录笔记，对有感触和自己认为有价值的地方多驻足。这个项目从我的视角来看，和实践结合会更加紧密，对我来说有很多价值！">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408032115104.svg">
<meta property="article:published_time" content="2024-08-03T13:13:03.000Z">
<meta property="article:modified_time" content="2024-08-26T09:25:20.321Z">
<meta property="article:author" content="Alexander Liu">
<meta property="article:tag" content="自学">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408032115104.svg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>HuggingFace-NLP-Course - 兔の博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"alexanderliu-creator.github.io","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":1},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="兔の博客" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>兔的博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/background_post.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="HuggingFace-NLP-Course"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Alexander Liu
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-08-03 21:13" pubdate>
          2024年8月3日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          49k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          406 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">HuggingFace-NLP-Course</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：24 天前
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <p>新坑来自于自己的浅薄，在从洛阳回北京的动车上，李老师给我推荐了这个<a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/zh-CN/chapter1/1">HuggingFace教程</a>。尽量不大篇幅记录，记录笔记，对有感触和自己认为有价值的地方多驻足。这个项目从我的视角来看，和实践结合会更加紧密，对我来说有很多价值！</p>
<span id="more"></span>



<h1 id="Environment-Setup"><a href="#Environment-Setup" class="headerlink" title="Environment Setup"></a>Environment Setup</h1><ul>
<li>Anaconda</li>
<li>Conda create一个环境，并且<code>pip install transformers</code>。</li>
</ul>
<h1 id="Transformer-Model"><a href="#Transformer-Model" class="headerlink" title="Transformer Model"></a>Transformer Model</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>主要内容：Transformers、Datasets、Tokenizers 和 Accelerate——以及 Hugging Face Hub 教你自然语言处理 (NLP)。</li>
<li>课程设置：<ul>
<li>第 1 章到第 4 章介绍了 Transformers 库的主要概念。在本课程的这一部分结束时，您将熟悉 Transformer 模型的工作原理，并将了解如何使用 <a target="_blank" rel="noopener" href="https://huggingface.co/models">Hugging Face Hub</a> 中的模型，在数据集上对其进行微调，并在 Hub 上分享您的结果。</li>
<li>第 5 章到第 8 章在深入研究经典 NLP 任务之前，教授 Datasets和 Tokenizers的基础知识。在本部分结束时，您将能够自己解决最常见的 NLP 问题。</li>
<li>第 9 章到第 12 章更加深入，探讨了如何使用 Transformer 模型处理语音处理和计算机视觉中的任务。在此过程中，您将学习如何构建和分享模型，并针对生产环境对其进行优化。在这部分结束时，您将准备好将 Transformers 应用于（几乎）任何机器学习问题！</li>
</ul>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408032208494.svg" srcset="/img/loading.gif" lazyload alt="Course Setting"></p>
<h2 id="Transformer-Usage"><a href="#Transformer-Usage" class="headerlink" title="Transformer Usage"></a>Transformer Usage</h2><ul>
<li><p>常见NLP任务：对整个句子进行分类，对句子中的每个词进行分类，生成文本内容，从文本中提取答案，从输入文本生成新句子。NLP 不仅限于书面文本。它还解决了语音识别和计算机视觉中的复杂挑战，例如生成音频样本的转录或图像描述。Transformer就是广泛用于NLP领域的一个模型。Some of the currently <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/pipelines">available pipelines</a> are:</p>
</li>
<li><p>Transformers 库中最基本的对象是 <strong>pipeline()</strong> 函数。它将模型与其必要的预处理和后处理步骤连接起来，使我们能够通过直接输入任何文本并获得最终的答案：</p>
<ul>
<li><p>Single Sentence</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><br>classifier = pipeline(<span class="hljs-string">"sentiment-analysis"</span>)<br>classifier(<span class="hljs-string">"I've been waiting for a HuggingFace course my whole life."</span>)<br><br><span class="hljs-comment"># Result: [{'label': 'POSITIVE', 'score': 0.9598047137260437}]</span><br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>Multi Sentences</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">classifier(<br>    [<span class="hljs-string">"I've been waiting for a HuggingFace course my whole life."</span>, <span class="hljs-string">"I hate this so much!"</span>]<br>)<br><br><span class="hljs-comment"># Result: [{'label': 'POSITIVE', 'score': 0.9598047137260437},{'label': 'NEGATIVE', 'score': 0.9994558095932007}]</span><br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>默认情况下，此pipeline选择一个特定的预训练模型，该模型已针对英语情感分析进行了微调。创建classifier对象时，将下载并缓存模型。如果您重新运行该命令，则将使用缓存的模型，无需再次下载模型。</p>
</li>
</ul>
</li>
<li><p>将一些文本传递到pipeline时涉及三个主要步骤：</p>
<ol>
<li>文本被预处理为模型可以理解的格式。</li>
<li>预处理的输入被传递给模型。</li>
<li>模型处理后输出最终人类可以理解的结果。</li>
</ol>
</li>
<li><p>Some of the currently <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/pipelines">available pipelines</a> are: </p>
<ul>
<li><code>feature-extraction</code> (get the vector representation of a text)</li>
<li><code>fill-mask</code></li>
<li><code>ner</code> (named entity recognition)</li>
<li><code>question-answering</code></li>
<li><code>sentiment-analysis</code></li>
<li><code>summarization</code></li>
<li><code>text-generation</code></li>
<li><code>translation</code></li>
<li><code>zero-shot-classification</code></li>
</ul>
</li>
<li><p>下面是一些Demo</p>
<ul>
<li><p>Zero-shot classification</p>
<p>我们将首先处理一项非常具挑战性的任务，我们需要对尚未标记的文本进行分类。zero-shot-classification pipeline非常强大：它允许您直接指定用于分类的标签，因此您不必依赖预训练模型的标签。下面的模型展示了如何使用这两个标签将句子分类为正面或负面——但也可以使用您喜欢的任何其他标签集对文本进行分类。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><br>classifier = pipeline(<span class="hljs-string">"zero-shot-classification"</span>)<br>classifier(<br>    <span class="hljs-string">"This is a course about the Transformers library"</span>,<br>    candidate_labels=[<span class="hljs-string">"education"</span>, <span class="hljs-string">"politics"</span>, <span class="hljs-string">"business"</span>],<br>)<br><br><span class="hljs-comment"># Result: {'sequence': 'This is a course about the Transformers library', 'labels': ['education', 'business', 'politics'], 'scores': [0.8445963859558105, 0.111976258456707, 0.043427448719739914]}</span><br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>此pipeline称为zero-shot，因为您不需要对数据上的模型进行微调即可使用它。它可以直接返回您想要的任何标签列表的概率分数！</p>
</blockquote>
</li>
<li><p>Text generation</p>
<p>主要使用方法是您提供一个提示，模型将通过生成剩余的文本来自动完成整段话。这类似于许多手机上的预测文本功能。文本生成涉及随机性，因此如果您没有得到相同的如下所示的结果，这是正常的。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><br>generator = pipeline(<span class="hljs-string">"text-generation"</span>)<br>generator(<span class="hljs-string">"In this course, we will teach you how to"</span>)<br><br><span class="hljs-comment"># Result: [{'generated_text': 'In this course, we will teach you how to understand and use data flow and data interchange when handling user data. We will be working with one or more of the most commonly used data flows — data flows of various types, as seen by the HTTP}]</span><br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>您可以使用参数 <strong>num_return_sequences</strong> 控制生成多少个不同的序列，并使用参数 <strong>max_length</strong> 控制输出文本的总长度。</p>
</blockquote>
</li>
<li><p>Mask filling</p>
<p>此任务的想法是填充给定文本中的空白，</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><br>unmasker = pipeline(<span class="hljs-string">"fill-mask"</span>)<br>unmasker(<span class="hljs-string">"This course will teach you all about &lt;mask&gt; models."</span>, top_k=<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># Reuslt: [{'sequence': 'This course will teach you all about mathematical models.','score': 0.19619831442832947,'token': 30412,'token_str': ' mathematical'}, {'sequence': 'This course will teach you all about computational models.', 'score': 0.04052725434303284, 'token': 38163, 'token_str': ' computational'}]</span><br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p><strong>top_k</strong> 参数控制要显示的结果有多少种。请注意，这里模型填充了特殊的&lt; **mask** &gt;词，它通常被称为掩码标记。其他掩码填充模型可能有不同的掩码标记，因此在探索其他模型时要验证正确的掩码字是什么。检查它的一种方法是查看小组件中使用的掩码。</p>
</blockquote>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408032253866.png" srcset="/img/loading.gif" lazyload alt="image-20240803225327829" style="zoom:50%;">
</li>
<li><p>Named entity recognition</p>
<p>命名实体识别 (NER) 是一项任务，其中模型必须找到输入文本的哪些部分对应于诸如人员、位置或组织之类的实体。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><br>ner = pipeline(<span class="hljs-string">"ner"</span>, grouped_entities=<span class="hljs-literal">True</span>)<br>ner(<span class="hljs-string">"My name is Sylvain and I work at Hugging Face in Brooklyn."</span>)<br><br><span class="hljs-comment"># result [{'entity_group': 'PER', 'score': 0.99816, 'word': 'Sylvain', 'start': 11, 'end': 18},{'entity_group': 'ORG', 'score': 0.97960, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.99321, 'word': 'Brooklyn', 'start': 49, 'end': 57}]</span><br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>pipeline创建函数中传递选项 <strong>grouped_entities=True</strong> 以告诉pipeline将对应于同一实体的句子部分重新组合在一起：这里模型正确地将“Hugging”和“Face”分组为一个组织，即使名称由多个词组成。事实上，正如我们即将在下一章看到的，预处理甚至会将一些单词分成更小的部分。例如，<strong>Sylvain</strong> 分割为了四部分：<strong>S、##yl、##va</strong> 和 <strong>##in</strong>。在后处理步骤中，pipeline成功地重新组合了这些部分。</p>
</blockquote>
</li>
<li><p>Question answering</p>
<p>问答pipeline使用来自给定上下文的信息回答问题：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><br>question_answerer = pipeline(<span class="hljs-string">"question-answering"</span>)<br>question_answerer(<br>    question=<span class="hljs-string">"Where do I work?"</span>,<br>    context=<span class="hljs-string">"My name is Sylvain and I work at Hugging Face in Brooklyn"</span>,<br>)<br><br><span class="hljs-comment"># Result {'score': 0.6385916471481323, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}</span><br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>此pipeline通过从提供的上下文中提取信息来工作；它不会凭空生成答案。</p>
</blockquote>
</li>
<li><p>Summarization</p>
<p>文本摘要是将文本缩减为较短文本的任务，同时保留文本中的主要（重要）信息。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><br>summarizer = pipeline(<span class="hljs-string">"summarization"</span>)<br>summarizer(<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    America has changed dramatically during recent years. Not only has the number of </span><br><span class="hljs-string">    graduates in traditional engineering disciplines such as mechanical, civil, </span><br><span class="hljs-string">    electrical, chemical, and aeronautical engineering declined, but in most of </span><br><span class="hljs-string">    the premier American universities engineering curricula now concentrate on </span><br><span class="hljs-string">    and encourage largely the study of engineering science. As a result, there </span><br><span class="hljs-string">    are declining offerings in engineering subjects dealing with infrastructure, </span><br><span class="hljs-string">    the environment, and related issues, and greater concentration on high </span><br><span class="hljs-string">    technology subjects, largely supporting increasingly complex scientific </span><br><span class="hljs-string">    developments. While the latter is important, it should not be at the expense </span><br><span class="hljs-string">    of more traditional engineering.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Rapidly developing economies such as China and India, as well as other </span><br><span class="hljs-string">    industrial countries in Europe and Asia, continue to encourage and advance </span><br><span class="hljs-string">    the teaching of engineering. Both China and India, respectively, graduate </span><br><span class="hljs-string">    six and eight times as many traditional engineers as does the United States. </span><br><span class="hljs-string">    Other industrial countries at minimum maintain their output, while America </span><br><span class="hljs-string">    suffers an increasingly serious decline in the number of engineering graduates </span><br><span class="hljs-string">    and a lack of well-educated engineers.</span><br><span class="hljs-string">"""</span><br>)<br><br><br><span class="hljs-comment"># Result [{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil , electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India, as well as other industrial countries in Europe and Asia, continue to encourage and advance engineering .'}]</span><br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>与文本生成一样，您指定结果的 <strong>max_length</strong> 或 <strong>min_length</strong>。</p>
</blockquote>
</li>
<li><p>Translation</p>
<p>对于翻译，如果您在任务名称中提供语言对（例如“<strong>translation_en_to_fr</strong>”），则可以使用默认模型，但最简单的方法是在<a target="_blank" rel="noopener" href="https://huggingface.co/models">模型中心（hub）</a>选择要使用的模型。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><br>translator = pipeline(<span class="hljs-string">"translation"</span>, model=<span class="hljs-string">"Helsinki-NLP/opus-mt-fr-en"</span>)<br>translator(<span class="hljs-string">"Ce cours est produit par Hugging Face."</span>)<br><br><span class="hljs-comment"># Result [{'translation_text': 'This course is produced by Hugging Face.'}]</span><br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>与文本生成和摘要一样，您可以指定结果的 <strong>max_length</strong> 或 <strong>min_length</strong>。</p>
</blockquote>
</li>
<li><p>Using any model from the Hub in a pipeline</p>
<ul>
<li><p>您也可以从 Hub 中选择特定模型以在特定任务的pipeline中使用 - 例如，文本生成。转到<a target="_blank" rel="noopener" href="https://huggingface.co/models"><strong>模型中心（hub）</strong></a>并单击左侧的相应标签将会只显示该任务支持的模型。 <a target="_blank" rel="noopener" href="https://huggingface.co/distilgpt2"><strong>distilgpt2</strong></a> 模型为例子</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><br>generator = pipeline(<span class="hljs-string">"text-generation"</span>, model=<span class="hljs-string">"distilgpt2"</span>)<br>generator(<br>    <span class="hljs-string">"In this course, we will teach you how to"</span>,<br>    max_length=<span class="hljs-number">30</span>,<br>    num_return_sequences=<span class="hljs-number">2</span>,<br>)<br><br><span class="hljs-comment"># Result: [{'generated_text': 'In this course, we will teach you how to manipulate the world and move your mental and physical capabilities to your advantage.'}, {'generated_text': 'In this course, we will teach you how to become an expert and practice realtime, and with a hands on experience on both real time and real'}]</span><br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>可以从网站上找到对应的使用代码：</p>
<ul>
<li><p>模型页面：<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408032240087.png" srcset="/img/loading.gif" lazyload alt="image-20240803224043007" style="zoom:50%;"></p>
</li>
<li><p>模型使用代码：<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408032241609.png" srcset="/img/loading.gif" lazyload alt="image-20240803224104569" style="zoom:50%;"></p>
</li>
<li><p>The Inference API：所有模型都可以使用 Inference API 直接通过浏览器进行测试，该 API 可在 <a target="_blank" rel="noopener" href="https://huggingface.co/">Hugging Face 网站</a>上找到。通过输入自定义文本并观察模型的输出，您可以直接在此页面上使用模型。小组件形式的推理 API 也可作为付费产品使用，如果您的工作流程需要它，它会派上用场。有关更多详细信息，请参阅<a target="_blank" rel="noopener" href="https://huggingface.co/pricing">定价页面</a>。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408032253866.png" srcset="/img/loading.gif" lazyload alt="image-20240803225327829" style="zoom:50%;">









<h2 id="How-Transformer-Work"><a href="#How-Transformer-Work" class="headerlink" title="How Transformer Work"></a>How Transformer Work</h2><ul>
<li><p>以下是 Transformer 模型（简短）历史中的一些关键节点：</p>
<ul>
<li><p>Development Graph for Transformer<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg" srcset="/img/loading.gif" lazyload alt="A brief chronology of Transformers models."></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Transformer 架构</a> 于 2017 年 6 月推出。原本研究的重点是翻译任务。随后推出了几个有影响力的模型，包括</p>
<ul>
<li><p><strong>2018 年 6 月</strong>: <a target="_blank" rel="noopener" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT</a>, 第一个预训练的 Transformer 模型，用于各种 NLP 任务并获得极好的结果</p>
</li>
<li><p><strong>2018 年 10 月</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">BERT</a>, 另一个大型预训练模型，该模型旨在生成更好的句子摘要（下一章将详细介绍！）</p>
</li>
<li><p><strong>2019 年 2 月</strong>: <a target="_blank" rel="noopener" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>, GPT 的改进（并且更大）版本，由于道德问题没有立即公开发布</p>
</li>
<li><p><strong>2019 年 10 月</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.01108">DistilBERT</a>, BERT 的提炼版本，速度提高 60%，内存减轻 40%，但仍保留 BERT 97% 的性能</p>
</li>
<li><p><strong>2019 年 10 月</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.13461">BART</a> 和 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.10683">T5</a>, 两个使用与原始 Transformer 模型相同架构的大型预训练模型（第一个这样做）</p>
</li>
<li><p><strong>2020 年 5 月</strong>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.14165">GPT-3</a>, GPT-2 的更大版本，无需微调即可在各种任务上表现良好（称为零样本学习）</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>大体上，它们可以分为三类：</p>
<ul>
<li><p>GPT-like (也被称作自回归Transformer模型, Decoder)</p>
</li>
<li><p>BERT-like (也被称作自动编码Transformer模型, Encoder)</p>
</li>
<li><p>BART/T5-like (也被称作序列到序列的 Transformer模型, Encoder + Decoder)</p>
</li>
</ul>
</li>
</ul>
<h3 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h3><p><img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg" srcset="/img/loading.gif" lazyload alt="The pretraining of a language model is costly in both time and money."></p>
<p><img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg" srcset="/img/loading.gif" lazyload alt="The fine-tuning of a language model is cheaper than pretraining in both time and money."></p>
<ul>
<li>预训练通常是在非常大量的数据上进行的。因此，它需要大量的数据，而且训练可能需要几周的时间。 另一方面，微调是在模型经过预训练后完成的训练。要执行微调，首先需要获取一个经过预训练的语言模型，然后使用特定于任务的数据集执行额外的训练。等等，为什么不直接为最后的任务而训练呢？有几个原因：<ul>
<li>预训练模型已经在与微调数据集有一些相似之处的数据集上进行了训练。因此，微调过程能够利用模型在预训练期间获得的知识（例如，对于NLP问题，预训练模型将对您在任务中使用的语言有某种统计规律上的理解）。</li>
<li>由于预训练模型已经在大量数据上进行了训练，因此微调需要更少的数据来获得不错的结果。</li>
<li>出于同样的原因，获得好结果所需的时间和资源要少得多</li>
</ul>
</li>
</ul>
<blockquote>
<p>微调模型具有较低的时间、数据、财务和环境成本。迭代不同的微调方案也更快、更容易，因为与完整的预训练相比，训练的约束更少。这个过程也会比从头开始的训练（除非你有很多数据）取得更好的效果，这就是为什么你应该总是尝试利用一个预训练的模型—一个尽可能接近你手头的任务的模型—并对其进行微调。（大多数场景下，你其实都没有足够量的数据orz）</p>
</blockquote>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408051003943.png" srcset="/img/loading.gif" lazyload alt="Architecture of a Transformers models" style="zoom:50%;">

<ul>
<li><p>结构介绍</p>
<ul>
<li><p><strong>Encoder (左侧)</strong>: 编码器接收输入并构建其表示（其特征）。这意味着对模型进行了优化，以从输入中获得理解。</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408050956329.png" srcset="/img/loading.gif" lazyload alt="image-20240805095646242" style="zoom:50%;">

<blockquote>
<p>将Sentence中的每一个单词，通过self-attention提取它的context和它本身的含义，并转换为一个多维的向量（例如768纬），这个向量实际上就可以理解为，保存了当前词在当前上下文中的”语义“。因此这个语义vector后续就可以被用来实现更多的NLP任务。（例如加DNN，做情感分类之类的事儿）</p>
</blockquote>
</li>
<li><p><strong>Decoder (右侧)</strong>: 解码器使用编码器的表示（特征）以及其他输入来生成目标序列。这意味着该模型已针对生成输出进行了优化。</p>
<img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408051007934.png" srcset="/img/loading.gif" lazyload alt="image-20240805100710856" style="zoom:50%;">

<blockquote>
<p>通过Masked Self-Attention，把右边遮住，只暴露左边给Decoder，并让其输出右边。Good at Causal Language Modeling(Guessing the next word in the sentence).</p>
</blockquote>
</li>
<li><p><strong>Seq2Seq</strong>: Encoder-Decoder接在一起。</p>
</li>
</ul>
  <img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202408051015069.png" srcset="/img/loading.gif" lazyload alt="image-20240805101526009" style="zoom:50%;">

<blockquote>
<ul>
<li>Encoder将输入Sentence所表述的语义抽取出来，并且在Decoder生成期间使用(Encoder在输出了对应的语义后，便不在Decoder生成的过程中起别的作用了，Decoder生成本质上是拿着Encoder的输出作为自己的输入的一部分的。</li>
<li>Decoder会接受一个特殊的Start token，和Encoder输出的语义，开始生成文本。每次生成的新的token，都会接在现有的token sequence最后，并成为一个新的token sequence重新输入Decoder，Decoder会一直输出，直到遇到某个特殊的End token为止。(Encoder就使用了一次，Decoder会被使用多次)</li>
<li>Encoder和Decoder权重不一定是共享的，因此我们可以只使用其中的一部分，例如Encoder or Decoder only，去解决一些实际任务。</li>
</ul>
</blockquote>
</li>
<li><p>这些部件中的每一个都可以独立使用，具体取决于任务：</p>
<ul>
<li><p><strong>Encoder-only models</strong>: 适用于需要理解输入的任务，如句子分类和命名实体识别。</p>
</li>
<li><p><strong>Decoder-only models</strong>: 适用于生成任务，如文本生成。</p>
</li>
<li><p><strong>Encoder-decoder models</strong> 或者 <strong>sequence-to-sequence models</strong>: 适用于需要根据输入进行生成的任务，如翻译或摘要。</p>
</li>
</ul>
</li>
<li><p>Attension Layer</p>
<ul>
<li>Transformer模型的一个关键特性是<em>注意力层</em>。事实上，介绍Transformer架构的文章的标题是<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">“Attention Is All You Need”</a>！这一层将告诉模型在处理每个单词的表示时，要特别重视您传递给它的句子中的某些单词（并且或多或少地忽略其他单词）。</li>
</ul>
</li>
</ul>
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg" srcset="/img/loading.gif" lazyload alt="Architecture of a Transformers models" style="zoom:50%;">

<ul>
<li>原始的Transformer架构如下所示，编码器在左侧，解码器在右侧：请注意，解码器块中的第一个注意力层关注解码器的所有（过去的）输入，但第二个注意力层使用编码器的输出。因此，它可以访问整个输入句子，以最好地预测当前的词。这非常有用，因为不同的语言可能有将单词放在不同顺序的语法规则，或者句子后面的一些上下文可能有助于确定给定词的最佳翻译。注意力掩码也可以在编码器/解码器中使用，以防止模型关注一些特殊词——例如，用于使所有输入在批处理句子时具有相同长度的特殊填充词。</li>
</ul>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li><p>当我们深入探讨Transformers模型时，您将看到 架构、参数和模型。这些术语的含义略有不同：</p>
<ul>
<li><p><strong>Architecture</strong>: 这是模型的骨架 — 每个层的定义以及模型中发生的每个操作。</p>
</li>
<li><p><strong>Checkpoints</strong>: 这些是将在给架构中结构中加载的权重。</p>
</li>
<li><p><strong>Model</strong>: 这是一个笼统的术语，没有“架构”或“参数”那么精确：它可以指两者。为了避免歧义，本课程使用将使用架构和参数。</p>
</li>
</ul>
</li>
<li><p>Bias and limitations: 其中最大的一个问题是，为了对大量数据进行预训练，研究人员通常会搜集所有他们能找到的内容，中间可能夹带一些意识形态或者价值观的刻板印象。 尽管一些大模型是使用经过筛选和清洗后，明显中立的数据集上建立的的Transformer模型，仍然会有这样的问题存在。当您使用这些工具时，您需要记住，使用的原始模型的时候，很容易生成性别歧视、种族主义或恐同内容。这种固有偏见不会随着微调模型而使消失。</p>
</li>
<li><p>Summary: 您可以使用完整的体系结构，也可以仅使用编码器或解码器，具体取决于您要解决的任务类型。下表总结了这一点：</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>示例</th>
<th>任务</th>
</tr>
</thead>
<tbody><tr>
<td>编码器</td>
<td>ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa</td>
<td>句子分类、命名实体识别、从文本中提取答案</td>
</tr>
<tr>
<td>解码器</td>
<td>CTRL, GPT, GPT-2, Transformer XL</td>
<td>文本生成</td>
</tr>
<tr>
<td>编码器-解码器</td>
<td>BART, T5, Marian, mBART</td>
<td>文本摘要、翻译、生成问题的回答</td>
</tr>
</tbody></table>
</li>
<li><p>Little test: <a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/en/chapter1/10?fw=pt">https://huggingface.co/learn/nlp-course/en/chapter1/10?fw=pt</a></p>
</li>
</ul>
<h1 id="Using-Transformers"><a href="#Using-Transformers" class="headerlink" title="Using Transformers"></a>Using Transformers</h1><h2 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><p>由于几乎每天都在发布新模型，而且每种模型都有自己的实现，因此尝试它们绝非易事。创建🤗 Transformers库就是为了解决这个问题。它的目标是提供一个API，通过它可以加载、训练和保存任何Transformer模型。这个库的主要特点是：</p>
<ul>
<li><p><strong>易于使用</strong>：下载、加载和使用最先进的NLP模型进行推理只需两行代码即可完成。</p>
</li>
<li><p><strong>灵活</strong>：所有型号的核心都是简单的PyTorch <strong>nn.Module</strong> 或者 TensorFlow <strong>tf.kears.Model</strong>，可以像它们各自的机器学习（ML）框架中的任何其他模型一样进行处理。</p>
</li>
<li><p><strong>简单</strong>：当前位置整个库几乎没有任何摘要。“都在一个文件中”是一个核心概念：模型的正向传递完全定义在一个文件中，因此代码本身是可以理解的，并且是可以破解的。</p>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>最后一个特性使🤗 Transformers与其他ML库截然不同。这些模型<strong>不是基于通过文件共享的模块构建的</strong>；相反，每一个模型都有自己的菜单。除了使模型更加容易接受和更容易理解，这还允许你轻松地在一个模型上实验，而且不影响其他模型。</p>
</blockquote>
<h2 id="How-Pipeline-Works"><a href="#How-Pipeline-Works" class="headerlink" title="How Pipeline Works"></a>How Pipeline Works</h2><ul>
<li>Demo</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><br>classifier = pipeline(<span class="hljs-string">"sentiment-analysis"</span>)<br>classifier(<br>    [<br>        <span class="hljs-string">"I've been waiting for a HuggingFace course my whole life."</span>,<br>        <span class="hljs-string">"I hate this so much!"</span>,<br>    ]<br>)<br><br><span class="hljs-comment"># Result: [{'label': 'POSITIVE', 'score': 0.9598047137260437}, {'label': 'NEGATIVE', 'score': 0.9994558095932007}]</span><br></code></pre></td></tr></tbody></table></figure>

<p>此管道将三个步骤组合在一起：预处理、通过模型传递输入和后处理：</p>
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg" srcset="/img/loading.gif" lazyload alt="The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head." style="zoom:50%;">

<h3 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h3><ul>
<li>使用分词器进行预处理： 我们管道的第一步是将文本输入转换为模型能够理解的数字。 为此，我们使用<em>tokenizer</em>，负责：<ul>
<li>将输入拆分为单词、子单词或符号（如标点符号），称为标记(<em>token</em>)</li>
<li>将每个标记(token)映射到一个整数</li>
<li>添加可能对模型有用的其他输入</li>
</ul>
</li>
<li>所有这些预处理都需要以与模型预训练时完全相同的方式完成，因此我们首先需要从<a target="_blank" rel="noopener" href="https://huggingface.co/models">Model Hub</a>中下载这些信息。为此，我们使用<code>AutoTokenizer</code>类及其<code>from_pretrained()</code>方法。使用我们模型的检查点名称，它将自动获取与模型的标记器相关联的数据，并对其进行缓存（因此只有在您第一次运行下面的代码时才会下载）。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer<br><br>checkpoint = <span class="hljs-string">"distilbert-base-uncased-finetuned-sst-2-english"</span><br>tokenizer = AutoTokenizer.from_pretrained(checkpoint)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>一旦我们有了tokenizer，我们就可以直接将我们的句子传递给它，然后我们就会得到返回的dictionary，它可以提供给我们的模型！The only thing left to do is to convert the list of input IDs to tensors.</p>
</blockquote>
<ul>
<li>您可以使用Transformers，而不必担心使用的是哪个机器学习框架作为后端；它可能是PyTorch或TensorFlow，或者对于某些模型来说是Flax。然而，Transformer模型只接受张量作为输入。为了指定我们想要返回的张量类型（PyTorch、TensorFlow或普通的NumPy），我们使用<code>return_tensors</code>参数：</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">raw_inputs = [<br>    <span class="hljs-string">"I've been waiting for a HuggingFace course my whole life."</span>,<br>    <span class="hljs-string">"I hate this so much!"</span>,<br>]<br>inputs = tokenizer(raw_inputs, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">"pt"</span>)<br><span class="hljs-built_in">print</span>(inputs)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>不要担心填充和截断的问题，我们稍后会解释这些。这里要记住的主要事项是您可以传递一个句子或一个句子列表，并且可以指定您想要返回的张量类型（如果没有传递类型，您将得到一个列表列表作为结果）。以下是PyTorch张量形式的结果示例：</p>
</blockquote>
<figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">{</span><br>    'input_ids'<span class="hljs-punctuation">:</span> tensor(<span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">[</span>  <span class="hljs-number">101</span><span class="hljs-punctuation">,</span>  <span class="hljs-number">1045</span><span class="hljs-punctuation">,</span>  <span class="hljs-number">1005</span><span class="hljs-punctuation">,</span>  <span class="hljs-number">2310</span><span class="hljs-punctuation">,</span>  <span class="hljs-number">2042</span><span class="hljs-punctuation">,</span>  <span class="hljs-number">3403</span><span class="hljs-punctuation">,</span>  <span class="hljs-number">2005</span><span class="hljs-punctuation">,</span>  <span class="hljs-number">1037</span><span class="hljs-punctuation">,</span> <span class="hljs-number">17662</span><span class="hljs-punctuation">,</span> <span class="hljs-number">12172</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2607</span><span class="hljs-punctuation">,</span>  <span class="hljs-number">2026</span><span class="hljs-punctuation">,</span>  <span class="hljs-number">2878</span><span class="hljs-punctuation">,</span>  <span class="hljs-number">2166</span><span class="hljs-punctuation">,</span>  <span class="hljs-number">1012</span><span class="hljs-punctuation">,</span>   <span class="hljs-number">102</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-punctuation">[</span>  <span class="hljs-number">101</span><span class="hljs-punctuation">,</span>  <span class="hljs-number">1045</span><span class="hljs-punctuation">,</span>  <span class="hljs-number">5223</span><span class="hljs-punctuation">,</span>  <span class="hljs-number">2023</span><span class="hljs-punctuation">,</span>  <span class="hljs-number">2061</span><span class="hljs-punctuation">,</span>  <span class="hljs-number">2172</span><span class="hljs-punctuation">,</span>   <span class="hljs-number">999</span><span class="hljs-punctuation">,</span>   <span class="hljs-number">102</span><span class="hljs-punctuation">,</span>     <span class="hljs-number">0</span><span class="hljs-punctuation">,</span>     <span class="hljs-number">0</span><span class="hljs-punctuation">,</span>     <span class="hljs-number">0</span><span class="hljs-punctuation">,</span>     <span class="hljs-number">0</span><span class="hljs-punctuation">,</span>     <span class="hljs-number">0</span><span class="hljs-punctuation">,</span>     <span class="hljs-number">0</span><span class="hljs-punctuation">,</span>     <span class="hljs-number">0</span><span class="hljs-punctuation">,</span>     <span class="hljs-number">0</span><span class="hljs-punctuation">]</span><br>    <span class="hljs-punctuation">]</span>)<span class="hljs-punctuation">,</span> <br>    'attention_mask'<span class="hljs-punctuation">:</span> tensor(<span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">]</span><br>    <span class="hljs-punctuation">]</span>)<br><span class="hljs-punctuation">}</span><br></code></pre></td></tr></tbody></table></figure>



<h3 id="Going-through-the-model"><a href="#Going-through-the-model" class="headerlink" title="Going through the model"></a>Going through the model</h3><ul>
<li>我们可以像使用标记器一样下载预训练模型。Transformers提供了一个<code>AutoModel</code>类，该类还具有<code>from_pretrained()</code>方法：</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel<br><br>checkpoint = <span class="hljs-string">"distilbert-base-uncased-finetuned-sst-2-english"</span><br>model = AutoModel.from_pretrained(checkpoint)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>在这段代码片段中，我们下载了之前在管道中使用过的同一个检查点（实际上应该已经被缓存了），并用它实例化了一个模型。这个架构只包含基础的Transformer模块：给定一些输入，它输出我们称之为隐藏状态，也被称为特征的东西。对于每个模型输入，我们将检索到一个高维向量，代表Transformer模型对那个输入的上下文理解。虽然这些隐藏状态本身可能很有用，但它们通常是模型另一部分的输入。</p>
</blockquote>
<ul>
<li>High dimension vector，Transformers模块的矢量输出通常较大。它通常有三个维度：<ul>
<li><strong>Batch size</strong>: 一次处理的序列数（在我们的示例中为2）。</li>
<li><strong>Sequence length</strong>: 序列的数值表示的长度（在我们的示例中为16）。</li>
<li><strong>Hidden size</strong>: 每个模型输入的向量维度。</li>
</ul>
</li>
</ul>
<blockquote>
<p>由于最后一个值，它被称为“高维”。隐藏的大小可能非常大（768通常用于较小的型号，而在较大的型号中，这可能达到3072或更大）。</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">outputs = model(**inputs)<br><span class="hljs-built_in">print</span>(outputs.last_hidden_state.shape)<br><br><span class="hljs-comment"># Result torch.Size([2, 16, 768])</span><br></code></pre></td></tr></tbody></table></figure>



<h3 id="Model-heads-Making-sense-out-of-numbers"><a href="#Model-heads-Making-sense-out-of-numbers" class="headerlink" title="Model heads: Making sense out of numbers"></a>Model heads: Making sense out of numbers</h3><img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg" srcset="/img/loading.gif" lazyload alt="A Transformer network alongside its head." style="zoom:50%;">

<ul>
<li><p>模型头部接收高维隐藏状态向量作为输入，并将它们投影到不同的维度。它们通常由一个或几个线性层组成：Transformer模型的输出直接发送到模型头部进行处理。</p>
</li>
<li><p>模型由其嵌入层和随后的层表示。嵌入层将标记化输入中的每个输入ID转换为表示相关标记的向量。随后的层使用注意力机制操作这些向量，以产生句子的最终表示。</p>
</li>
<li><p>Transformers中有许多不同的体系结构，每种体系结构都是围绕处理特定任务而设计的。以下是一个非详尽的列表：</p>
<ul>
<li><p><code>*Model</code> (retrieve the hidden states)</p>
</li>
<li><p><code>*ForCausalLM</code></p>
</li>
<li><p><code>*ForMaskedLM</code></p>
</li>
<li><p><code>*ForMultipleChoice</code></p>
</li>
<li><p><code>*ForQuestionAnswering</code></p>
</li>
<li><p><code>*ForSequenceClassification</code></p>
</li>
<li><p><code>*ForTokenClassification</code></p>
</li>
<li><p>以及其他</p>
</li>
</ul>
</li>
<li><p>对于我们的示例，我们需要一个带有序列分类头的模型（能够将句子分类为肯定或否定）。因此，我们实际上不会使用<code>AutoModel</code>类，而是使用<code>AutoModelForSequenceClassification</code>：</p>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification<br><br>checkpoint = <span class="hljs-string">"distilbert-base-uncased-finetuned-sst-2-english"</span><br>model = AutoModelForSequenceClassification.from_pretrained(checkpoint)<br>outputs = model(**inputs)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>现在，如果我们观察输出的形状，维度将低得多：模型头将我们之前看到的高维向量作为输入，并输出包含两个值的向量。因为我们只有两个句子和两个标签，所以我们从模型中得到的结果是2 x 2的形状。</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(outputs.logits.shape)<br><br><span class="hljs-comment"># torch.Size([2, 2])</span><br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>什么是模型的Head层？ 一个附加组件，通常由一个或几个层组成，用于将Transformer的预测转换为特定于任务的输出。</li>
</ul>
<h3 id="Postprocessing-the-output"><a href="#Postprocessing-the-output" class="headerlink" title="Postprocessing the output"></a>Postprocessing the output</h3><ul>
<li>我们从模型中得到的输出值本身并不一定有意义，我们的模型预测第一句为<code>[-1.5607, 1.6123]</code>，第二句为<code>[ 4.1692, -3.3464]</code>。这些不是概率，而是<em>logits</em>，即模型最后一层输出的原始非标准化分数。要转换为概率，它们需要经过<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Softmax_function">SoftMax</a>层（所有Transformers模型输出logits，因为用于训练的损耗函数通常会将最后的激活函数（如SoftMax）与实际损耗函数（如交叉熵）融合）：</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>predictions = torch.nn.functional.softmax(outputs.logits, dim=-<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(predictions)<br><br><span class="hljs-comment"># tensor([[4.0195e-02, 9.5980e-01], [9.9946e-01, 5.4418e-04]], grad_fn=&lt;SoftmaxBackward&gt;)</span><br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>现在我们可以看到，模型预测第一句为<code>[0.0402, 0.9598]</code>，第二句为<code>[0.9995, 0.0005]</code>。这些是可识别的概率分数。为了获得每个位置对应的标签，我们可以检查模型配置的<code>id2label</code>属性（下一节将对此进行详细介绍）：</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">model.config.id2label<br><br><span class="hljs-comment"># Result {0: 'NEGATIVE', 1: 'POSITIVE'}</span><br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<ul>
<li><p>第一句：否定：0.0402，肯定：0.9598</p>
</li>
<li><p>第二句：否定：0.9995，肯定：0.0005</p>
</li>
</ul>
</blockquote>
<h3 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>管道的三个步骤：<ul>
<li>使用标记化器进行预处理（是将文本转换为单词或子词序列的过程。在自然语言处理中，文本通常是由一系列单词或子词组成的，而分词器的任务就是将这些单词或子词从文本中分离出来，并将它们转换为计算机可以处理的数字表示。）</li>
<li>通过模型传递输入（是将单词或子词转换为向量表示的过程。在自然语言处理中，单词或子词通常被表示为一个高维度的稀疏向量，其中每个维度对应一个单词或子词的特征。然后再把Embedding塞入模型中，拿到输出的Logits）</li>
<li>后处理（将Logits塞入Softmax中，拿到最后的概率结果）</li>
</ul>
</li>
</ul>
<h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h2><ul>
<li><p>AutoModel类及其所有相关项实际上是对库中各种可用模型的简单包装。它是一个聪明的包装器，因为它可以自动猜测checkpoint的适当architecture，然后用该体系结构实例化模型。但是，如果您知道要使用的模型类型，则可以使用直接定义其体系结构的类。让我们看看这是如何与BERT模型一起工作的。</p>
</li>
<li><p>Create a Transformer</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertConfig, BertModel<br><br><span class="hljs-comment"># Building the config</span><br>config = BertConfig()<br><br><span class="hljs-comment"># Building the model from the config</span><br>model = BertModel(config)<br><br><span class="hljs-comment"># 从默认配置创建模型会使用随机值对其进行初始化。Model is randomly initialized!</span><br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>该模型可以在这种状态下使用，但会输出胡言乱语；首先需要对其进行训练。为了避免不必要的重复工作，必须能够共享和重用已经训练过的模型。</p>
</blockquote>
</li>
<li><p>Load a Transformer</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertModel<br><br>model = BertModel.from_pretrained(<span class="hljs-string">"bert-base-cased"</span>)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>我们可以将BertModel替换为等价的AutoModel类。我们将这样做，因为这会产生与checkpoint无关的代码；如果您的代码适用于一个checkpoint，它应该可以无缝地适用于另一个。即使架构不同，只要检查点是为类似任务（例如，情感分析任务）训练的，这也适用。</p>
<p>在上面的代码示例中，我们没有使用BertConfig，而是通过bert-base-cased标识符加载了一个预训练模型。这是一个由BERT的作者自己训练的模型检查点；您可以在其模型卡片中找到更多详细信息。</p>
<p>这个模型现在已用检查点的所有权重初始化。<strong>它可以直接用于它所训练的任务上的推理，也可以在新任务上进行微调。</strong>通过使用预训练权重而不是从头开始训练，我们可以快速取得良好的结果。</p>
<p>权重已经被下载并缓存（因此future calls to the from_pretrained()方法不会重新下载它们），缓存文件夹默认为~/.cache/huggingface/transformers。<strong>您可以通过设置HF_HOME环境变量来自定义您的缓存文件夹。</strong></p>
<p>用于加载模型的标识符可以是模型中心上<strong>任何与BERT架构兼容的模型的标识符</strong>。完整的可用BERT检查点列表可以在<a target="_blank" rel="noopener" href="https://huggingface.co/models?other=bert">这里</a>找到。</p>
</blockquote>
</li>
<li><p>Save methods</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model.save_pretrained(<span class="hljs-string">"directory_on_my_computer"</span>)<br></code></pre></td></tr></tbody></table></figure>

<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">ls directory_on_my_computer<br><br>config.json pytorch_model.bin<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>config.json: 您将识别构建模型体系结构所需的属性。该文件还包含一些元数据，例如checkpoint的来源以及上次保存检查点时使用的Transformers版本。</p>
<p><em>pytorch_model.bin</em>: <em>state dictionary</em>; 它包含模型的所有权重。</p>
<p>这两个文件齐头并进；配置是了解模型体系结构所必需的，而模型权重是模型的参数。</p>
</blockquote>
</li>
<li><p>Using a Transformer model for inference</p>
<ul>
<li>Transformer模型只能处理数字—这些数字是由分词器生成的。分词器可以负责将输入转换为适当框架的张量。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">sequences = [<span class="hljs-string">"Hello!"</span>, <span class="hljs-string">"Cool."</span>, <span class="hljs-string">"Nice!"</span>]<br><br>=====================&gt; Tokenizer<br><br>encoded_sequences = [<br>    [<span class="hljs-number">101</span>, <span class="hljs-number">7592</span>, <span class="hljs-number">999</span>, <span class="hljs-number">102</span>],<br>    [<span class="hljs-number">101</span>, <span class="hljs-number">4658</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>],<br>    [<span class="hljs-number">101</span>, <span class="hljs-number">3835</span>, <span class="hljs-number">999</span>, <span class="hljs-number">102</span>],<br>]<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>这是一系列编码序列：一个列表的列表。张量只接受矩形形状（想象矩阵）。这个“数组”已经是矩形形状，所以将其转换为张量很容易：</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>model_inputs = torch.tensor(encoded_sequences)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>使用张量作为模型的输入</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">output = model(model_inputs)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>While the model accepts a lot of different arguments, <strong>only the input IDs are necessary</strong>.</p>
</blockquote>
</li>
</ul>
<h2 id="Tokenizers"><a href="#Tokenizers" class="headerlink" title="Tokenizers"></a>Tokenizers</h2><ul>
<li><p>在 NLP 任务中，通常处理的数据是原始文本。 但是，模型只能处理数字，因此我们需要找到一种将原始文本转换为数字的方法。这就是标记器（tokenizer）所做的，并且有很多方法可以解决这个问题。目标是找到最有意义的表示——即对模型最有意义的表示——并且如果可能的话，找到最小的表示。</p>
</li>
<li><p>Tokenizers的示例：</p>
<ul>
<li><p>Word-based: </p>
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg" srcset="/img/loading.gif" lazyload alt="An example of word-based tokenization." style="zoom:50%;">

<ul>
<li><p>它通常很容易设置和使用，只需几条规则，并且通常会产生不错的结果。例如，将原始文本拆分为单词，并为每个单词找到一个数字表示：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">tokenized_text = <span class="hljs-string">"Jim Henson was a puppeteer"</span>.split()<br><span class="hljs-built_in">print</span>(tokenized_text)<br><br><span class="hljs-comment"># Result: ['Jim', 'Henson', 'was', 'a', 'puppeteer']</span><br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>还有一些单词标记器的变体，它们具有额外的标点符号规则。使用这种标记器，我们最终可以得到一些非常大的“词汇表”，其中词汇表由我们在语料库中拥有的独立标记的总数定义。每个单词都分配了一个 ID，从 0 开始一直到词汇表的大小。该模型使用这些 ID 来识别每个单词。</p>
</blockquote>
</li>
<li><p>如果我们想用基于单词的标记器(tokenizer)完全覆盖一种语言，我们需要为语言中的每个单词都有一个标识符，这将生成大量的标记。例如，英语中有超过 500,000 个单词，因此要构建从每个单词到输入 ID 的映射，我们需要跟踪这么多 ID。此外，像“dog”这样的词与“dogs”这样的词的表示方式不同，模型最初无法知道“dog”和“dogs”是相似的：它会将这两个词识别为不相关。这同样适用于其他相似的词，例如“run”和“running”，模型最初不会认为它们是相似的。</p>
</li>
<li><p>最后，我们需要一个自定义标记(token)来表示不在我们词汇表中的单词。这被称为“未知”标记(token)，通常表示为“[UNK]”或”<unk>“。如果你看到标记器产生了很多这样的标记，这通常是一个不好的迹象，因为它无法检索到一个词的合理表示，并且你会在这个过程中丢失信息。制作词汇表时的目标是以这样一种方式进行，即标记器将尽可能少的单词标记为未知标记。</unk></p>
</li>
<li><p>减少未知标记数量的一种方法是使用更深一层的标记器(tokenizer)，即基于字符的(<em>character-based</em>)标记器(tokenizer)。</p>
</li>
</ul>
</li>
<li><p>Character-based:</p>
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg" srcset="/img/loading.gif" lazyload alt="An example of character-based tokenization." style="zoom:50%;">

<ul>
<li>基于字符的标记器(tokenizer)将文本拆分为字符，而不是单词。这有两个主要好处： 词汇量要小得多。 词汇外（未知）标记(token)要少得多，因为每个单词都可以从字符构建。</li>
<li>这种方法也不是完美的。由于现在表示是基于字符而不是单词，因此人们可能会争辩说，从直觉上讲，它的意义不大：每个字符本身并没有多大意义，而单词就是这种情况。然而，这又因语言而异；例如，在中文中，每个字符比拉丁语言中的字符包含更多的信息。另一个需要考虑的问题是，我们的模型最终将处理大量的标记：使用基于词的分词器时，一个词只会是一个单独的标记，但在转换为字符时，它很容易变成10个或更多的标记。</li>
<li>标点符号也是一个重要的问题和考量方面。</li>
</ul>
</li>
<li><p>Subword tokenization</p>
<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/bpe_subword.svg" srcset="/img/loading.gif" lazyload alt="A subword tokenization algorithm." style="zoom:50%;">

<ul>
<li>子词分词算法基于这样一个原则：常用词不应被拆分为更小子词，但罕见词应该被分解成有意义的子词。例如，“annoyingly”可能被视为一个罕见词，并可以被分解为“annoying”和“ly”。这两个子词作为独立的子词出现的可能性更大，同时“annoyingly”的含义通过“annoying”和“ly”的组合含义得以保留。</li>
<li>以下是一个示例，展示了子词分词算法如何对序列“Let’s do tokenization!”进行分词：<ol>
<li>“Let” -&gt; “Let”</li>
<li>“’s” -&gt; “‘s” (可能被视为一个特殊字符或缩写)</li>
<li>“do” -&gt; “do”</li>
<li>“token” -&gt; “token”</li>
<li>“ization” -&gt; “ization”</li>
<li>“!” -&gt; “!”</li>
</ol>
</li>
</ul>
</li>
<li><p>Other tokenizers</p>
<ul>
<li>**字节级BPE (Byte-level BPE)**：在GPT-2中使用，它将文本分解为字节级别的子词，这允许模型处理Unicode字符，并且能够更好地处理未知词汇。</li>
<li><strong>WordPiece</strong>：在BERT中使用，这是一种基于子词的分词方法，它允许模型处理超出词汇表的词汇，通过将它们分解为已知的子词。</li>
<li><strong>SentencePiece或Unigram</strong>：在多种多语言模型中使用，这些技术通常用于处理多种语言的文本，并且能够很好地处理词汇表外的词汇。</li>
</ul>
</li>
</ul>
</li>
<li><p>How to use tokenizers?</p>
<ul>
<li><p>Loading and saving</p>
<ul>
<li><p>加载和保存标记器(tokenizer)就像使用模型一样简单。实际上，它基于相同的两种方法： from_pretrained() 和 save_pretrained() 。这些方法将加载或保存标记器(tokenizer)使用的算法加载和保存tokenizer，就像使用模型一样简单。实际上，它基于相同的两种方法： from_pretrained() 和 save_pretrained() 。这些方法将加载或保存tokenizer使用的算法(类似于architecture）以及它的词汇（类似于weights）。</p>
</li>
<li><p>加载方式：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer<br><br>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">"bert-base-cased"</span>)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>如同 <code>AutoModel</code>，<code>AutoTokenizer</code> 类将根据检查点名称在库中获取正确的标记器(tokenizer)类，并且可以直接与任何检查点一起使用。</p>
</blockquote>
</li>
<li><p>使用方式：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">  tokenizer(<span class="hljs-string">"Using a Transformer network is simple"</span>)<br>  <br><span class="hljs-comment"># {'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102], token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}</span><br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>保存方式：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">tokenizer.save_pretrained(<span class="hljs-string">"directory_on_my_computer"</span>)<br></code></pre></td></tr></tbody></table></figure></li>
</ul>
</li>
</ul>
</li>
<li><p>编码</p>
<ul>
<li><p>将文本翻译成数字被称为编码(<em>encoding</em>).编码分两步完成：标记化，然后转换为输入 ID。</p>
<ul>
<li>第一步是将文本拆分为单词（或单词的一部分、标点符号等），通常称为*标记(token)*。有多个规则可以管理该过程，这就是为什么我们需要使用模型名称来实例化标记器(tokenizer)，以确保我们使用模型预训练时使用的相同规则。</li>
<li>第二步是将这些标记转换为数字，这样我们就可以用它们构建一个张量并将它们提供给模型。为此，标记器(tokenizer)有一个*词汇(vocabulary)*，这是我们在实例化它时下载的部分 <code>from_pretrained()</code> 方法。同样，我们需要使用模型预训练时使用的相同词汇。</li>
</ul>
</li>
<li><p><strong>我们将使用一些单独执行部分标记化管道的方法来向您展示这些步骤的中间结果，但实际上，您应该直接在您的输入上调用tokenizer</strong></p>
<ul>
<li><p>标记化过程由tokenizer的<code>tokenize()</code> 方法实现：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer<br><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"bert-base-cased"</span>)<br><br>sequence = <span class="hljs-string">"Using a Transformer network is simple"</span><br>tokens = tokenizer.tokenize(sequence)<br><br><span class="hljs-built_in">print</span>(tokens)<br><br><span class="hljs-comment"># Result ['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']</span><br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>这个标记器(tokenizer)是一个子词标记器(tokenizer)：它对词进行拆分，直到获得可以用其词汇表表示的标记(token)。<code>transformer</code> 就是这种情况，它分为两个标记：<code>transform</code> 和 <code>##er</code>。</p>
</blockquote>
</li>
<li><p>Tokens -&gt; Input IDs</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">ids = tokenizer.convert_tokens_to_ids(tokens)<br><br><span class="hljs-built_in">print</span>(ids)<br><br><span class="hljs-comment"># Result [7993, 170, 11303, 1200, 2443, 1110, 3014]</span><br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>这些输出(Input IDs)一旦转换为适当的框架张量(Tensor)，就可以用作模型的输入</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p>解码：</p>
<ul>
<li><p><em>解码(Decoding)</em> 正好相反：从词汇索引中，我们想要得到一个字符串。这可以通过 <code>decode()</code> 方法实现，如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">decoded_string = tokenizer.decode([<span class="hljs-number">7993</span>, <span class="hljs-number">170</span>, <span class="hljs-number">11303</span>, <span class="hljs-number">1200</span>, <span class="hljs-number">2443</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">3014</span>])<br><span class="hljs-built_in">print</span>(decoded_string)<br><br><span class="hljs-comment"># 'Using a Transformer network is simple'</span><br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>请注意， <code>decode</code> 方法不仅将索引转换回标记(token)，还将属于相同单词的标记(token)组合在一起以生成可读的句子。当我们使用预测新文本的模型（根据提示生成的文本，或序列到序列问题（如翻译或摘要））时，这种行为将非常有用。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h2 id="Handling-multiple-sequences"><a href="#Handling-multiple-sequences" class="headerlink" title="Handling multiple sequences"></a>Handling multiple sequences</h2><ul>
<li><p>New questions:</p>
<ul>
<li>我们如何处理多个序列？</li>
<li>我们如何处理不同长度的多个序列？</li>
<li>词汇索引是唯一能让模型良好工作的输入吗？</li>
<li>序列是否可能太长？</li>
</ul>
</li>
<li><p>Models expect a batch of inputs:</p>
<ul>
<li><p>批处理是将多个句子一次性通过模型发送的行为。如果您只有一个句子，您可以只构建一个包含单个序列的批次：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">batched_ids = [ids, ids]<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>批处理允许模型在您给它输入多个句子时工作。使用多个序列就像构建一个包含单个序列的批次一样简单。然而，还有第二个问题。当您尝试将两个（或更多）句子组合成一个批次时，它们可能长度不同。如果您以前使用过张量，您知道它们需要是矩形形状，所以您不能直接将输入ID列表转换为张量。为了解决这个问题，我们通常对输入进行填充。</p>
</blockquote>
</li>
</ul>
</li>
<li><p>Padding the inputs</p>
<ul>
<li><p>为了解决这个问题，我们将使用填充使张量具有矩形。Padding通过在值较少的句子中添加一个名为Padding token的特殊单词来确保我们所有的句子长度相同。</p>
</li>
<li><p>Transformer模型的关键特性是关注层，它将每个标记上下文化。这些将考虑填充标记，因为它们涉及序列中的所有标记。为了在通过模型传递不同长度的单个句子时，或者在传递一批应用了相同句子和填充的句子时获得相同的结果，我们需要告诉这些注意层忽略填充标记。这是通过使用 attention mask来实现的。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">batched_ids = [<br>    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],<br>    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],<br>]<br><br><span class="hljs-built_in">print</span>(model(torch.tensor(batched_ids)).logits)<br></code></pre></td></tr></tbody></table></figure></li>
</ul>
</li>
<li><p>Attention masks</p>
<ul>
<li><p><em>Attention masks</em>是与输入ID张量形状完全相同的张量，用0和1填充：1s表示应注意相应的标记，0s表示不应注意相应的标记（即，模型的注意力层应忽略它们）。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">batched_ids = [<br>    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],<br>    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],<br>]<br><br>attention_mask = [<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>]<br><br>outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))<br><span class="hljs-built_in">print</span>(outputs.logits)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>第二个序列的最后一个值是一个填充ID，它在attention mask中是一个0值。</p>
</blockquote>
</li>
</ul>
</li>
<li><p>Longer sequences</p>
<ul>
<li><p>对于Transformers模型，我们可以通过模型的序列长度是有限的。大多数模型处理多达512或1024个令牌的序列，当要求处理更长的序列时，会崩溃。此问题有两种解决方案：</p>
<ul>
<li><p>使用支持的序列长度较长的模型。</p>
</li>
<li><p>截断序列。</p>
</li>
</ul>
</li>
<li><p>模型有不同的支持序列长度，有些模型专门处理很长的序列。 <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/longformer.html">Longformer</a> 这是一个例子，另一个是 <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/led.html">LED</a> . 如果您正在处理一项需要很长序列的任务，我们建议您查看这些模型。否则，我们建议您通过指定max_sequence_length参数：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">sequence = sequence[:max_sequence_length]<br></code></pre></td></tr></tbody></table></figure></li>
</ul>
</li>
</ul>
<h2 id="Putting-it-all-together"><a href="#Putting-it-all-together" class="headerlink" title="Putting it all together"></a>Putting it all together</h2><ul>
<li>Transformers API可以通过一个高级函数为我们处理所有这些</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer<br><br>checkpoint = <span class="hljs-string">"distilbert-base-uncased-finetuned-sst-2-english"</span><br>tokenizer = AutoTokenizer.from_pretrained(checkpoint)<br><br>sequence = <span class="hljs-string">"I've been waiting for a HuggingFace course my whole life."</span><br><br>model_inputs = tokenizer(sequence)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p><code>model_inputs</code> 变量包含模型良好运行所需的一切。对于DistilBERT，它包括输入 ID和注意力掩码(attention mask)。其他接受额外输入的模型也会有tokenizer的输出。</p>
</blockquote>
<ul>
<li>一次处理多个序列：</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">sequences = [<span class="hljs-string">"I've been waiting for a HuggingFace course my whole life."</span>, <span class="hljs-string">"So have I!"</span>]<br><br>model_inputs = tokenizer(sequences)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>标记器对象可以处理到特定框架张量的转换，然后可以直接发送到模型。例如，在下面的代码示例中，我们提示标记器从不同的框架返回张量——<code>"pt"</code>返回Py Torch张量，<code>"tf"</code>返回TensorFlow张量，<code>"np"</code>返回NumPy数组：</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">sequences = [<span class="hljs-string">"I've been waiting for a HuggingFace course my whole life."</span>, <span class="hljs-string">"So have I!"</span>]<br><br><span class="hljs-comment"># Returns PyTorch tensors</span><br>model_inputs = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">"pt"</span>)<br><br><span class="hljs-comment"># Returns TensorFlow tensors</span><br>model_inputs = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">"tf"</span>)<br><br><span class="hljs-comment"># Returns NumPy arrays</span><br>model_inputs = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">"np"</span>)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li><p>Special tokens</p>
<ul>
<li><p>我们看一下标记器返回的输入 ID，我们会发现它们与之前的略有不同：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">sequence = <span class="hljs-string">"I've been waiting for a HuggingFace course my whole life."</span><br><br>model_inputs = tokenizer(sequence)<br><span class="hljs-built_in">print</span>(model_inputs[<span class="hljs-string">"input_ids"</span>])<br><br>tokens = tokenizer.tokenize(sequence)<br>ids = tokenizer.convert_tokens_to_ids(tokens)<br><span class="hljs-built_in">print</span>(ids)<br><br><span class="hljs-built_in">print</span>(tokenizer.decode(model_inputs[<span class="hljs-string">"input_ids"</span>]))<br><span class="hljs-built_in">print</span>(tokenizer.decode(ids))<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>Result</p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]<br><br>[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]<br><br>"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"<br><br>"i've been waiting for a huggingface course my whole life."<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>Tokenizer在开头添加了特殊单词<code>[CLS]</code>，在结尾添加了特殊单词<code>[SEP]</code>。这是因为模型是用这些数据预训练的，所以为了得到相同的推理结果，我们还需要添加它们。请注意，有些模型不添加特殊单词，或者添加不同的单词；模型也可能只在开头或结尾添加这些特殊单词。在任何情况下，Tokenizer都知道需要哪些词符，并将为您处理这些词符。</p>
</li>
</ul>
</li>
<li><p>Wrapping up: From tokenizer to model</p>
</li>
</ul>
<p>现在我们已经看到了标记器对象在应用于文本时使用的所有单独步骤，让我们最后一次看看它如何处理多个序列（填充！），非常长的序列（截断！），以及多种类型的张量及其主要API：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification<br><br>checkpoint = <span class="hljs-string">"distilbert-base-uncased-finetuned-sst-2-english"</span><br>tokenizer = AutoTokenizer.from_pretrained(checkpoint)<br>model = AutoModelForSequenceClassification.from_pretrained(checkpoint)<br>sequences = [<span class="hljs-string">"I've been waiting for a HuggingFace course my whole life."</span>, <span class="hljs-string">"So have I!"</span>]<br><br>tokens = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">"pt"</span>)<br>output = model(**tokens)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<ol>
<li><strong>填充（Padding）</strong>：将较短的序列用特殊的填充值（通常是<code>&lt;PAD&gt;</code> token）扩展到与最长序列相同的长度。这可以避免批处理中因为序列长度不一致而导致的张量维度不匹配的问题。</li>
<li><strong>截断（Truncation）</strong>：对于过长的序列，截取到指定的最大长度，避免序列过长超过模型的最大处理能力。</li>
</ol>
<p><code>padding=True</code>和<code>truncation=True</code>一起使用，可以确保所有输入序列的长度一致，从而可以方便地进行批量处理。<code>return_tensors="pt"</code>则表示返回的是PyTorch张量。</p>
</blockquote>
<h2 id="Summary-2"><a href="#Summary-2" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li><p>学习的内容</p>
<ul>
<li><p>学习了Transformers模型的基本构造块。</p>
</li>
<li><p>了解了标记化管道的组成。</p>
</li>
<li><p>了解了如何在实践中使用Transformers模型。</p>
</li>
<li><p>学习了如何利用分词器将文本转换为模型可以理解的张量。</p>
</li>
<li><p>将分词器和模型一起设置，以从文本到预测。</p>
</li>
<li><p>了解了inputs IDs的局限性，并了解了attention mask。</p>
</li>
<li><p>使用多功能和可配置的分词器方法。</p>
</li>
</ul>
</li>
<li><p>Little test: <a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/en/chapter2/8?fw=pt">https://huggingface.co/learn/nlp-course/en/chapter2/8?fw=pt</a></p>
</li>
</ul>
<h1 id="Fine-tuning-a-pretrained-model"><a href="#Fine-tuning-a-pretrained-model" class="headerlink" title="Fine-tuning a pretrained model"></a>Fine-tuning a pretrained model</h1><h2 id="Introduction-2"><a href="#Introduction-2" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li>Table of contents<ul>
<li>如何从模型中心(hub)准备大型数据集</li>
<li>如何使用高级<code>训练</code>API微调一个模型</li>
<li>如何使用自定义训练过程</li>
<li>如何利用🤗 Accelerate库在任何分布式设备上轻松运行自定义训练过程</li>
</ul>
</li>
</ul>
<h2 id="Processing-the-data"><a href="#Processing-the-data" class="headerlink" title="Processing the data"></a>Processing the data</h2><ul>
<li>Easy demo</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW, AutoTokenizer, AutoModelForSequenceClassification<br><br><span class="hljs-comment"># Same as before</span><br>checkpoint = <span class="hljs-string">"bert-base-uncased"</span><br>tokenizer = AutoTokenizer.from_pretrained(checkpoint)<br>model = AutoModelForSequenceClassification.from_pretrained(checkpoint)<br>sequences = [<br>    <span class="hljs-string">"I've been waiting for a HuggingFace course my whole life."</span>,<br>    <span class="hljs-string">"This course is amazing!"</span>,<br>]<br>batch = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">"pt"</span>)<br><br><span class="hljs-comment"># This is new</span><br>batch[<span class="hljs-string">"labels"</span>] = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br><br>optimizer = AdamW(model.parameters())<br>loss = model(**batch).loss<br>loss.backward()<br>optimizer.step()<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>数据太少了，训练没有效果！</p>
</blockquote>
<ul>
<li><p>MRPC数据集Demo</p>
<ul>
<li><p>该数据集由威廉·多兰和克里斯·布罗克特在<a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/I05-5002.pdf">这篇文章</a>发布。该数据集由5801对句子组成，每个句子对带有一个标签，指示它们是否为同义（即，如果两个句子的意思相同）。我们在本章中选择了它，因为它是一个小数据集，所以很容易对它进行训练。</p>
</li>
<li><p>点击<a target="_blank" rel="noopener" href="https://huggingface.co/datasets">数据集</a>的链接即可进行浏览，也可以学习：<a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/loading">加载和处理新的数据集</a>这篇文章。我们使用MRPC数据集中的<a target="_blank" rel="noopener" href="https://gluebenchmark.com/">GLUE 基准测试数据集</a>，它是构成MRPC数据集的10个数据集之一，这是一个学术基准，用于衡量机器学习模型在10个不同文本分类任务中的性能。</p>
</li>
<li><p>Datasets库提供了一个非常便捷的命令，可以在模型中心（hub）上下载和缓存数据集。我们可以通过以下的代码下载MRPC数据集：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br><br>raw_datasets = load_dataset(<span class="hljs-string">"glue"</span>, <span class="hljs-string">"mrpc"</span>)<br>raw_datasets<br><span class="hljs-comment"># 此命令在下载数据集并缓存到 ~/.cache/huggingface/datasets，您可以通过设置HF_HOME环境变量来自定义缓存的文件夹。</span><br><br><span class="hljs-comment"># Result</span><br>DatasetDict({<br>    train: Dataset({<br>        features: [<span class="hljs-string">'sentence1'</span>, <span class="hljs-string">'sentence2'</span>, <span class="hljs-string">'label'</span>, <span class="hljs-string">'idx'</span>],<br>        num_rows: <span class="hljs-number">3668</span><br>    })<br>    validation: Dataset({<br>        features: [<span class="hljs-string">'sentence1'</span>, <span class="hljs-string">'sentence2'</span>, <span class="hljs-string">'label'</span>, <span class="hljs-string">'idx'</span>],<br>        num_rows: <span class="hljs-number">408</span><br>    })<br>    test: Dataset({<br>        features: [<span class="hljs-string">'sentence1'</span>, <span class="hljs-string">'sentence2'</span>, <span class="hljs-string">'label'</span>, <span class="hljs-string">'idx'</span>],<br>        num_rows: <span class="hljs-number">1725</span><br>    })<br>})<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>我们获得了一个<strong>DatasetDict</strong>对象，其中包含训练集、验证集和测试集。每一个集合都包含几个列(<strong>sentence1</strong>, <strong>sentence2</strong>, <strong>label</strong>, and <strong>idx</strong>)以及一个代表行数的变量。</p>
</blockquote>
</li>
<li><p>访问数据</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">raw_train_dataset = raw_datasets[<span class="hljs-string">"train"</span>]<br>raw_train_dataset[<span class="hljs-number">0</span>]<br><span class="hljs-comment"># 这个就是一条真真切切，含有各种feature的数据</span><br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>要知道哪个数字对应于哪个标签，我们可以查看<strong>raw_train_dataset</strong>的<strong>features</strong>. </p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">raw_train_dataset.features<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p><strong>Label（标签）</strong> 是一种<strong>ClassLabel（分类标签）</strong>，使用整数建立起到类别标签的映射关系。<strong>0</strong>对应于<strong>not_equivalent</strong>，<strong>1</strong>对应于<strong>equivalent</strong>。</p>
</blockquote>
</li>
</ul>
</li>
<li><p>预处理数据集</p>
<ul>
<li><p>将文本转换为模型能够理解的数字</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs = tokenizer(<span class="hljs-string">"This is the first sentence."</span>, <span class="hljs-string">"This is the second one."</span>)<br>inputs<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p><strong>输入词id(input_ids)</strong> 和 <strong>注意力遮罩(attention_mask)</strong> ，**类型标记ID(token_type_ids)**的作用就是告诉模型输入的哪一部分是第一句，哪一部分是第二句。</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">tokenizer.convert_ids_to_tokens(inputs[<span class="hljs-string">"input_ids"</span>])<br><br><span class="hljs-comment"># ['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']</span><br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p><strong>[CLS] sentence1 [SEP] sentence2 [SEP]<strong>，输入中 <strong>[CLS] sentence1 [SEP]</strong> 它们的类型标记ID均为</strong>0</strong>，而其他部分，对应于<strong>sentence2 [SEP]<strong>，所有的类型标记ID均为</strong>1</strong>。</p>
</blockquote>
</li>
<li><p>如果选择其他的checkpoint，则不一定具有<strong>token_type_ids</strong>（例如，如果使用DistilBERT模型，就不会返回它们）。只有当它在预训练期间使用过这一层，模型在构建时依赖它们，才会返回它们。</p>
</li>
</ul>
</li>
<li><p>使用<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter1">第一章</a>的遮罩语言模型，还有一个额外的应用类型，叫做下一句预测。</p>
<ul>
<li><p>训练过程中，会给模型输入成对的句子（带有随机遮罩的标记），并被要求预测第二个句子是否紧跟第一个句子。为了提高模型的泛化能力，数据集中一半的两个句子在原始文档中挨在一起，另一半的两个句子来自两个不同的文档。</p>
</li>
<li><p>一般来说，不需要担心是否有token_type_ids。在您的标输入中：只要您对标记器和模型使用相同的检查点，一切都会很好，因为标记器知道向其模型提供什么。</p>
</li>
<li><p>我们可以给标记器提供一组句子，第一个参数是它第一个句子的列表，第二个参数是第二个句子的列表。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">tokenized_dataset = tokenizer(<br>    raw_datasets[<span class="hljs-string">"train"</span>][<span class="hljs-string">"sentence1"</span>],<br>    raw_datasets[<span class="hljs-string">"train"</span>][<span class="hljs-string">"sentence2"</span>],<br>    padding=<span class="hljs-literal">True</span>,<br>    truncation=<span class="hljs-literal">True</span>,<br>)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>它的缺点是返回字典（字典的键是<strong>输入词id(input_ids)</strong> ， <strong>注意力遮罩(attention_mask)</strong> 和 **类型标记ID(token_type_ids)**，字典的值是键所对应值的列表）。而且只有当您在转换过程中有足够的内存来存储整个数据集时才不会出错</p>
</blockquote>
</li>
<li><p>Hugging face数据集库中的数据集是以<a target="_blank" rel="noopener" href="https://arrow.apache.org/">Apache Arrow</a>文件存储在磁盘上，因此您只需将接下来要用的数据加载在内存中，因此会对内存容量的需求要低一些。</p>
</li>
</ul>
</li>
<li><p>为了将数据保存为数据集，我们将使用<a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map">Dataset.map()</a>方法，如果我们需要做更多的预处理而不仅仅是标记化，那么这也给了我们一些额外的自定义的方法。这个方法的工作原理是在数据集的每个元素上应用一个函数。</p>
<ul>
<li><p>代码</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">example</span>):<br>    <span class="hljs-keyword">return</span> tokenizer(example[<span class="hljs-string">"sentence1"</span>], example[<span class="hljs-string">"sentence2"</span>], truncation=<span class="hljs-literal">True</span>)<br>  <br>tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)<br>tokenized_datasets<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<ul>
<li>输入是一个字典（与数据集的项类似），并返回一个包含<strong>输入词id(input_ids)</strong> ， <strong>注意力遮罩(attention_mask)</strong> 和 <strong>类型标记ID(token_type_ids)</strong> 键的新字典。</li>
<li>可以处理成对的句子列表 像上面的示例一样，如果键所对应的值包含多个句子（每个键作为一个句子列表），那么它依然可以工作。</li>
<li>我们可以在调用<strong>map()<strong>使用该选项 <strong>batched=True</strong> ，这将显著加快标记与标记的速度。这个</strong>标记器</strong>来自<a target="_blank" rel="noopener" href="https://github.com/huggingface/tokenizers">🤗 Tokenizers</a>库由Rust编写而成。当我们一次给它大量的输入时，这个标记器可以非常快。</li>
</ul>
</blockquote>
</li>
<li><p>我们现在在标记函数中省略了<strong>padding</strong>参数。这是因为在标记的时候将所有样本填充到最大长度的效率不高。一个更好的做法：在构建批处理时填充样本更好，因为这样我们只需要填充到该批处理中的最大长度，而不是整个数据集的最大长度。当输入长度变化很大时，这可以节省大量时间和处理能力!</p>
</li>
<li><p>Datasets库应用这种处理的方式是向数据集添加新的字段，those three fields are added to all splits of our dataset. </p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">DatasetDict({<br>    train: Dataset({<br>        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],<br>        num_rows: 3668<br>    })<br>    validation: Dataset({<br>        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],<br>        num_rows: 408<br>    })<br>    test: Dataset({<br>        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],<br>        num_rows: 1725<br>    })<br>})<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>You can even use multiprocessing when applying your preprocessing function with <code>map()</code> by passing along a <code>num_proc</code> argument. We didn’t do this here because the Tokenizers library already uses multiple threads to tokenize our samples faster, but if you are <strong>not using a fast tokenizer backed by this library</strong>, this could speed up your preprocessing.</p>
</li>
<li><p>我们的tokenize_function返回一个包含input_ids、attention_mask和token_type_ids键的字典，因此这三个字段被添加到我们数据集的所有分割中。请注意，如果我们的预处理函数返回了数据集中现有键的新值，我们也可以改变现有字段，前提是我们对数据集应用了map()。</p>
</li>
</ul>
</li>
<li><p>Dynamic padding</p>
<ul>
<li><p>负责将样本组合成一个批次的函数称为collate函数。它是你在构建DataLoader时可以传递的一个参数，默认情况下是一个函数，它只会将你的样本转换为PyTorch张量并将它们连接起来（如果你的元素是列表、元组或字典，则递归连接）。在我们的案例中，这将是不可能的，因为我们的输入不会全部是相同的大小。我们故意推迟了填充，只在每个批次中按需应用它，以避免有过多填充的过长输入。这将通过相当多的方式加快训练速度，但请注意，如果你在TPU上训练，它可能会导致问题——TPU更喜欢固定的形状，即使这需要额外的填充。</p>
</li>
<li><p>collate函数，它将对数据集中我们想要组合成批次的项目应用正确的填充量。Transformers库通过DataCollatorWithPadding为我们提供了这样一个函数。当你实例化它时，它需要一个tokenizer（以知道使用哪个填充标记，以及模型是否期望填充在输入的左侧还是右侧），并且会做你需要做的一切：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding<br><br>data_collator = DataCollatorWithPadding(tokenizer=tokenizer)<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>从我们的训练集中抓取一些我们想要组合在一起的样本。在这里，我们移除了idx、sentence1和sentence2列，因为它们不需要，并且包含字符串。（Tensor中不能包含字符串），查看每个条目的长度：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">samples = tokenized_datasets[<span class="hljs-string">"train"</span>][:<span class="hljs-number">8</span>]<br>samples = {k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> samples.items() <span class="hljs-keyword">if</span> k <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">"idx"</span>, <span class="hljs-string">"sentence1"</span>, <span class="hljs-string">"sentence2"</span>]}<br>[<span class="hljs-built_in">len</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> samples[<span class="hljs-string">"input_ids"</span>]]<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>到了不同长度的样本，从32到67。动态填充意味着这个一次sample中，所有的样本都应该被填充到67的长度，即批次内的最大长度。如果没有动态填充，所有的样本都必须被填充到整个数据集中的最大长度，或者模型可以接受的最大长度。</p>
</blockquote>
</li>
<li><p>动态填充效果</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">batch = data_collator(samples)<br>{k: v.shape <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>Result</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">{<span class="hljs-string">'attention_mask'</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),<br> <span class="hljs-string">'input_ids'</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),<br> <span class="hljs-string">'token_type_ids'</span>: torch.Size([<span class="hljs-number">8</span>, <span class="hljs-number">67</span>]),<br> <span class="hljs-string">'labels'</span>: torch.Size([<span class="hljs-number">8</span>])}<br></code></pre></td></tr></tbody></table></figure></li>
</ul>
</li>
</ul>
<h2 id="Fine-tuning-a-model-with-the-Trainer-API"><a href="#Fine-tuning-a-model-with-the-Trainer-API" class="headerlink" title="Fine-tuning a model with the Trainer API"></a>Fine-tuning a model with the Trainer API</h2><ul>
<li><p>Train</p>
<ul>
<li><p>Transformers提供了一个 <strong>Trainer</strong> 类来帮助您在自己的数据集上微调任何预训练模型。预处理数据：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, DataCollatorWithPadding<br><br>raw_datasets = load_dataset(<span class="hljs-string">"glue"</span>, <span class="hljs-string">"mrpc"</span>)<br>checkpoint = <span class="hljs-string">"bert-base-uncased"</span><br>tokenizer = AutoTokenizer.from_pretrained(checkpoint)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">example</span>):<br>    <span class="hljs-keyword">return</span> tokenizer(example[<span class="hljs-string">"sentence1"</span>], example[<span class="hljs-string">"sentence2"</span>], truncation=<span class="hljs-literal">True</span>)<br><br><br>tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)<br>data_collator = DataCollatorWithPadding(tokenizer=tokenizer)<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>第一步是定义一个 <strong>TrainingArguments</strong> 类，它将包含 <strong>Trainer</strong>用于训练和评估的所有超参数。您唯一必须提供的参数是保存训练模型的目录，以及训练过程中的checkpoint。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments<br><br>training_args = TrainingArguments(<span class="hljs-string">"test-trainer"</span>)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>如果您想在训练期间自动将模型上传到 Hub，请将push_to_hub=True添加到TrainingArguments之中</p>
</blockquote>
</li>
<li><p>第二步是定义我们的模型，我们将使用 <strong>AutoModelForSequenceClassification</strong>类，它有两个参数：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification<br><br>model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="hljs-number">2</span>)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>在实例化此预训练模型后会收到警告。这是因为 BERT 没有在句子对分类方面进行过预训练，所以预训练模型的头部已经被丢弃，而是添加了一个适合句子序列分类的新头部。警告表明一些权重没有使用（对应于丢弃的预训练头的那些），而其他一些权重被随机初始化（新头的那些）。最后鼓励您训练模型，这正是我们现在要做的。</p>
</blockquote>
</li>
<li><p>第三步就可以定义一个 <strong>Trainer</strong> 通过将之前构造的所有对象传递给它</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer<br><br>trainer = Trainer(<br>  model,<br>  training_args,<br>  train_dataset=tokenized_datasets[<span class="hljs-string">"train"</span>],<br>  eval_dataset=tokenized_datasets[<span class="hljs-string">"validation"</span>],<br>  data_collator=data_collator,<br>  tokenizer=tokenizer,<br>)<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>开始训练：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">trainer.train()<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>这将开始微调，并每500步报告一次训练损失。但是，它不会告诉您模型的性能如何（或质量如何）。这是因为:</p>
<ol>
<li>我们没有通过将<strong>evaluation_strategy</strong>设置为“<strong>steps</strong>”(在每次更新参数的时候评估)或“<strong>epoch</strong>”(在每个epoch结束时评估)来告诉<strong>Trainer</strong>在训练期间进行评估。</li>
<li>我们没有为<strong>Trainer</strong>提供一个**compute_metrics()**函数来直接计算模型的好坏(否则评估将只输出loss，这不是一个非常直观的数字)。</li>
</ol>
</blockquote>
</li>
</ul>
</li>
<li><p>Evaluate</p>
<ul>
<li><p>构建一个有用的 <strong>compute_metrics()</strong> 函数并在我们下次训练时使用它。该函数必须采用 <strong>EvalPrediction</strong> 对象（带有 <strong>predictions</strong> 和 <strong>label_ids</strong> 字段的参数元组）并将返回一个字符串到浮点数的字典（字符串是返回的指标的名称，而浮点数是它们的值）。我们可以使用 <strong>Trainer.predict()</strong> 命令来使用我们的模型进行预测：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">predictions = trainer.predict(tokenized_datasets[<span class="hljs-string">"validation"</span>])<br><span class="hljs-built_in">print</span>(predictions.predictions.shape, predictions.label_ids.shape)<br><br><span class="hljs-comment"># Result</span><br>(<span class="hljs-number">408</span>, <span class="hljs-number">2</span>) (<span class="hljs-number">408</span>,)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p><strong>predict()</strong> 的输出结果是具有三个字段的命名元组： <strong>predictions</strong> , <strong>label_ids</strong> ， 和 <strong>metrics</strong> .这 <strong>metrics</strong> 字段将只包含传递的数据集的loss，以及一些运行时间（预测所需的总时间和平均时间）。如果我们定义了自己的 <strong>compute_metrics()</strong> 函数并将其传递给 <strong>Trainer</strong> ，该字段还将包含**compute_metrics()**的结果。</p>
<p><strong>predict()</strong> 方法是具有三个字段的命名元组： <strong>predictions</strong> , <strong>label_ids</strong> ， 和 <strong>metrics</strong> .这 <strong>metrics</strong> 字段将只包含传递的数据集的loss，以及一些运行时间（预测所需的总时间和平均时间）。如果我们定义了自己的 <strong>compute_metrics()</strong> 函数并将其传递给 <strong>Trainer</strong> ，该字段还将包含<strong>compute_metrics()</strong> 的结果。如你看到的， <strong>predictions</strong> 是一个形状为 408 x 2 的二维数组（408 是我们使用的数据集中元素的数量）。这些是我们传递给**predict()**的数据集的每个元素的结果(logits)。</p>
</blockquote>
</li>
<li><p>要将我们的预测的可以与真正的标签进行比较，我们需要在第二个轴上取最大值的索引：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>preds = np.argmax(predictions.predictions, axis=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>现在建立我们的 compute_metric() 函数来较为直观地评估模型的好坏，我们将使用   Evaluate 库中的指标。我们可以像加载数据集一样轻松加载与 MRPC 数据集关联的指标，这次使用 evaluate.load() 函数。返回的对象有一个 compute()方法我们可以用来进行度量计算的方法：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> evaluate<br><br>metric = evaluate.load(<span class="hljs-string">"glue"</span>, <span class="hljs-string">"mrpc"</span>)<br>metric.compute(predictions=preds, references=predictions.label_ids)<br><br><span class="hljs-comment"># Result {'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}</span><br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>将所有东西打包在一起，我们得到了我们的 <strong>compute_metrics()</strong> 函数：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_preds</span>):<br>    metric = evaluate.load(<span class="hljs-string">"glue"</span>, <span class="hljs-string">"mrpc"</span>)<br>    logits, labels = eval_preds<br>    predictions = np.argmax(logits, axis=-<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>为了查看模型在每个训练周期结束的好坏，下面是我们如何使用**compute_metrics()**函数定义一个新的 <strong>Trainer</strong> ：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">training_args = TrainingArguments(<span class="hljs-string">"test-trainer"</span>, evaluation_strategy=<span class="hljs-string">"epoch"</span>)<br>model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="hljs-number">2</span>)<br><br>trainer = Trainer(<br>    model,<br>    training_args,<br>    train_dataset=tokenized_datasets[<span class="hljs-string">"train"</span>],<br>    eval_dataset=tokenized_datasets[<span class="hljs-string">"validation"</span>],<br>    data_collator=data_collator,<br>    tokenizer=tokenizer,<br>    compute_metrics=compute_metrics,<br>)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>它将在训练loss之外，还会输出每个 epoch 结束时的验证loss和指标。同样，由于模型的随机头部初始化，您达到的准确率/F1 分数可能与我们发现的略有不同，但它应该在同一范围内。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<h2 id="A-Full-Training"><a href="#A-Full-Training" class="headerlink" title="A Full Training"></a>A Full Training</h2><ul>
<li>简单总结：</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, DataCollatorWithPadding<br><br>raw_datasets = load_dataset(<span class="hljs-string">"glue"</span>, <span class="hljs-string">"mrpc"</span>)<br>checkpoint = <span class="hljs-string">"bert-base-uncased"</span><br>tokenizer = AutoTokenizer.from_pretrained(checkpoint)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">example</span>):<br>    <span class="hljs-keyword">return</span> tokenizer(example[<span class="hljs-string">"sentence1"</span>], example[<span class="hljs-string">"sentence2"</span>], truncation=<span class="hljs-literal">True</span>)<br><br><br>tokenized_datasets = raw_datasets.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)<br>data_collator = DataCollatorWithPadding(tokenizer=tokenizer)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li><p>我们需要对我们的<code>tokenized_datasets</code>做一些处理，来处理<code>Trainer</code>自动为我们做的一些事情。具体来说，我们需要:</p>
<ul>
<li>删除与模型不期望的值相对应的列（如<code>sentence1</code>和<code>sentence2</code>列）。</li>
<li>将列名<code>label</code>重命名为<code>labels</code>（因为模型期望参数是<code>labels</code>）。</li>
<li>设置数据集的格式，使其返回 PyTorch 张量而不是列表。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">tokenized_datasets = tokenized_datasets.remove_columns([<span class="hljs-string">"sentence1"</span>, <span class="hljs-string">"sentence2"</span>, <span class="hljs-string">"idx"</span>])<br>tokenized_datasets = tokenized_datasets.rename_column(<span class="hljs-string">"label"</span>, <span class="hljs-string">"labels"</span>)<br>tokenized_datasets.set_format(<span class="hljs-string">"torch"</span>)<br>tokenized_datasets[<span class="hljs-string">"train"</span>].column_names<br><br><br><span class="hljs-comment"># Result ["attention_mask", "input_ids", "labels", "token_type_ids"]</span><br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>定义Dataloader</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br>train_dataloader = DataLoader(<br>    tokenized_datasets[<span class="hljs-string">"train"</span>], shuffle=<span class="hljs-literal">True</span>, batch_size=<span class="hljs-number">8</span>, collate_fn=data_collator<br>)<br>eval_dataloader = DataLoader(<br>    tokenized_datasets[<span class="hljs-string">"validation"</span>], batch_size=<span class="hljs-number">8</span>, collate_fn=data_collator<br>)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>为了快速检验数据处理中没有错误，我们可以这样检验其中的一个批次:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:<br>    <span class="hljs-keyword">break</span><br>{k: v.shape <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}<br></code></pre></td></tr></tbody></table></figure>

<p>实际的形状可能略有不同，因为我们为训练数据加载器设置了<code>shuffle=True</code>，并且模型会将句子填充到<code>batch</code>中的最大长度。</p>
</blockquote>
</li>
<li><p>模型</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification<br><br>model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="hljs-number">2</span>)<br><br>outputs = model(**batch)<br><span class="hljs-built_in">print</span>(outputs.loss, outputs.logits.shape)<br><br><span class="hljs-comment"># Result tensor(0.5441, grad_fn=&lt;NllLossBackward&gt;) torch.Size([8, 2])</span><br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>当我们提供 <code>labels</code> 时，Transformers 模型都将返回这个<code>batch</code>的<code>loss</code>，我们还得到了 <code>logits</code>(<code>batch</code>中的每个输入有两个，所以张量大小为 8 x 2)。</p>
</blockquote>
</li>
<li><p>优化器和学习率调度器</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler<br><br>optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">5e-5</span>) <br>num_epochs = <span class="hljs-number">3</span><br>num_training_steps = num_epochs * <span class="hljs-built_in">len</span>(train_dataloader)<br>lr_scheduler = get_scheduler(<br>    <span class="hljs-string">"linear"</span>,<br>    optimizer=optimizer,<br>    num_warmup_steps=<span class="hljs-number">0</span>,<br>    num_training_steps=num_training_steps,<br>)<br><span class="hljs-built_in">print</span>(num_training_steps)<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>循环训练：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm<br><br>device = torch.device(<span class="hljs-string">"cuda"</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">"cpu"</span>)<br>model.to(device)<br><br>progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))<br><br>model.train()<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:<br>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}<br>        outputs = model(**batch)<br>        loss = outputs.loss<br>        loss.backward()<br><br>        optimizer.step()<br>        lr_scheduler.step()<br>        optimizer.zero_grad()<br>        progress_bar.update(<span class="hljs-number">1</span>)<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>添加评估：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> evaluate<br><br>metric = evaluate.load(<span class="hljs-string">"glue"</span>, <span class="hljs-string">"mrpc"</span>)<br>model.<span class="hljs-built_in">eval</span>()<br><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> eval_dataloader:<br>    batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        outputs = model(**batch)<br><br>    logits = outputs.logits<br>    predictions = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)<br>    metric.add_batch(predictions=predictions, references=batch[<span class="hljs-string">"labels"</span>])<br><br>metric.compute()<br><br><span class="hljs-comment"># Result: {'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}</span><br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>Evaluate 库提供的指标。我们已经了解了 <code>metric.compute()</code> 方法，当我们使用 <code>add_batch()</code>方法进行预测循环时，实际上该指标可以为我们累积所有 <code>batch</code> 的结果。一旦我们累积了所有 <code>batch</code> ，我们就可以使用 <code>metric.compute()</code> 得到最终结果 .以下是在评估循环中实现所有这些的方法。</p>
</blockquote>
</li>
<li><p>使用Accelerate加速</p>
<ul>
<li><p>使用<a target="_blank" rel="noopener" href="https://github.com/huggingface/accelerate">Accelerate</a>库，只需进行一些调整，我们就可以在多个 GPU 或 TPU 上启用分布式训练。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW, AutoModelForSequenceClassification, get_scheduler<br><br>model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="hljs-number">2</span>)<br>optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">3e-5</span>)<br><br>device = torch.device(<span class="hljs-string">"cuda"</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">"cpu"</span>)<br>model.to(device)<br><br>num_epochs = <span class="hljs-number">3</span><br>num_training_steps = num_epochs * <span class="hljs-built_in">len</span>(train_dataloader)<br>lr_scheduler = get_scheduler(<br>    <span class="hljs-string">"linear"</span>,<br>    optimizer=optimizer,<br>    num_warmup_steps=<span class="hljs-number">0</span>,<br>    num_training_steps=num_training_steps,<br>)<br><br>progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))<br><br>model.train()<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:<br>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}<br>        outputs = model(**batch)<br>        loss = outputs.loss<br>        loss.backward()<br><br>        optimizer.step()<br>        lr_scheduler.step()<br>        optimizer.zero_grad()<br>        progress_bar.update(<span class="hljs-number">1</span>)<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>加入Accelerate</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW, AutoModelForSequenceClassification, get_scheduler<br><br>accelerator = Accelerator()<br><br>model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="hljs-number">2</span>)<br>optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">3e-5</span>)<br><br>train_dl, eval_dl, model, optimizer = accelerator.prepare(<br>    train_dataloader, eval_dataloader, model, optimizer<br>)<br><br>num_epochs = <span class="hljs-number">3</span><br>num_training_steps = num_epochs * <span class="hljs-built_in">len</span>(train_dl)<br>lr_scheduler = get_scheduler(<br>    <span class="hljs-string">"linear"</span>,<br>    optimizer=optimizer,<br>    num_warmup_steps=<span class="hljs-number">0</span>,<br>    num_training_steps=num_training_steps,<br>)<br><br>progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))<br><br>model.train()<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dl:<br>        outputs = model(**batch)<br>        loss = outputs.loss<br>        accelerator.backward(loss)<br><br>        optimizer.step()<br>        lr_scheduler.step()<br>        optimizer.zero_grad()<br>        progress_bar.update(<span class="hljs-number">1</span>)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>要添加的第一行是导入<code>Accelerator</code>。第二行实例化一个 <code>Accelerator</code>对象 ，它将查看环境并初始化适当的分布式设置。 Accelerate 为您处理数据在设备间的传递，因此您可以删除将模型放在设备上的那行代码（或者，如果您愿意，可使用 <code>accelerator.device</code> 代替 <code>device</code> ）。</p>
<p>大部分工作会在将数据加载器、模型和优化器发送到的<code>accelerator.prepare()</code>中完成。这将会把这些对象包装在适当的容器中，以确保您的分布式训练按预期工作。要进行的其余更改是删除将<code>batch</code>放在 <code>device</code> 的那行代码（同样，如果您想保留它，您可以将其更改为使用 <code>accelerator.device</code> ) 并将 <code>loss.backward()</code> 替换为<code>accelerator.backward(loss)</code>。</p>
</blockquote>
</li>
<li><p>⚠️ 为了使云端 TPU 提供的加速发挥最大的效益，我们建议使用标记器(tokenizer)的 <code>padding=max_length</code> 和 <code>max_length</code> 参数将您的样本填充到固定长度。</p>
</li>
<li><p>要在分布式设置中试用它，请运行以下命令:</p>
<figure class="highlight arduino"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arduino">accelerate config<br></code></pre></td></tr></tbody></table></figure>

<p>这将询问您几个配置的问题并将您的回答转储到此命令使用的配置文件中。</p>
</li>
<li><p>启动分布式训练</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">accelerate launch train.py<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>如果您想在 Notebook 中尝试此操作（例如，在 Colab 上使用 TPU 进行测试），只需将代码粘贴到 <code>training_function()</code> 并使用以下命令运行最后一个单元格:</p>
<figure class="highlight stylus"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">from accelerate import notebook_launcher<br><br><span class="hljs-function"><span class="hljs-title">notebook_launcher</span><span class="hljs-params">(training_function)</span></span><br></code></pre></td></tr></tbody></table></figure>

<p>您可以在<a target="_blank" rel="noopener" href="https://github.com/huggingface/accelerate/tree/main/examples">Accelerate repo</a>找到更多的示例。</p>
</li>
</ul>
</li>
</ul>
<h2 id="Summary-3"><a href="#Summary-3" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li>汇总：<ul>
<li>了解了<a target="_blank" rel="noopener" href="https://huggingface.co/datasets">Hub</a>中的数据集</li>
<li>学习了如何加载和预处理数据集，包括使用动态填充和整理器</li>
<li>实现您自己的模型微调和评估</li>
<li>实施了一个较为底层的训练循环</li>
<li>使用 Accelerate 调整您的训练循环，使其适用于多个 GPU 或 TPU</li>
</ul>
</li>
<li>Little test: <a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/en/chapter3/6?fw=pt">https://huggingface.co/learn/nlp-course/en/chapter3/6?fw=pt</a></li>
</ul>
<h1 id="Sharing-Models-and-Tokenizers-Optional"><a href="#Sharing-Models-and-Tokenizers-Optional" class="headerlink" title="Sharing Models and Tokenizers(Optional)"></a>Sharing Models and Tokenizers(Optional)</h1><h2 id="Using-pretrained-models"><a href="#Using-pretrained-models" class="headerlink" title="Using pretrained models"></a>Using pretrained models</h2><ul>
<li>你唯一需要注意的是所选检查点是否适合它将用于的任务。例如，这里我们正在将 <code>camembert-base</code> 检查点加载在 <code>fill-mask</code> 管道，这完全没问题。但是如果我们在 <code>text-classification</code> 管道加载检查点，结果没有任何意义，因为 <code>camembert-base</code> 不适合这个任务！我们建议使用 Hugging Face Hub 界面中的任务选择器来选择合适的检查点：</li>
</ul>
<p><img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/tasks.png" srcset="/img/loading.gif" lazyload alt="The task selector on the web interface."></p>
<ul>
<li>用pipeline使用</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br><br>camembert_fill_mask = pipeline(<span class="hljs-string">"fill-mask"</span>, model=<span class="hljs-string">"camembert-base"</span>)<br>results = camembert_fill_mask(<span class="hljs-string">"Le camembert est &lt;mask&gt; :)"</span>)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li><p>用model/tokenizer使用</p>
<ul>
<li><p>直接使用</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CamembertTokenizer, CamembertForMaskedLM<br><br>tokenizer = CamembertTokenizer.from_pretrained(<span class="hljs-string">"camembert-base"</span>)<br>model = CamembertForMaskedLM.from_pretrained(<span class="hljs-string">"camembert-base"</span>)<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>使用Auto类(我们建议使用 <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/auto.html?highlight=auto#auto-classes"><code>Auto*</code> 类</a>，因为 <code>Auto*</code> 类设计与架构无关。)：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForMaskedLM<br><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"camembert-base"</span>)<br>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">"camembert-base"</span>)<br></code></pre></td></tr></tbody></table></figure></li>
</ul>
</li>
</ul>
<h2 id="Sharing-pretrained-models"><a href="#Sharing-pretrained-models" class="headerlink" title="Sharing pretrained models"></a>Sharing pretrained models</h2><ul>
<li><p>创建新模型存储库的方法有以下三种：</p>
<ul>
<li><p>使用 push_to_hub API 接口</p>
</li>
<li><p>使用 huggingface_hub Python 库</p>
</li>
<li><p>使用 web 界面</p>
</li>
</ul>
</li>
<li><p>不是重点，其余请参考：<a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/zh-CN/chapter4/3?fw=pt">https://huggingface.co/learn/nlp-course/zh-CN/chapter4/3?fw=pt</a></p>
</li>
</ul>
<h2 id="Building-model-cards"><a href="#Building-model-cards" class="headerlink" title="Building model cards"></a>Building model cards</h2><ul>
<li>不是重点，其余请参考：<a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/zh-CN/chapter4/4?fw=pt">https://huggingface.co/learn/nlp-course/zh-CN/chapter4/4?fw=pt</a></li>
</ul>
<h2 id="Summary-4"><a href="#Summary-4" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li>此章节不是重点，请参考：<a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/zh-CN/chapter4/6?fw=pt">https://huggingface.co/learn/nlp-course/zh-CN/chapter4/6?fw=pt</a></li>
</ul>
<h1 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h1><h2 id="Introduction-3"><a href="#Introduction-3" class="headerlink" title="Introduction"></a>Introduction</h2><ul>
<li><p>我们将找到以下问题的答案：</p>
<ul>
<li><p>当数据集不在hub上时，您该怎么做？</p>
</li>
<li><p>如何对数据集进行切片？（如果你真正的特别需要使用pandas的时候该怎么办？）</p>
</li>
<li><p>当你的数据集很大，会撑爆你笔记本电脑的RAM时，你会怎么做？</p>
</li>
<li><p>“内存映射”和Apache Arrow到底是什么？</p>
</li>
<li><p>如何创建自己的数据集并将其推送到中心？</p>
</li>
</ul>
</li>
</ul>
<h2 id="Dataset-Loading"><a href="#Dataset-Loading" class="headerlink" title="Dataset Loading"></a>Dataset Loading</h2><h3 id="Local-dataset"><a href="#Local-dataset" class="headerlink" title="Local dataset"></a>Local dataset</h3><ul>
<li>加载数据集的方法：</li>
</ul>
<p>atasets 提供了加载脚本来加载本地和远程数据集。它支持几种常见的数据格式，例如：</p>
<table>
<thead>
<tr>
<th>Data format</th>
<th>Loading script</th>
<th>Example</th>
</tr>
</thead>
<tbody><tr>
<td>CSV &amp; TSV</td>
<td><code>csv</code></td>
<td><code>load_dataset("csv", data_files="my_file.csv")</code></td>
</tr>
<tr>
<td>Text files</td>
<td><code>text</code></td>
<td><code>load_dataset("text", data_files="my_file.txt")</code></td>
</tr>
<tr>
<td>JSON &amp; JSON Lines</td>
<td><code>json</code></td>
<td><code>load_dataset("json", data_files="my_file.jsonl")</code></td>
</tr>
<tr>
<td>Pickled DataFrames</td>
<td><code>pandas</code></td>
<td><code>load_dataset("pandas", data_files="my_dataframe.pkl")</code></td>
</tr>
</tbody></table>
<p>如表所示, 对于每种数据格式, 我们只需要使用 <code>load_dataset()</code> 函数, 使用 <code>data_files</code> 指定一个或多个文件的路径的参数。 让我们从本地文件加载数据集开始；稍后我们将看到如何对远程文件执行相同的操作。</p>
<ul>
<li><p>一个Demo</p>
<ul>
<li><p>下载解压数据集</p>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz<br>!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">我们可以用Linux的解压命令 gzip</span><br>!gzip -dkv SQuAD_it-*.json.gz<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>我们可以看到压缩文件已经被替换为SQuAD_it-train.json和SQuAD_it-test.json,并且数据以 JSON 格式存储。使用<code>load_dataset()</code>函数来加载JSON文件, 我们只需要知道我们是在处理普通的 JSON(类似于嵌套字典)还是 JSON 行(行分隔的 JSON)。像许多问答数据集一样, SQuAD-it 使用嵌套格式,所有文本都存储在 <code>data</code>文件中。这意味着我们可以通过指定参数<code>field</code>来加载数据集,如下所示:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br><br>squad_it_dataset = load_dataset(<span class="hljs-string">"json"</span>, data_files=<span class="hljs-string">"SQuAD_it-train.json"</span>, field=<span class="hljs-string">"data"</span>)<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>默认情况下, 加载本地文件会创建一个带有<code>train</code>的<code>DatasetDict</code> 对象。 我们可以通过 <code>squad_it_dataset</code>查看:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">squad_it_dataset<br></code></pre></td></tr></tbody></table></figure>

<figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs json">DatasetDict(<span class="hljs-punctuation">{</span><br>    train<span class="hljs-punctuation">:</span> Dataset(<span class="hljs-punctuation">{</span><br>        features<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>'title'<span class="hljs-punctuation">,</span> 'paragraphs'<span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>        num_rows<span class="hljs-punctuation">:</span> <span class="hljs-number">442</span><br>    <span class="hljs-punctuation">}</span>)<br><span class="hljs-punctuation">}</span>)<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>我们可以通过索引到 <code>train</code> 查看示例</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">squad_it_dataset[<span class="hljs-string">"train"</span>][<span class="hljs-number">0</span>]<br></code></pre></td></tr></tbody></table></figure>

<figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">{</span><br>    <span class="hljs-attr">"title"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"Terremoto del Sichuan del 2008"</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">"paragraphs"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">{</span><br>            <span class="hljs-attr">"context"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"Il terremoto del Sichuan del 2008 o il terremoto..."</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">"qas"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>                <span class="hljs-punctuation">{</span><br>                    <span class="hljs-attr">"answers"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">{</span><span class="hljs-attr">"answer_start"</span><span class="hljs-punctuation">:</span> <span class="hljs-number">29</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">"text"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"2008"</span><span class="hljs-punctuation">}</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>                    <span class="hljs-attr">"id"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"56cdca7862d2951400fa6826"</span><span class="hljs-punctuation">,</span><br>                    <span class="hljs-attr">"question"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"In quale anno si è verificato il terremoto nel Sichuan?"</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>                ...<br>            <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-punctuation">}</span><span class="hljs-punctuation">,</span><br>        ...<br>    <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">}</span><br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>我们真正想要的是包括 <code>train</code> 和 <code>test</code> 的 <code>DatasetDict</code> 对象。这样的话就可以使用 <code>Dataset.map()</code> 函数同时处理训练集和测试集。 为此, 我们提供参数<code>data_files</code>的字典,将每个分割名称映射到与该分割相关联的文件：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">data_files = {<span class="hljs-string">"train"</span>: <span class="hljs-string">"SQuAD_it-train.json"</span>, <span class="hljs-string">"test"</span>: <span class="hljs-string">"SQuAD_it-test.json"</span>}<br>squad_it_dataset = load_dataset(<span class="hljs-string">"json"</span>, data_files=data_files, field=<span class="hljs-string">"data"</span>)<br>squad_it_dataset<br></code></pre></td></tr></tbody></table></figure>

<figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs json">DatasetDict(<span class="hljs-punctuation">{</span><br>    train<span class="hljs-punctuation">:</span> Dataset(<span class="hljs-punctuation">{</span><br>        features<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>'title'<span class="hljs-punctuation">,</span> 'paragraphs'<span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>        num_rows<span class="hljs-punctuation">:</span> <span class="hljs-number">442</span><br>    <span class="hljs-punctuation">}</span>)<br>    test<span class="hljs-punctuation">:</span> Dataset(<span class="hljs-punctuation">{</span><br>        features<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>'title'<span class="hljs-punctuation">,</span> 'paragraphs'<span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>        num_rows<span class="hljs-punctuation">:</span> <span class="hljs-number">48</span><br>    <span class="hljs-punctuation">}</span>)<br><span class="hljs-punctuation">}</span>)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p><code>load_dataset()</code>函数的<code>data_files</code>参数非常灵活并且可以是单个文件路径、文件路径列表或将分割后的名称映射到文件路径的字典。您还可以根据Unix shell使用的规则对与指定模式匹配的文件进行全局定位（例如，您可以通过设置’data_files=“*.JSON”‘将目录中的所有JSON文件作为单个拆分进行全局定位）。有关更多详细信息，请参阅🤗Datasets 文档。</p>
</blockquote>
</li>
</ul>
</li>
<li><p>Datasets实际上支持输入文件的自动解压,所以我们可以跳过使用<code>gzip</code>,直接设置 <code>data_files</code>参数传递压缩文件:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">data_files = {<span class="hljs-string">"train"</span>: <span class="hljs-string">"SQuAD_it-train.json.gz"</span>, <span class="hljs-string">"test"</span>: <span class="hljs-string">"SQuAD_it-test.json.gz"</span>}<br>squad_it_dataset = load_dataset(<span class="hljs-string">"json"</span>, data_files=data_files, field=<span class="hljs-string">"data"</span>)<br></code></pre></td></tr></tbody></table></figure>

<p>如果您不想手动解压缩许多 GZIP 文件，这会很有用。自动解压也适用于其他常见格式,如 ZIP 和 TAR,因此您只需将 <code>data_files</code> 设置为压缩文件所在的路径,你就可以开始了!</p>
</li>
</ul>
<h3 id="Remote-dataset"><a href="#Remote-dataset" class="headerlink" title="Remote dataset"></a>Remote dataset</h3><p>我们没有提供本地文件的路径, 而是将<code>load_dataset()</code>的<code>data_files</code>参数指向存储远程文件的一个或多个URL。例如, 对于托管在 GitHub 上的 SQuAD-it 数据集, 我们可以将 <code>data_files</code> 指向 <em>SQuAD_it-*.json.gz</em> 的网址,如下所示:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">url = <span class="hljs-string">"https://github.com/crux82/squad-it/raw/master/"</span><br>data_files = {<br>    <span class="hljs-string">"train"</span>: url + <span class="hljs-string">"SQuAD_it-train.json.gz"</span>,<br>    <span class="hljs-string">"test"</span>: url + <span class="hljs-string">"SQuAD_it-test.json.gz"</span>,<br>}<br>squad_it_dataset = load_dataset(<span class="hljs-string">"json"</span>, data_files=data_files, field=<span class="hljs-string">"data"</span>)<br></code></pre></td></tr></tbody></table></figure>

<p>这将返回和上面的本地例子相同的 <code>DatasetDict</code> 对象, 但省去了我们手动下载和解压 <em>SQuAD_it-*.json.gz</em> 文件的步骤。</p>
<h2 id="Slice-and-dice"><a href="#Slice-and-dice" class="headerlink" title="Slice and dice"></a>Slice and dice</h2><h3 id="Slicing-and-dicing-our-data"><a href="#Slicing-and-dicing-our-data" class="headerlink" title="Slicing and dicing our data"></a>Slicing and dicing our data</h3><ul>
<li>Datasets 提供了几个函数来操作 <strong>Dataset</strong> 和 <strong>DatasetDict</strong> 对象。</li>
</ul>
<blockquote>
<p>这一节的数据Demo: 我们将使用托管在<a target="_blank" rel="noopener" href="https://archive.ics.uci.edu/ml/index.php">加州大学欧文分校机器学习存储库</a>的<a target="_blank" rel="noopener" href="https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+(Drugs.com)">药物审查数据集</a>，其中包含患者对各种药物的评论，以及正在治疗的病情和患者满意度的 10 星评级。</p>
</blockquote>
<ul>
<li>加载数据</li>
</ul>
<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">!wget "https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip"<br>!unzip drugsCom_raw.zip<br></code></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br><br>data_files = {<span class="hljs-string">"train"</span>: <span class="hljs-string">"drugsComTrain_raw.tsv"</span>, <span class="hljs-string">"test"</span>: <span class="hljs-string">"drugsComTest_raw.tsv"</span>}<br><span class="hljs-comment"># \t is the tab character in Python</span><br>drug_dataset = load_dataset(<span class="hljs-string">"csv"</span>, data_files=data_files, delimiter=<span class="hljs-string">"\t"</span>)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>抽取一个小的随机样本，以快速了解您正在处理的数据类型(shuffle + select)</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">drug_sample = drug_dataset[<span class="hljs-string">"train"</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))<br><span class="hljs-comment"># Peek at the first few examples</span><br>drug_sample[:<span class="hljs-number">3</span>]<br></code></pre></td></tr></tbody></table></figure>

<figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">{</span>'Unnamed<span class="hljs-punctuation">:</span> <span class="hljs-number">0</span>'<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-number">87571</span><span class="hljs-punctuation">,</span> <span class="hljs-number">178045</span><span class="hljs-punctuation">,</span> <span class="hljs-number">80482</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br> 'drugName'<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>'Naproxen'<span class="hljs-punctuation">,</span> 'Duloxetine'<span class="hljs-punctuation">,</span> 'Mobic'<span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br> 'condition'<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>'Gout<span class="hljs-punctuation">,</span> Acute'<span class="hljs-punctuation">,</span> 'ibromyalgia'<span class="hljs-punctuation">,</span> 'Inflammatory Conditions'<span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br> 'review'<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>'<span class="hljs-string">"like the previous person mention, I&amp;#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!"</span>'<span class="hljs-punctuation">,</span><br>  '<span class="hljs-string">"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\r\nas a pain reducer and an anti-depressant, however, the side effects outweighed \r\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\r\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\r\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\r\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects."</span>'<span class="hljs-punctuation">,</span><br>  '<span class="hljs-string">"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days."</span>'<span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br> 'rating'<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-number">9.0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">3.0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">10.0</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br> 'date'<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>'September <span class="hljs-number">2</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2015</span>'<span class="hljs-punctuation">,</span> 'November <span class="hljs-number">7</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2011</span>'<span class="hljs-punctuation">,</span> 'June <span class="hljs-number">5</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2013</span>'<span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br> 'usefulCount'<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-number">36</span><span class="hljs-punctuation">,</span> <span class="hljs-number">13</span><span class="hljs-punctuation">,</span> <span class="hljs-number">128</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">}</span><br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<ul>
<li>Data特点：<ul>
<li><strong>Unnamed: 0</strong>这列看起来很像每个患者的匿名 ID。</li>
<li><strong>condition</strong> 这列包含有描述健康状况的标签。</li>
<li>评论长短不一，混合有 Python 行分隔符 (<strong>\r\n</strong>) 以及 HTML 字符代码，如**’**。</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li>为了验证<strong>Unnamed: 0</strong> 列存储的是患者 ID的猜想，我们可以使用 <strong>Dataset.unique()</strong> 函数来验证匿名ID 的数量是否与拆分后每部分中的行数匹配：</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> drug_dataset.keys():<br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(drug_dataset[split]) == <span class="hljs-built_in">len</span>(drug_dataset[split].unique(<span class="hljs-string">"Unnamed: 0"</span>))<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>我们可以使用 **DatasetDict.rename_column()**函数一次性重命名DatasetDict中共有的列：</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">drug_dataset = drug_dataset.rename_column(<br>    original_column_name=<span class="hljs-string">"Unnamed: 0"</span>, new_column_name=<span class="hljs-string">"patient_id"</span><br>)<br>drug_dataset<br></code></pre></td></tr></tbody></table></figure>

<figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs json">DatasetDict(<span class="hljs-punctuation">{</span><br>    train<span class="hljs-punctuation">:</span> Dataset(<span class="hljs-punctuation">{</span><br>        features<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>'patient_id'<span class="hljs-punctuation">,</span> 'drugName'<span class="hljs-punctuation">,</span> 'condition'<span class="hljs-punctuation">,</span> 'review'<span class="hljs-punctuation">,</span> 'rating'<span class="hljs-punctuation">,</span> 'date'<span class="hljs-punctuation">,</span> 'usefulCount'<span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>        num_rows<span class="hljs-punctuation">:</span> <span class="hljs-number">161297</span><br>    <span class="hljs-punctuation">}</span>)<br>    test<span class="hljs-punctuation">:</span> Dataset(<span class="hljs-punctuation">{</span><br>        features<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>'patient_id'<span class="hljs-punctuation">,</span> 'drugName'<span class="hljs-punctuation">,</span> 'condition'<span class="hljs-punctuation">,</span> 'review'<span class="hljs-punctuation">,</span> 'rating'<span class="hljs-punctuation">,</span> 'date'<span class="hljs-punctuation">,</span> 'usefulCount'<span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>        num_rows<span class="hljs-punctuation">:</span> <span class="hljs-number">53766</span><br>    <span class="hljs-punctuation">}</span>)<br><span class="hljs-punctuation">}</span>)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>使用 **Dataset.map()**标准化所有 <strong>condition</strong> 标签</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">lowercase_condition</span>(<span class="hljs-params">example</span>):<br>    <span class="hljs-keyword">return</span> {<span class="hljs-string">"condition"</span>: example[<span class="hljs-string">"condition"</span>].lower()}<br>  <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">filter_nones</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> x[<span class="hljs-string">"condition"</span>] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br>  <br>drug_dataset = drug_dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-string">"condition"</span>] <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>)<br>drug_dataset = drug_dataset.<span class="hljs-built_in">map</span>(lowercase_condition)<br><span class="hljs-comment"># Check that lowercasing worked</span><br>drug_dataset[<span class="hljs-string">"train"</span>][<span class="hljs-string">"condition"</span>][:<span class="hljs-number">3</span>]<br></code></pre></td></tr></tbody></table></figure>



<h3 id="Creating-new-columns"><a href="#Creating-new-columns" class="headerlink" title="Creating new columns"></a>Creating new columns</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_review_length</span>(<span class="hljs-params">example</span>):<br>    <span class="hljs-keyword">return</span> {<span class="hljs-string">"review_length"</span>: <span class="hljs-built_in">len</span>(example[<span class="hljs-string">"review"</span>].split())}<br><br>drug_dataset = drug_dataset.<span class="hljs-built_in">map</span>(compute_review_length)<br><span class="hljs-comment"># Inspect the first training example</span><br>drug_dataset[<span class="hljs-string">"train"</span>][<span class="hljs-number">0</span>]<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>然后进行排序</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">drug_dataset[<span class="hljs-string">"train"</span>].sort(<span class="hljs-string">"review_length"</span>)[:<span class="hljs-number">3</span>]<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>向数据集添加新列的另一种方法是使用函数Dataset.add_column() 。这允许您输入Python 列表或 NumPy，在不适合使用Dataset.map()情况下可以很方便。</p>
</blockquote>
<ul>
<li>一些评论只包含一个词，虽然这对于情感分析来说可能没问题，但如果我们想要预测病情，这些评论可能并不适合。我们使用 <strong>Dataset.filter()</strong> 功能来删除包含少于 30 个单词的评论。与我们对 <strong>condition</strong> 列的处理相似，我们可以通过选取评论的长度高于此阈值来过滤掉非常短的评论：</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">drug_dataset = drug_dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-string">"review_length"</span>] &gt; <span class="hljs-number">30</span>)<br><span class="hljs-built_in">print</span>(drug_dataset.num_rows)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>这已经从我们的原始训练和测试集中删除了大约 15% 的评论。</p>
</blockquote>
<ul>
<li>评论中存在 HTML 字符代码，我们可以使用 Python 的<strong>html</strong>模块取消这些字符的转义</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> html<br><br>text = <span class="hljs-string">"I&amp;#039;m a transformer called BERT"</span><br>html.unescape(text)<br><br>drug_dataset = drug_dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: {<span class="hljs-string">"review"</span>: html.unescape(x[<span class="hljs-string">"review"</span>])})<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p> <strong>Dataset.map()</strong> 方法对于处理数据非常有用</p>
</blockquote>
<h3 id="Map-method’s-superpowers"><a href="#Map-method’s-superpowers" class="headerlink" title="Map() method’s superpowers"></a>Map() method’s superpowers</h3><ul>
<li>Dataset.map() 方法有一个 batched 参数，如果设置为 True , map 函数将会分批执行所需要进行的操作（批量大小是可配置的，但默认为 1,000）。 当您在使用 Dataset.map()函数时指定 batched=True。该函数会接收一个包含数据集字段的字典，每个值都是一个列表，而不仅仅是单个值。Dataset.map() 的返回值应该是相同的：一个包含我们想要更新或添加到数据集中的字段的字典，字典的键是要添加的字段，字典的值是结果的列表。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">new_drug_dataset = drug_dataset.<span class="hljs-built_in">map</span>(<br>    <span class="hljs-keyword">lambda</span> x: {<span class="hljs-string">"review"</span>: [html.unescape(o) <span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> x[<span class="hljs-string">"review"</span>]]}, batched=<span class="hljs-literal">True</span><br>)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>列表推导式通常比在同一代码中用 <strong>for</strong> 循环执行相同的代码更快，并且我们还通过同时访问多个元素而不是一个一个来处理来提高处理的速度。使用 <strong>Dataset.map()</strong> 和 <strong>batched=True</strong> 是加速的关键。</p>
</blockquote>
<ul>
<li>“快速”标记器</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer<br><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"bert-base-cased"</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">examples</span>):<br>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">"review"</span>], truncation=<span class="hljs-literal">True</span>)<br><br>tokenized_dataset = drug_dataset.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li><strong>Dataset.map()</strong> 也有一些自己的并行化能力。由于它们不受 Rust 的支持，因此慢速分词器的速度赶不上快速分词器，但它们仍然会更快一些（尤其是当您使用没有快速版本的分词器时）。要启用多处理，请在**Dataset.map()**时使用 <strong>num_proc</strong> 参数并指定要在调用中使用的进程数 ：</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">slow_tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"bert-base-cased"</span>, use_fast=<span class="hljs-literal">False</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">slow_tokenize_function</span>(<span class="hljs-params">examples</span>):<br>    <span class="hljs-keyword">return</span> slow_tokenizer(examples[<span class="hljs-string">"review"</span>], truncation=<span class="hljs-literal">True</span>)<br><br>tokenized_dataset = drug_dataset.<span class="hljs-built_in">map</span>(slow_tokenize_function, batched=<span class="hljs-literal">True</span>, num_proc=<span class="hljs-number">8</span>)<br></code></pre></td></tr></tbody></table></figure>



<h1 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h1><ul>
<li>其他内容其实已经不是重点了（包括上面的Datasets，最主要的部分，是前面几节，由不同的层拼出来具体的处理逻辑和pipeline）。</li>
</ul>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/zh-CN/chapter1/1">HuggingFace NLP Course Reference</a></li>
</ol>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/AI/" class="category-chain-item">AI</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E8%87%AA%E5%AD%A6/">#自学</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>HuggingFace-NLP-Course</div>
      <div>https://alexanderliu-creator.github.io/2024/08/03/huggingface-nlp-course/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Alexander Liu</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年8月3日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/08/16/recommendation-system/" title="Recommendation System">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Recommendation System</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/02/21/suan-fa-gang-mian-shi-hui-zong-1/" title="算法岗面试汇总-1">
                        <span class="hidden-mobile">算法岗面试汇总-1</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.events.registerRefreshCallback(function() {
      if ('mermaid' in window) {
        mermaid.init();
      }
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
