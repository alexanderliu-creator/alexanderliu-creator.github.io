

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/tuzi.png">
  <link rel="icon" href="/img/tuzi.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Alexander Liu">
  <meta name="keywords" content="分布式系统,后端研发,数据协同">
  
    <meta name="description" content="针对于有顺序的数据类型，我们需要设计特定模型。我们不仅仅可以接收一个序列作为输入，而是还可能期望继续猜测这个序列的后续。如果说卷积神经网络可以有效地处理空间信息， 那么本章的循环神经网络（recurrent neural network，RNN）则可以更好地处理序列信息。 循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。 循环神经网络 (RNN) 是深度学习模型，通">
<meta property="og:type" content="article">
<meta property="og:title" content="D2L-9-Recurrent Neural Networks">
<meta property="og:url" content="https://alexanderliu-creator.github.io/2023/08/07/d2l-9-recurrent-neural-networks/index.html">
<meta property="og:site_name" content="兔の博客">
<meta property="og:description" content="针对于有顺序的数据类型，我们需要设计特定模型。我们不仅仅可以接收一个序列作为输入，而是还可能期望继续猜测这个序列的后续。如果说卷积神经网络可以有效地处理空间信息， 那么本章的循环神经网络（recurrent neural network，RNN）则可以更好地处理序列信息。 循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。 循环神经网络 (RNN) 是深度学习模型，通">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202307231956594.jpg">
<meta property="article:published_time" content="2023-08-07T11:47:21.000Z">
<meta property="article:modified_time" content="2023-08-14T11:43:40.928Z">
<meta property="article:author" content="Alexander Liu">
<meta property="article:tag" content="研0自学">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202307231956594.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>D2L-9-Recurrent Neural Networks - 兔の博客</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"alexanderliu-creator.github.io","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":1},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="兔の博客" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>兔的博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/background_post.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="D2L-9-Recurrent Neural Networks"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Alexander Liu
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-08-07 19:47" pubdate>
          2023年8月7日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          25k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          206 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">D2L-9-Recurrent Neural Networks</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：1 年前
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <p>针对于有顺序的数据类型，我们需要设计特定模型。我们不仅仅可以接收一个序列作为输入，而是还可能期望继续猜测这个序列的后续。如果说卷积神经网络可以有效地处理空间信息， 那么本章的<em>循环神经网络</em>（recurrent neural network，RNN）则可以更好地处理序列信息。 循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。</p>
<p><em>循环神经网络 (RNN) 是深度学习模型，通过循环</em>连接捕获序列的动态，循环连接可以被视为节点网络中的循环。乍一看这似乎违反直觉。毕竟，正是神经网络的前馈性质使得计算顺序变得明确。然而，循环边缘以精确的方式定义，以确保不会出现此类歧义。循环神经网络 跨时间步长（或序列步长）<em>展开</em>，每个步骤应用<em>相同的基础参数。**标准连接同步</em>应用以<em>在同一时间步长</em>将每一层的激活传播到后续层，循环连接是 <em>动态的</em>，跨相邻时间步传递信息。</p>
<span id="more"></span>





<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308071957945.svg" srcset="/img/loading.gif" lazyload alt="../_images/unfolded-rnn.svg"></p>
<h1 id="序列模型"><a href="#序列模型" class="headerlink" title="序列模型"></a>序列模型</h1><ul>
<li>数据呈现时间性，季节性等，会随着时间而改变！</li>
<li>预测明天的股价要比过去的股价更困难，尽管两者都只是估计一个数字。 毕竟，先见之明比事后诸葛亮难得多。 在统计学中，前者（对超出已知观测范围进行预测）称为<em>外推法</em>（extrapolation）， 而后者（在现有观测值之间进行估计）称为<em>内插法</em>（interpolation）。前者比后者困难的多得多！！！</li>
</ul>
<h2 id="统计工具"><a href="#统计工具" class="headerlink" title="统计工具"></a>统计工具</h2><p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308072001662.png" srcset="/img/loading.gif" lazyload alt="../_images/ftse100.png"></p>
<ul>
<li>用$x_t$表示价格，即在<em>时间步</em>（time step）$t \in \mathbb{Z}^+$时，观察到的价格$x_t$。 请注意，$t$对于本文中的序列通常是离散的，并在整数或其子集上变化。 假设一个交易员想在$t$日的股市中表现良好，于是通过以下途径预测$x_t$：</li>
</ul>
<p>$$<br>x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1).<br>$$</p>
<h3 id="自回归模型"><a href="#自回归模型" class="headerlink" title="自回归模型"></a>自回归模型</h3><ul>
<li><p>输入数据的数量， 输入$x_{t-1}, \ldots, x_1$本身因$t$而异。 也就是说，输入数据的数量这个数字将会随着我们遇到的数据量的增加而增加， 因此需要一个近似方法来使这个计算变得容易处理。 本章后面的大部分内容将围绕着如何有效估计$P(x_t \mid x_{t-1}, \ldots, x_1)$展开。 简单地说，它归结为以下两种策略。</p>
<ul>
<li><p>第一种策略，假设在现实情况下相当长的序列$x_{t-1}, \ldots, x_1$可能是不必要的， 因此我们只需要满足某个长度为$\tau$的时间跨度， 即使用观测序列$x_{t-1}, \ldots, x_{t-\tau}$。 当下获得的最直接的好处就是参数的数量总是不变的， 至少在$t &gt; \tau$时如此，这就使我们能够训练一个上面提及的深度网络。 这种模型被称为<em>自回归模型</em>（autoregressive models）， 因为它们是对自己执行回归。</p>
</li>
<li><p>第二种策略，保留一些对过去观测的总结$h_t$， 并且同时更新预测$\hat{x}<em>t$和总结$h_t$。 这就产生了基于$\hat{x}<em>t = P(x_t \mid h</em>{t})$估计$x_t$， 以及公式$h_t = g(h</em>{t-1}, x_{t-1})$更新的模型。 由于$h_t$从未被观测到，这类模型也被称为 <em>隐变量自回归模型</em>（latent autoregressive models）。</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308072011626.svg" srcset="/img/loading.gif" lazyload alt="../_images/sequence-model.svg"></p>
</li>
</ul>
</li>
<li><p>如何生成训练数据？ 一个经典方法是使用历史观测来预测下一个未来观测。 显然，我们并不指望时间会停滞不前。 然而，一个常见的假设是虽然特定值$x_t$可能会改变， 但是序列本身的动力学不会改变。 这样的假设是合理的，因为新的动力学一定受新的数据影响， 而我们不可能用目前所掌握的数据来预测新的动力学。 统计学家称不变的动力学为<em>静止的</em>（stationary）。 因此，整个序列的估计值都将通过以下的方式获得：</p>
</li>
</ul>
<p>$$<br>P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}, \ldots, x_1).<br>$$</p>
<blockquote>
<p>注意，如果我们处理的是离散的对象（如单词）， 而不是连续的数字，则上述的考虑仍然有效。 唯一的差别是，对于离散的对象， 我们需要使用分类器而不是回归模型来估计$P(x_t \mid x_{t-1}, \ldots, x_1)$。</p>
</blockquote>
<h3 id="马尔可夫模型"><a href="#马尔可夫模型" class="headerlink" title="马尔可夫模型"></a>马尔可夫模型</h3><ul>
<li>在自回归模型的近似法中， 我们使用$x_{t-1}, \ldots, x_{t-\tau}$而不是$x_{t-1}, \ldots, x_1$来估计$x_t$。 只要这种是近似精确的，我们就说序列满足<em>马尔可夫条件</em>（Markov condition）。 特别是，如果$\tau = 1$，得到一个 <em>一阶马尔可夫模型</em>（first-order Markov model）， $P(x)$由下式给出：</li>
</ul>
<p>$$<br>P(x_1, \ldots, x_T) = \prod_{t=1}^T P(x_t \mid x_{t-1}) \text{ 当 } P(x_1 \mid x_0) = P(x_1).<br>$$</p>
<ul>
<li>假设$x_t$仅是离散值时，这样的模型特别棒， 因为在这种情况下，使用动态规划可以沿着马尔可夫链精确地计算结果。 例如，我们可以高效地计算$P(x_{t+1} \mid x_{t-1})$：</li>
</ul>
<p>$$<br>\begin{split}\begin{aligned}<br>P(x_{t+1} \mid x_{t-1})<br>&amp;= \frac{\sum_{x_t} P(x_{t+1}, x_t, x_{t-1})}{P(x_{t-1})}\<br>&amp;= \frac{\sum_{x_t} P(x_{t+1} \mid x_t, x_{t-1}) P(x_t, x_{t-1})}{P(x_{t-1})}\<br>&amp;= \sum_{x_t} P(x_{t+1} \mid x_t) P(x_t \mid x_{t-1})<br>\end{aligned}\end{split}<br>$$</p>
<blockquote>
<p>离散的情况下，只需要对可能出现的情况进行罗列，对每种出现情况的概率进行求和罢了。参考最后的马尔可夫链昂！</p>
</blockquote>
<h3 id="因果关系"><a href="#因果关系" class="headerlink" title="因果关系"></a>因果关系</h3><ul>
<li>$P(x_1, \ldots, x_T)$基于条件概率公式，总是可以写出：</li>
</ul>
<p>$$<br>P(x_1, \ldots, x_T) = \prod_{t=T}^1 P(x_t \mid x_{t+1}, \ldots, x_T).<br>$$</p>
<blockquote>
<p>如果基于一个马尔可夫模型， 我们还可以得到一个反向的条件概率分布。 然而，在许多情况下，数据存在一个自然的方向，即在时间上是前进的。 很明显，未来的事件不能影响过去。 </p>
</blockquote>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><ul>
<li>首先，我们生成一些数据：使用正弦函数和一些可加性噪声来生成序列数据。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">%matplotlib inline<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>T = <span class="hljs-number">1000</span>  <span class="hljs-comment"># 总共产生1000个点</span><br>time = torch.arange(<span class="hljs-number">1</span>, T + <span class="hljs-number">1</span>, dtype=torch.float32)<br>x = torch.sin(<span class="hljs-number">0.01</span> * time) + torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">0.2</span>, (T,))<br>d2l.plot(time, [x], <span class="hljs-string">'time'</span>, <span class="hljs-string">'x'</span>, xlim=[<span class="hljs-number">1</span>, <span class="hljs-number">1000</span>], figsize=(<span class="hljs-number">6</span>, <span class="hljs-number">3</span>))<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>我们使用一个相当简单的架构训练模型： 一个拥有两个全连接层的多层感知机，ReLU激活函数和平方损失。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 初始化网络权重的函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_weights</span>(<span class="hljs-params">m</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) == nn.Linear:<br>        nn.init.xavier_uniform_(m.weight)<br><br><span class="hljs-comment"># 一个简单的多层感知机</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_net</span>():<br>    net = nn.Sequential(nn.Linear(<span class="hljs-number">4</span>, <span class="hljs-number">10</span>),<br>                        nn.ReLU(),<br>                        nn.Linear(<span class="hljs-number">10</span>, <span class="hljs-number">1</span>))<br>    net.apply(init_weights)<br>    <span class="hljs-keyword">return</span> net<br><br><span class="hljs-comment"># 平方损失。注意：MSELoss计算平方误差时不带系数1/2</span><br>loss = nn.MSELoss(reduction=<span class="hljs-string">'none'</span>)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>训练：</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">net, train_iter, loss, epochs, lr</span>):<br>    trainer = torch.optim.Adam(net.parameters(), lr)<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> train_iter:<br>            trainer.zero_grad()<br>            l = loss(net(X), y)<br>            l.<span class="hljs-built_in">sum</span>().backward()<br>            trainer.step()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f'epoch <span class="hljs-subst">{epoch + <span class="hljs-number">1</span>}</span>, '</span><br>              <span class="hljs-string">f'loss: <span class="hljs-subst">{d2l.evaluate_loss(net, train_iter, loss):f}</span>'</span>)<br><br>net = get_net()<br>train(net, train_iter, loss, <span class="hljs-number">5</span>, <span class="hljs-number">0.01</span>)<br><br><span class="hljs-comment"># result</span><br>epoch <span class="hljs-number">1</span>, loss: <span class="hljs-number">0.063133</span><br>epoch <span class="hljs-number">2</span>, loss: <span class="hljs-number">0.053832</span><br>epoch <span class="hljs-number">3</span>, loss: <span class="hljs-number">0.051174</span><br>epoch <span class="hljs-number">4</span>, loss: <span class="hljs-number">0.050547</span><br>epoch <span class="hljs-number">5</span>, loss: <span class="hljs-number">0.047369</span><br></code></pre></td></tr></tbody></table></figure>



<h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><ul>
<li>由于训练损失很小，因此我们期望模型能有很好的工作效果。 让我们看看这在实践中意味着什么。 首先是检查模型预测下一个时间步的能力， 也就是<em>单步预测</em>（one-step-ahead prediction）。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">onestep_preds = net(features)<br>d2l.plot([time, time[tau:]],<br>         [x.detach().numpy(), onestep_preds.detach().numpy()], <span class="hljs-string">'time'</span>,<br>         <span class="hljs-string">'x'</span>, legend=[<span class="hljs-string">'data'</span>, <span class="hljs-string">'1-step preds'</span>], xlim=[<span class="hljs-number">1</span>, <span class="hljs-number">1000</span>],<br>         figsize=(<span class="hljs-number">6</span>, <span class="hljs-number">3</span>))<br></code></pre></td></tr></tbody></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308072040653.svg" srcset="/img/loading.gif" lazyload alt="../_images/output_sequence_ce248f_66_0.svg"></p>
<blockquote>
<p>单步预测效果不错。 即使这些预测的时间步超过了600+4（n_train + tau）， 其结果看起来仍然是可信的。 </p>
</blockquote>
<ul>
<li>对于直到$x_t$的观测序列，其在时间步$t+k$处的预测输出$\hat{x}_{t+k}$ 称为$k$步预测（k-step-ahead-prediction）。换句话说，我们必须使用我们自己的预测（而不是原始数据）来进行多步预测。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">multistep_preds = torch.zeros(T)<br>multistep_preds[: n_train + tau] = x[: n_train + tau]<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_train + tau, T):<br>    multistep_preds[i] = net(<br>        multistep_preds[i - tau:i].reshape((<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)))<br><br>d2l.plot([time, time[tau:], time[n_train + tau:]],<br>         [x.detach().numpy(), onestep_preds.detach().numpy(),<br>          multistep_preds[n_train + tau:].detach().numpy()], <span class="hljs-string">'time'</span>,<br>         <span class="hljs-string">'x'</span>, legend=[<span class="hljs-string">'data'</span>, <span class="hljs-string">'1-step preds'</span>, <span class="hljs-string">'multistep preds'</span>],<br>         xlim=[<span class="hljs-number">1</span>, <span class="hljs-number">1000</span>], figsize=(<span class="hljs-number">6</span>, <span class="hljs-number">3</span>))<br></code></pre></td></tr></tbody></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308072044949.svg" srcset="/img/loading.gif" lazyload alt="../_images/output_sequence_ce248f_81_0.svg"></p>
<blockquote>
<p>绿线的预测显然并不理想。经过几个预测步骤之后，预测的结果很快就会衰减到一个常数。 事实是由于错误的累积，预测误差会接着影响下一次的预测，依此类推。 误差可能会相当快地偏离真实的观测结果。</p>
</blockquote>
<ul>
<li>基于$k = 1, 4, 16, 64$，通过对整个序列预测的计算， 让我们更仔细地看一下$k$步预测的困难。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">max_steps = <span class="hljs-number">64</span><br><br>features = torch.zeros((T - tau - max_steps + <span class="hljs-number">1</span>, tau + max_steps))<br><span class="hljs-comment"># 列i（i&lt;tau）是来自x的观测，其时间步从（i）到（i+T-tau-max_steps+1）</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tau):<br>    features[:, i] = x[i: i + T - tau - max_steps + <span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># 列i（i&gt;=tau）是来自（i-tau+1）步的预测，其时间步从（i）到（i+T-tau-max_steps+1）</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(tau, tau + max_steps):<br>    features[:, i] = net(features[:, i - tau:i]).reshape(-<span class="hljs-number">1</span>)<br><br>steps = (<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">16</span>, <span class="hljs-number">64</span>)<br>d2l.plot([time[tau + i - <span class="hljs-number">1</span>: T - max_steps + i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> steps],<br>         [features[:, tau + i - <span class="hljs-number">1</span>].detach().numpy() <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> steps], <span class="hljs-string">'time'</span>, <span class="hljs-string">'x'</span>,<br>         legend=[<span class="hljs-string">f'<span class="hljs-subst">{i}</span>-step preds'</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> steps], xlim=[<span class="hljs-number">5</span>, <span class="hljs-number">1000</span>],<br>         figsize=(<span class="hljs-number">6</span>, <span class="hljs-number">3</span>))<br></code></pre></td></tr></tbody></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308072047297.svg" srcset="/img/loading.gif" lazyload alt="../_images/output_sequence_ce248f_96_0.svg"></p>
<blockquote>
<p>以上例子清楚地说明了当我们试图预测更远的未来时，预测的质量是如何变化的。 虽然“4步预测”看起来仍然不错，但超过这个跨度的任何预测几乎都是无用的。</p>
</blockquote>
<h1 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h1><ul>
<li>一篇文章可以被简单地看作一串单词序列，甚至是一串字符序列。</li>
</ul>
<h2 id="读取数据集"><a href="#读取数据集" class="headerlink" title="读取数据集"></a>读取数据集</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br>d2l.DATA_HUB[<span class="hljs-string">'time_machine'</span>] = (d2l.DATA_URL + <span class="hljs-string">'timemachine.txt'</span>,<br>                                <span class="hljs-string">'090b5e7e70c295757f55df93cb0a180b9691891a'</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_time_machine</span>():  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">"""将时间机器数据集加载到文本行的列表中"""</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(d2l.download(<span class="hljs-string">'time_machine'</span>), <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:<br>        lines = f.readlines()<br>    <span class="hljs-keyword">return</span> [re.sub(<span class="hljs-string">'[^A-Za-z]+'</span>, <span class="hljs-string">' '</span>, line).strip().lower() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines]<br><br>lines = read_time_machine()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f'# 文本总行数: <span class="hljs-subst">{<span class="hljs-built_in">len</span>(lines)}</span>'</span>)<br><span class="hljs-built_in">print</span>(lines[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(lines[<span class="hljs-number">10</span>])<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>下面的函数将数据集读取到由多条文本行组成的列表中，其中每条文本行都是一个字符串。 为简单起见，我们在这里忽略了标点符号和字母大写。</p>
</blockquote>
<h2 id="词元化"><a href="#词元化" class="headerlink" title="词元化"></a>词元化</h2><ul>
<li>下面的<code>tokenize</code>函数将文本行列表（<code>lines</code>）作为输入， 列表中的每个元素是一个文本序列（如一条文本行）。 每个文本序列又被拆分成一个词元列表，<em>词元</em>（token）是文本的基本单位。 最后，返回一个由词元列表组成的列表，其中的每个词元都是一个字符串（string）。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">lines, token=<span class="hljs-string">'word'</span></span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">"""将文本行拆分为单词或字符词元"""</span><br>    <span class="hljs-keyword">if</span> token == <span class="hljs-string">'word'</span>:<br>        <span class="hljs-keyword">return</span> [line.split() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines]<br>    <span class="hljs-keyword">elif</span> token == <span class="hljs-string">'char'</span>:<br>        <span class="hljs-keyword">return</span> [<span class="hljs-built_in">list</span>(line) <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines]<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">'错误：未知词元类型：'</span> + token)<br><br>tokens = tokenize(lines)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">11</span>):<br>    <span class="hljs-built_in">print</span>(tokens[i])<br></code></pre></td></tr></tbody></table></figure>





<h2 id="词表"><a href="#词表" class="headerlink" title="词表"></a>词表</h2><ul>
<li>我们构建一个字典，通常也叫做<em>词表</em>（vocabulary）， 用来将字符串类型的词元映射到从0开始的数字索引中。 我们先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计， 得到的统计结果称之为<em>语料</em>（corpus）。 然后根据每个唯一词元的出现频率，为其分配一个数字索引。 很少出现的词元通常被移除，这可以降低复杂性。 另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“<unk>”。 我们可以选择增加一个列表，用于保存那些被保留的词元， 例如：填充词元（“<pad>”）； 序列开始词元（“<bos>”）； 序列结束词元（“<eos>”）。</eos></bos></pad></unk></li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Vocab</span>:  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">"""文本词表"""</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, tokens=<span class="hljs-literal">None</span>, min_freq=<span class="hljs-number">0</span>, reserved_tokens=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-keyword">if</span> tokens <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            tokens = []<br>        <span class="hljs-keyword">if</span> reserved_tokens <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            reserved_tokens = []<br>        <span class="hljs-comment"># 按出现频率排序</span><br>        counter = count_corpus(tokens)<br>        <span class="hljs-variable language_">self</span>._token_freqs = <span class="hljs-built_in">sorted</span>(counter.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>],<br>                                   reverse=<span class="hljs-literal">True</span>)<br>        <span class="hljs-comment"># 未知词元的索引为0</span><br>        <span class="hljs-variable language_">self</span>.idx_to_token = [<span class="hljs-string">'&lt;unk&gt;'</span>] + reserved_tokens<br>        <span class="hljs-variable language_">self</span>.token_to_idx = {token: idx<br>                             <span class="hljs-keyword">for</span> idx, token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-variable language_">self</span>.idx_to_token)}<br>        <span class="hljs-keyword">for</span> token, freq <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>._token_freqs:<br>            <span class="hljs-keyword">if</span> freq &lt; min_freq:<br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">if</span> token <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.token_to_idx:<br>                <span class="hljs-variable language_">self</span>.idx_to_token.append(token)<br>                <span class="hljs-variable language_">self</span>.token_to_idx[token] = <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.idx_to_token) - <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.idx_to_token)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, tokens</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(tokens, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.token_to_idx.get(tokens, <span class="hljs-variable language_">self</span>.unk)<br>        <span class="hljs-keyword">return</span> [<span class="hljs-variable language_">self</span>.__getitem__(token) <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> tokens]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">to_tokens</span>(<span class="hljs-params">self, indices</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(indices, (<span class="hljs-built_in">list</span>, <span class="hljs-built_in">tuple</span>)):<br>            <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.idx_to_token[indices]<br>        <span class="hljs-keyword">return</span> [<span class="hljs-variable language_">self</span>.idx_to_token[index] <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> indices]<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">unk</span>(<span class="hljs-params">self</span>):  <span class="hljs-comment"># 未知词元的索引为0</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">token_freqs</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>._token_freqs<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">count_corpus</span>(<span class="hljs-params">tokens</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">"""统计词元的频率"""</span><br>    <span class="hljs-comment"># 这里的tokens是1D列表或2D列表</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(tokens) == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> <span class="hljs-built_in">isinstance</span>(tokens[<span class="hljs-number">0</span>], <span class="hljs-built_in">list</span>):<br>        <span class="hljs-comment"># 将词元列表展平成一个列表</span><br>        tokens = [token <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> line]<br>    <span class="hljs-keyword">return</span> collections.Counter(tokens)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>使用时光机器数据集作为语料库来构建词表</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">vocab = Vocab(tokens)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(vocab.token_to_idx.items())[:<span class="hljs-number">10</span>])<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>我们可以将每一条文本行转换成一个数字索引列表。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> [<span class="hljs-number">0</span>, <span class="hljs-number">10</span>]:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">'文本:'</span>, tokens[i])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">'索引:'</span>, vocab[tokens[i]])<br></code></pre></td></tr></tbody></table></figure>



<h2 id="整合所有功能"><a href="#整合所有功能" class="headerlink" title="整合所有功能"></a>整合所有功能</h2><ul>
<li>我们将所有功能打包到<code>load_corpus_time_machine</code>函数中， 该函数返回<code>corpus</code>（词元索引列表）和<code>vocab</code>（时光机器语料库的词表）。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_corpus_time_machine</span>(<span class="hljs-params">max_tokens=-<span class="hljs-number">1</span></span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">"""返回时光机器数据集的词元索引列表和词表"""</span><br>    lines = read_time_machine()<br>    tokens = tokenize(lines, <span class="hljs-string">'char'</span>)<br>    vocab = Vocab(tokens)<br>    <span class="hljs-comment"># 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，</span><br>    <span class="hljs-comment"># 所以将所有文本行展平到一个列表中</span><br>    corpus = [vocab[token] <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> line]<br>    <span class="hljs-keyword">if</span> max_tokens &gt; <span class="hljs-number">0</span>:<br>        corpus = corpus[:max_tokens]<br>    <span class="hljs-keyword">return</span> corpus, vocab<br><br>corpus, vocab = load_corpus_time_machine()<br><span class="hljs-built_in">len</span>(corpus), <span class="hljs-built_in">len</span>(vocab)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<ol>
<li>为了简化后面章节中的训练，我们使用字符（而不是单词）实现文本词元化；</li>
<li>时光机器数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词，因此返回的<code>corpus</code>仅处理为单个列表，而不是使用多词元列表构成的一个列表。</li>
</ol>
</blockquote>
<h1 id="语言模型和数据集"><a href="#语言模型和数据集" class="headerlink" title="语言模型和数据集"></a>语言模型和数据集</h1><ul>
<li>将文本数据映射为词元， 以及将这些词元可以视为一系列离散的观测，例如单词或字符。 假设长度为$T$的文本序列中的词元依次为$x_1, x_2, \ldots, x_T$。 于是，$x_t(1 \leq t \leq T)$可以被认为是文本序列在时间步$t$处的观测或标签。 在给定这样的文本序列时，<em>语言模型</em>（language model）的目标是估计序列的联合概率</li>
</ul>
<p>$$<br>P(x_1, x_2, \ldots, x_T).<br>$$</p>
<ul>
<li>例如：只需要一次抽取一个词元$x_t \sim P(x_t \mid x_{t-1}, \ldots, x_1)$，一个理想的语言模型就能够基于模型本身生成自然文本。</li>
</ul>
<h2 id="学习语言模型"><a href="#学习语言模型" class="headerlink" title="学习语言模型"></a>学习语言模型</h2><ul>
<li>基本概率规则开始：</li>
</ul>
<p>$$<br>P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T P(x_t  \mid  x_1, \ldots, x_{t-1}). \<br>Example: \ P(\text{deep}, \text{learning}, \text{is}, \text{fun}) =  P(\text{deep}) P(\text{learning}  \mid  \text{deep}) P(\text{is}  \mid  \text{deep}, \text{learning}) P(\text{fun}  \mid  \text{deep}, \text{learning}, \text{is}).<br>$$</p>
<blockquote>
<p>我们需要计算单词的概率， 以及给定前面几个单词后出现某个单词的条件概率。 这些概率本质上就是语言模型的参数。</p>
</blockquote>
<ul>
<li>一种（稍稍不太精确的）方法是统计单词“deep”在数据集中的出现次数， 然后将其除以整个语料库中的单词总数。 这种方法效果不错，特别是对于频繁出现的单词。 接下来，我们可以尝试估计</li>
</ul>
<p>$$<br>\hat{P}(\text{learning} \mid \text{deep}) = \frac{n(\text{deep, learning})}{n(\text{deep})},<br>$$</p>
<ul>
<li>$n(x)$和$n(x, x’)$分别是单个单词和连续单词对的出现次数。由于连续单词对“deep learning”的出现频率要低得多， 所以估计这类单词正确的概率要困难得多。 特别是对于一些不常见的单词组合，要想找到足够的出现次数来获得准确的估计可能都不容易。 而对于三个或者更多的单词组合，情况会变得更糟。 许多合理的三个单词组合可能是存在的，但是在数据集中却找不到。 如果数据集很小，或者单词非常罕见，那么这类单词出现一次的机会可能都找不到。</li>
<li>一种常见的策略是执行某种形式的<em>拉普拉斯平滑</em>（Laplace smoothing）， 具体方法是在所有计数中添加一个小常量。 用n表示训练集中的单词总数，用m表示唯一单词的数量。 此解决方案有助于处理单元素问题，例如通过：</li>
</ul>
<p>$$<br>\begin{split}\begin{aligned}<br>    \hat{P}(x) &amp; = \frac{n(x) + \epsilon_1/m}{n + \epsilon_1}, \<br>    \hat{P}(x’ \mid x) &amp; = \frac{n(x, x’) + \epsilon_2 \hat{P}(x’)}{n(x) + \epsilon_2}, \<br>    \hat{P}(x’’ \mid x,x’) &amp; = \frac{n(x, x’,x’’) + \epsilon_3 \hat{P}(x’’)}{n(x, x’) + \epsilon_3}.<br>\end{aligned}\end{split}<br>$$</p>
<ul>
<li>$\epsilon_1,\epsilon_2,\epsilon_3$是超参数。 当$\epsilon$为0的时候，不应用平滑。无穷大的时候，$\hat{P}(x)$接近均匀概率分布1/m。这样的模型很容易变得无效，原因如下： 首先，我们需要存储所有的计数； 其次，这完全忽略了单词的意思。 例如，“猫”（cat）和“猫科动物”（feline）可能出现在相关的上下文中， 但是想根据上下文调整这类模型其实是相当困难的。 最后，长单词序列大部分是没出现过的， 因此一个模型如果只是简单地统计先前“看到”的单词序列频率， 那么模型面对这种问题肯定是表现不佳的。</li>
</ul>
<h2 id="马尔可夫模型与n元语法"><a href="#马尔可夫模型与n元语法" class="headerlink" title="马尔可夫模型与n元语法"></a>马尔可夫模型与n元语法</h2><ul>
<li>序列上的分布满足一阶马尔可夫性质。 阶数越高，对应的依赖关系就越长。 这种性质推导出了许多可以应用于序列建模的近似公式：</li>
</ul>
<p>$$<br>\begin{split}\begin{aligned}<br>P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2) P(x_3) P(x_4),\<br>P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_2) P(x_4  \mid  x_3),\<br>P(x_1, x_2, x_3, x_4) &amp;=  P(x_1) P(x_2  \mid  x_1) P(x_3  \mid  x_1, x_2) P(x_4  \mid  x_2, x_3).<br>\end{aligned}\end{split}<br>$$</p>
<blockquote>
<p>涉及一个、两个和三个变量的概率公式分别被称为 <em>一元语法</em>（unigram）、<em>二元语法</em>（bigram）和<em>三元语法</em>（trigram）模型。</p>
<p><strong>本质就是$\tau = 0, \tau = 1, \tau = 2$，每个词和前面多少个词相关，因此就有这个概率了昂！</strong></p>
</blockquote>
<h2 id="自然语言统计"><a href="#自然语言统计" class="headerlink" title="自然语言统计"></a>自然语言统计</h2><ul>
<li>查看出现频率最高的十个词：</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>tokens = d2l.tokenize(d2l.read_time_machine())<br><span class="hljs-comment"># 因为每个文本行不一定是一个句子或一个段落，因此我们把所有文本行拼接到一起</span><br>corpus = [token <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> line]<br>vocab = d2l.Vocab(corpus)<br>vocab.token_freqs[:<span class="hljs-number">10</span>]<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>最流行的词看起来很无聊， 这些词通常被称为<em>停用词</em>（stop words），因此可以被过滤掉。 尽管如此，它们本身仍然是有意义的，我们仍然会在模型中使用它们。 此外，还有个明显的问题是词频衰减的速度相当地快。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">freqs = [freq <span class="hljs-keyword">for</span> token, freq <span class="hljs-keyword">in</span> vocab.token_freqs]<br>d2l.plot(freqs, xlabel=<span class="hljs-string">'token: x'</span>, ylabel=<span class="hljs-string">'frequency: n(x)'</span>,<br>         xscale=<span class="hljs-string">'log'</span>, yscale=<span class="hljs-string">'log'</span>)<br></code></pre></td></tr></tbody></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308072134274.svg" srcset="/img/loading.gif" lazyload alt="../_images/output_language-models-and-dataset_789d14_21_0.svg"></p>
<blockquote>
<p>词频以一种明确的方式迅速衰减。 将前几个单词作为例外消除后，剩余的所有单词大致遵循双对数坐标图上的一条直线。 这意味着单词的频率满足<em>齐普夫定律</em>（Zipf’s law）， 即第$i$个最常用单词的频率$n_i$为：</p>
</blockquote>
<p>$$<br>n_i \propto \frac{1}{i^\alpha},<br>$$</p>
<blockquote>
<p>等价于</p>
</blockquote>
<p>$$<br>\log n_i = -\alpha \log i + c,<br>$$</p>
<ul>
<li>$\alpha$是刻画分布的指数，$c$是常数。 这告诉我们想要通过计数统计和平滑来建模单词是不可行的， 因为这样建模的结果会大大高估尾部单词的频率，也就是所谓的不常用单词。 那么其他的词元组合，比如二元语法、三元语法等等，又会如何呢？ 我们来看看二元语法的频率是否与一元语法的频率表现出相同的行为方式。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">bigram_tokens = [pair <span class="hljs-keyword">for</span> pair <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(corpus[:-<span class="hljs-number">1</span>], corpus[<span class="hljs-number">1</span>:])]<br>bigram_vocab = d2l.Vocab(bigram_tokens)<br>bigram_vocab.token_freqs[:<span class="hljs-number">10</span>]<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>在十个最频繁的词对中，有九个是由两个停用词组成的， 只有一个与“the time”有关。 我们再进一步看看三元语法的频率是否表现出相同的行为方式。</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">trigram_tokens = [triple <span class="hljs-keyword">for</span> triple <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<br>    corpus[:-<span class="hljs-number">2</span>], corpus[<span class="hljs-number">1</span>:-<span class="hljs-number">1</span>], corpus[<span class="hljs-number">2</span>:])]<br>trigram_vocab = d2l.Vocab(trigram_tokens)<br>trigram_vocab.token_freqs[:<span class="hljs-number">10</span>]<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>我们直观地对比三种模型中的词元频率：一元语法、二元语法和三元语法。</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">bigram_freqs = [freq <span class="hljs-keyword">for</span> token, freq <span class="hljs-keyword">in</span> bigram_vocab.token_freqs]<br>trigram_freqs = [freq <span class="hljs-keyword">for</span> token, freq <span class="hljs-keyword">in</span> trigram_vocab.token_freqs]<br>d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel=<span class="hljs-string">'token: x'</span>,<br>         ylabel=<span class="hljs-string">'frequency: n(x)'</span>, xscale=<span class="hljs-string">'log'</span>, yscale=<span class="hljs-string">'log'</span>,<br>         legend=[<span class="hljs-string">'unigram'</span>, <span class="hljs-string">'bigram'</span>, <span class="hljs-string">'trigram'</span>])<br></code></pre></td></tr></tbody></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308072141219.png" srcset="/img/loading.gif" lazyload alt="image-20230807214144040"></p>
<ul>
<li>这张图非常令人振奋！原因有很多：<ol>
<li>除了一元语法词，单词序列似乎也遵循齐普夫定律， 尽管公式 <a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html#equation-eq-zipf-law">(8.3.7)</a>中的指数$\alpha$更小 （指数的大小受序列长度的影响）；</li>
<li>词表中n元组的数量并没有那么大，这说明语言中存在相当多的结构， 这些结构给了我们应用模型的希望；</li>
<li>很多n元组很少出现，这使得拉普拉斯平滑非常不适合语言建模。 作为代替，我们将使用基于深度学习的模型。</li>
</ol>
</li>
</ul>
<h2 id="读取长序列数据"><a href="#读取长序列数据" class="headerlink" title="读取长序列数据"></a>读取长序列数据</h2><ul>
<li>当序列变得太长而不能被模型一次性全部处理时， 我们可能希望拆分这样的序列方便模型读取。我们看一下总体策略。 假设我们将使用神经网络来训练语言模型， 模型中的网络一次处理具有预定义长度 （例如n个时间步）的一个小批量序列。 现在的问题是如何随机生成一个小批量数据的特征和标签以供读取。</li>
<li>由于文本序列可以是任意长的， 例如整本《时光机器》（<em>The Time Machine</em>）， 于是任意长的序列可以被我们划分为具有相同时间步数的子序列。 当训练我们的神经网络时，这样的小批量子序列将被输入到模型中。 假设网络一次只处理具有n个时间步的子序列。每个时间步的词元对应于一个字符，如果大小为5，我们可以选择任意偏移量来指示初始位置，所以我们有相当大的自由度。</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308072152942.svg" srcset="/img/loading.gif" lazyload alt="../_images/timemachine-5gram.svg"></p>
<ul>
<li>如果我们只选择一个偏移量， 那么用于训练网络的、所有可能的子序列的覆盖范围将是有限的。 因此，我们可以从随机偏移量开始划分序列， 以同时获得<em>覆盖性</em>（coverage）和<em>随机性</em>（randomness）。 下面，我们将描述如何实现<em>随机采样</em>（random sampling）和 <em>顺序分区</em>（sequential partitioning）策略。</li>
</ul>
<h3 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h3><ul>
<li>在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列。 在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。 对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元， 因此标签是移位了一个词元的原始序列。</li>
<li>下面的代码每次可以从数据中随机生成一个小批量。 在这里，参数<code>batch_size</code>指定了每个小批量中子序列样本的数目， 参数<code>num_steps</code>是每个子序列中预定义的时间步数。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">seq_data_iter_random</span>(<span class="hljs-params">corpus, batch_size, num_steps</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">"""使用随机抽样生成一个小批量子序列"""</span><br>    <span class="hljs-comment"># 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1</span><br>    corpus = corpus[random.randint(<span class="hljs-number">0</span>, num_steps - <span class="hljs-number">1</span>):]<br>    <span class="hljs-comment"># 减去1，是因为我们需要考虑标签</span><br>    num_subseqs = (<span class="hljs-built_in">len</span>(corpus) - <span class="hljs-number">1</span>) // num_steps<br>    <span class="hljs-comment"># 长度为num_steps的子序列的起始索引</span><br>    initial_indices = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_subseqs * num_steps, num_steps))<br>    <span class="hljs-comment"># 在随机抽样的迭代过程中，</span><br>    <span class="hljs-comment"># 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻</span><br>    random.shuffle(initial_indices)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">data</span>(<span class="hljs-params">pos</span>):<br>        <span class="hljs-comment"># 返回从pos位置开始的长度为num_steps的序列</span><br>        <span class="hljs-keyword">return</span> corpus[pos: pos + num_steps]<br><br>    num_batches = num_subseqs // batch_size<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, batch_size * num_batches, batch_size):<br>        <span class="hljs-comment"># 在这里，initial_indices包含子序列的随机起始索引</span><br>        initial_indices_per_batch = initial_indices[i: i + batch_size]<br>        X = [data(j) <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> initial_indices_per_batch]<br>        Y = [data(j + <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> initial_indices_per_batch]<br>        <span class="hljs-keyword">yield</span> torch.tensor(X), torch.tensor(Y)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>下面我们生成一个从0到34的序列。 假设批量大小为2，时间步数为5，这意味着可以生成 ⌊(35−1)/5⌋=6个“特征－标签”子序列对。 如果设置小批量大小为2，我们只能得到3个小批量。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">my_seq = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">35</span>))<br><span class="hljs-keyword">for</span> X, Y <span class="hljs-keyword">in</span> seq_data_iter_random(my_seq, batch_size=<span class="hljs-number">2</span>, num_steps=<span class="hljs-number">5</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">'X: '</span>, X, <span class="hljs-string">'\nY:'</span>, Y)<br></code></pre></td></tr></tbody></table></figure>



<h3 id="顺序分区"><a href="#顺序分区" class="headerlink" title="顺序分区"></a>顺序分区</h3><ul>
<li>除了对原始序列可以随机抽样外， 我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。 这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">seq_data_iter_sequential</span>(<span class="hljs-params">corpus, batch_size, num_steps</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">"""使用顺序分区生成一个小批量子序列"""</span><br>    <span class="hljs-comment"># 从随机偏移量开始划分序列</span><br>    offset = random.randint(<span class="hljs-number">0</span>, num_steps)<br>    num_tokens = ((<span class="hljs-built_in">len</span>(corpus) - offset - <span class="hljs-number">1</span>) // batch_size) * batch_size<br>    Xs = torch.tensor(corpus[offset: offset + num_tokens])<br>    Ys = torch.tensor(corpus[offset + <span class="hljs-number">1</span>: offset + <span class="hljs-number">1</span> + num_tokens])<br>    Xs, Ys = Xs.reshape(batch_size, -<span class="hljs-number">1</span>), Ys.reshape(batch_size, -<span class="hljs-number">1</span>)<br>    num_batches = Xs.shape[<span class="hljs-number">1</span>] // num_steps<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_steps * num_batches, num_steps):<br>        X = Xs[:, i: i + num_steps]<br>        Y = Ys[:, i: i + num_steps]<br>        <span class="hljs-keyword">yield</span> X, Y<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>通过顺序分区读取每个小批量的子序列的特征<code>X</code>和标签<code>Y</code>。 通过将它们打印出来可以发现： 迭代期间来自两个相邻的小批量中的子序列在原始序列中确实是相邻的。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> X, Y <span class="hljs-keyword">in</span> seq_data_iter_sequential(my_seq, batch_size=<span class="hljs-number">2</span>, num_steps=<span class="hljs-number">5</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">'X: '</span>, X, <span class="hljs-string">'\nY:'</span>, Y)<br></code></pre></td></tr></tbody></table></figure>



<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>我们将上面的两个采样函数包装到一个类中， 以便稍后可以将其用作数据迭代器。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SeqDataLoader</span>:  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">"""加载序列数据的迭代器"""</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, batch_size, num_steps, use_random_iter, max_tokens</span>):<br>        <span class="hljs-keyword">if</span> use_random_iter:<br>            <span class="hljs-variable language_">self</span>.data_iter_fn = d2l.seq_data_iter_random<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.data_iter_fn = d2l.seq_data_iter_sequential<br>        <span class="hljs-variable language_">self</span>.corpus, <span class="hljs-variable language_">self</span>.vocab = d2l.load_corpus_time_machine(max_tokens)<br>        <span class="hljs-variable language_">self</span>.batch_size, <span class="hljs-variable language_">self</span>.num_steps = batch_size, num_steps<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__iter__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.data_iter_fn(<span class="hljs-variable language_">self</span>.corpus, <span class="hljs-variable language_">self</span>.batch_size, <span class="hljs-variable language_">self</span>.num_steps)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>最后，我们定义了一个总函数<code>load_data_time_machine</code>， 它同时返回数据迭代器和词表。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data_time_machine</span>(<span class="hljs-params">batch_size, num_steps,  <span class="hljs-comment">#@save</span></span><br><span class="hljs-params">                           use_random_iter=<span class="hljs-literal">False</span>, max_tokens=<span class="hljs-number">10000</span></span>):<br>    <span class="hljs-string">"""返回时光机器数据集的迭代器和词表"""</span><br>    data_iter = SeqDataLoader(<br>        batch_size, num_steps, use_random_iter, max_tokens)<br>    <span class="hljs-keyword">return</span> data_iter, data_iter.vocab<br></code></pre></td></tr></tbody></table></figure>



<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><ul>
<li>我们可以基于当前输入$x_{t}$和先前隐状态$h_{t-1}$来计算时间步t处的任何时间的隐状态：</li>
</ul>
<p>$$<br>h_t = f(x_{t}, h_{t-1}).<br>$$</p>
<h2 id="无隐状态的神经网络"><a href="#无隐状态的神经网络" class="headerlink" title="无隐状态的神经网络"></a>无隐状态的神经网络</h2><ul>
<li>隐藏层的输出$\mathbf{H} \in \mathbb{R}^{n \times h}$通过下式计算：</li>
</ul>
<p>$$<br>\mathbf{H} = \phi(\mathbf{X} \mathbf{W}_{xh} + \mathbf{b}_h).<br>$$</p>
<ul>
<li>输出层由下式给出：</li>
</ul>
<p>$$<br>\mathbf{O} = \mathbf{H} \mathbf{W}_{hq} + \mathbf{b}_q,<br>$$</p>
<blockquote>
<p>只要可以随机选择“特征-标签”对， 并且通过自动微分和随机梯度下降能够学习网络参数就可以了。</p>
</blockquote>
<h2 id="有隐状态的循环神经网络"><a href="#有隐状态的循环神经网络" class="headerlink" title="有隐状态的循环神经网络"></a>有隐状态的循环神经网络</h2><ul>
<li>使用latent variable来总结过去的信息</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308140952186.png" srcset="/img/loading.gif" lazyload alt="image-20230814095201754"></p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308140953741.png" srcset="/img/loading.gif" lazyload alt="image-20230814095339544"></p>
<blockquote>
<p>更新输出状态：</p>
<ul>
<li>更新隐藏状态：$\mathbf{H}<em>t = \phi(\mathbf{X}<em>t \mathbf{W}</em>{xh} + \mathbf{H}</em>{t-1} \mathbf{W}_{hh}  + \mathbf{b}_h).$</li>
<li>输出层的输出类似于多层感知机中的计算：$\mathbf{O}_t = \mathbf{H}<em>t \mathbf{W}</em>{hq} + \mathbf{b}_q.$</li>
<li>步骤：<ul>
<li>先更新隐藏状态</li>
<li>再根据当前的隐藏状态和输入，算出输出</li>
<li>损失就是当前输出，和下一个输入之间的不同产生的昂！</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li>在任意时间步t，隐状态的计算可以被视为：<ul>
<li>拼接当前时间步t的输入$X_t$和前一时间步t-1的隐状态$X_{t-1}$；</li>
<li>将拼接的结果送入带有激活函数$\phi$的全连接层。 全连接层的输出是当前时间步t的隐状态$H_t$。</li>
</ul>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308141034264.svg" srcset="/img/loading.gif" lazyload alt="../_images/rnn.svg"></p>
<h2 id="困惑度（perplexity）"><a href="#困惑度（perplexity）" class="headerlink" title="困惑度（perplexity）"></a>困惑度（perplexity）</h2><ul>
<li>我们可以通过一个序列中所有的n个词元的交叉熵损失的平均值来衡量语言模型的好坏（准确程度），这里又涉及到交叉熵啊，Softmax的那一套，本质是一个概率的估算：</li>
</ul>
<p>$$<br>\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1),<br>$$</p>
<ul>
<li>这使得不同长度的文档的性能具有了可比性。 由于历史原因，自然语言处理的科学家更喜欢使用一个叫做<em>困惑度</em>（perplexity）的量。</li>
</ul>
<p>$$<br>\exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right).<br>$$</p>
<h2 id="梯度剪裁"><a href="#梯度剪裁" class="headerlink" title="梯度剪裁"></a>梯度剪裁</h2><blockquote>
<p>长度为$O(T)$的矩阵乘法链，导致数值不稳定。梯度剪裁可以有效预防梯度爆炸。</p>
</blockquote>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308141042284.png" srcset="/img/loading.gif" lazyload alt="image-20230814104238029"></p>
<h2 id="更多应用"><a href="#更多应用" class="headerlink" title="更多应用"></a>更多应用</h2><p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308141044042.png" srcset="/img/loading.gif" lazyload alt="image-20230814104416808"></p>
<h1 id="RNN手动实现"><a href="#RNN手动实现" class="headerlink" title="RNN手动实现"></a>RNN手动实现</h1><ul>
<li>Dependencies：</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">%matplotlib inline<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>batch_size, num_steps = <span class="hljs-number">32</span>, <span class="hljs-number">35</span><br>train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)<br></code></pre></td></tr></tbody></table></figure>

<h2 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">F.one_hot(torch.tensor([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>]), <span class="hljs-built_in">len</span>(vocab))<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>将这些索引直接输入神经网络可能会使学习变得困难。 我们通常将每个词元表示为更具表现力的特征向量。 最简单的表示称为<em>独热编码</em>（one-hot encoding）</p>
</blockquote>
<ul>
<li>我们每次采样的小批量数据形状是二维张量： （批量大小，时间步数）。 <code>one_hot</code>函数将这样一个小批量数据转换成三维张量， 张量的最后一个维度等于词表大小（<code>len(vocab)</code>）。 我们经常转换输入的维度，以便获得形状为 （时间步数，批量大小，词表大小）的输出。 这将使我们能够更方便地通过最外层的维度， 一步一步地更新小批量数据的隐状态。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.arange(<span class="hljs-number">10</span>).reshape((<span class="hljs-number">2</span>, <span class="hljs-number">5</span>))<br>F.one_hot(X.T, <span class="hljs-number">28</span>).shape<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>转置？把时间放在第一个维度，每次访问都是连续的（batch + one_hot vector）</p>
</blockquote>
<h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><ul>
<li>我们初始化循环神经网络模型的模型参数。 隐藏单元数<code>num_hiddens</code>是一个可调的超参数。 当训练语言模型时，输入和输出来自相同的词表。 因此，它们具有相同的维度，即词表的大小。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_params</span>(<span class="hljs-params">vocab_size, num_hiddens, device</span>):<br>    num_inputs = num_outputs = vocab_size<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">normal</span>(<span class="hljs-params">shape</span>):<br>        <span class="hljs-keyword">return</span> torch.randn(size=shape, device=device) * <span class="hljs-number">0.01</span><br><br>    <span class="hljs-comment"># 隐藏层参数</span><br>    W_xh = normal((num_inputs, num_hiddens))<br>    W_hh = normal((num_hiddens, num_hiddens))<br>    b_h = torch.zeros(num_hiddens, device=device)<br>    <span class="hljs-comment"># 输出层参数</span><br>    W_hq = normal((num_hiddens, num_outputs))<br>    b_q = torch.zeros(num_outputs, device=device)<br>    <span class="hljs-comment"># 附加梯度</span><br>    params = [W_xh, W_hh, b_h, W_hq, b_q]<br>    <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:<br>        param.requires_grad_(<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> params<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>W_hh移掉之后，就是一个单隐藏层的MLP嘛，一个输入的W和b，一个输出的W和b。</p>
</blockquote>
<h2 id="循环神经网络模型"><a href="#循环神经网络模型" class="headerlink" title="循环神经网络模型"></a>循环神经网络模型</h2><ul>
<li>我们首先需要一个<code>init_rnn_state</code>函数在初始化时返回隐状态。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_rnn_state</span>(<span class="hljs-params">batch_size, num_hiddens, device</span>):<br>    <span class="hljs-keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>下面的<code>rnn</code>函数定义了如何在一个时间步内计算隐状态和输出。 循环神经网络模型通过<code>inputs</code>最外层的维度实现循环， 以便逐时间步更新小批量数据的隐状态<code>H</code>。 此外，这里使用tanh函数作为激活函数。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">rnn</span>(<span class="hljs-params">inputs, state, params</span>):<br>    <span class="hljs-comment"># inputs的形状：(时间步数量，批量大小，词表大小)</span><br>    W_xh, W_hh, b_h, W_hq, b_q = params<br>    H, = state<br>    outputs = []<br>    <span class="hljs-comment"># X的形状：(批量大小，词表大小)</span><br>    <span class="hljs-keyword">for</span> X <span class="hljs-keyword">in</span> inputs:<br>        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)<br>        Y = torch.mm(H, W_hq) + b_q<br>        outputs.append(Y)<br>    <span class="hljs-keyword">return</span> torch.cat(outputs, dim=<span class="hljs-number">0</span>), (H,)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>定义了所有需要的函数之后，接下来我们创建一个类来包装这些函数， 并存储从零开始实现的循环神经网络模型的参数。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RNNModelScratch</span>: <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">"""从零开始实现的循环神经网络模型"""</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, num_hiddens, device,</span><br><span class="hljs-params">                 get_params, init_state, forward_fn</span>):<br>        <span class="hljs-variable language_">self</span>.vocab_size, <span class="hljs-variable language_">self</span>.num_hiddens = vocab_size, num_hiddens<br>        <span class="hljs-variable language_">self</span>.params = get_params(vocab_size, num_hiddens, device)<br>        <span class="hljs-variable language_">self</span>.init_state, <span class="hljs-variable language_">self</span>.forward_fn = init_state, forward_fn<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, X, state</span>):<br>        X = F.one_hot(X.T, <span class="hljs-variable language_">self</span>.vocab_size).<span class="hljs-built_in">type</span>(torch.float32)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.forward_fn(X, state, <span class="hljs-variable language_">self</span>.params)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">begin_state</span>(<span class="hljs-params">self, batch_size, device</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.init_state(batch_size, <span class="hljs-variable language_">self</span>.num_hiddens, device)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>检查输出是否具有正确的形状。 例如，隐状态的维数是否保持不变。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">num_hiddens = <span class="hljs-number">512</span><br>net = RNNModelScratch(<span class="hljs-built_in">len</span>(vocab), num_hiddens, d2l.try_gpu(), get_params,<br>                      init_rnn_state, rnn)<br>state = net.begin_state(X.shape[<span class="hljs-number">0</span>], d2l.try_gpu())<br>Y, new_state = net(X.to(d2l.try_gpu()), state)<br>Y.shape, <span class="hljs-built_in">len</span>(new_state), new_state[<span class="hljs-number">0</span>].shape<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>我们可以看到输出形状是（时间步数×批量大小，词表大小）， 而隐状态形状保持不变，即（批量大小，隐藏单元数）。</p>
</blockquote>
<h2 id="预测-1"><a href="#预测-1" class="headerlink" title="预测"></a>预测</h2><ul>
<li>我们首先定义预测函数来生成<code>prefix</code>之后的新字符， 其中的<code>prefix</code>是一个用户提供的包含多个字符的字符串。 在循环遍历<code>prefix</code>中的开始字符时， 我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。 这被称为<em>预热</em>（warm-up）期， 因为在此期间模型会自我更新（例如，更新隐状态）， 但不会进行预测。 预热期结束后，隐状态的值通常比刚开始的初始值更适合预测， 从而预测字符并输出它们。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict_ch8</span>(<span class="hljs-params">prefix, num_preds, net, vocab, device</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">"""在prefix后面生成新字符"""</span><br>    state = net.begin_state(batch_size=<span class="hljs-number">1</span>, device=device)<br>    outputs = [vocab[prefix[<span class="hljs-number">0</span>]]]<br>    get_input = <span class="hljs-keyword">lambda</span>: torch.tensor([outputs[-<span class="hljs-number">1</span>]], device=device).reshape((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> prefix[<span class="hljs-number">1</span>:]:  <span class="hljs-comment"># 预热期</span><br>        _, state = net(get_input(), state)<br>        outputs.append(vocab[y])<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_preds):  <span class="hljs-comment"># 预测num_preds步</span><br>        y, state = net(get_input(), state)<br>        outputs.append(<span class="hljs-built_in">int</span>(y.argmax(dim=<span class="hljs-number">1</span>).reshape(<span class="hljs-number">1</span>)))<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">''</span>.join([vocab.idx_to_token[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> outputs])<br></code></pre></td></tr></tbody></table></figure>



<h2 id="梯度裁剪"><a href="#梯度裁剪" class="headerlink" title="梯度裁剪"></a>梯度裁剪</h2><ul>
<li>对于长度为T的序列，我们在迭代中计算这T个时间步上的梯度， 将会在反向传播过程中产生长度为O(T)的矩阵乘法链。 如 <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#sec-numerical-stability">4.8节</a>所述， 当T较大时，它可能导致数值不稳定， 例如可能导致梯度爆炸或梯度消失。 因此，循环神经网络模型往往需要额外的方式来支持稳定训练。</li>
<li>有时梯度可能很大，从而优化算法可能无法收敛。我们可以通过降低$\eta$的学习率来解决这个问题。一个流行的替代方案是通过将梯度g投影回给定半径 （例如$\theta$）的球来裁剪梯度g。 如下式：</li>
</ul>
<p>$$<br>\mathbf{g} \leftarrow \min\left(1, \frac{\theta}{|\mathbf{g}|}\right) \mathbf{g}.<br>$$</p>
<blockquote>
<p>我们知道梯度范数永远不会超过$\theta$， 并且更新后的梯度完全与g的原始方向对齐。 它还有一个值得拥有的副作用， 即限制任何给定的小批量数据（以及其中任何给定的样本）对参数向量的影响， 这赋予了模型一定程度的稳定性。 梯度裁剪提供了一个快速修复梯度爆炸的方法， 虽然它并不能完全解决问题，但它是众多有效的技术之一。</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">grad_clipping</span>(<span class="hljs-params">net, theta</span>):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">"""裁剪梯度"""</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module):<br>        params = [p <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> net.parameters() <span class="hljs-keyword">if</span> p.requires_grad]<br>    <span class="hljs-keyword">else</span>:<br>        params = net.params<br>    norm = torch.sqrt(<span class="hljs-built_in">sum</span>(torch.<span class="hljs-built_in">sum</span>((p.grad ** <span class="hljs-number">2</span>)) <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> params))<br>    <span class="hljs-keyword">if</span> norm &gt; theta:<br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> params:<br>            param.grad[:] *= theta / norm<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>全局梯度裁剪</p>
</blockquote>
<h2 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h2><ul>
<li><p>我们定义一个函数在一个迭代周期内训练模型：</p>
<ol>
<li>序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。</li>
<li>我们在更新模型参数之前裁剪梯度。 这样的操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。</li>
<li>我们用困惑度来评价模型。如 <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/rnn.html#subsec-perplexity">8.4.4节</a>所述， 这样的度量确保了不同长度的序列具有可比性。</li>
</ol>
</li>
<li><p>定义一个函数在一个迭代周期内训练模型：</p>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_epoch_ch8</span>(<span class="hljs-params">net, train_iter, loss, updater, device, use_random_iter</span>):<br>    <span class="hljs-string">"""训练网络一个迭代周期（定义见第8章）"""</span><br>    state, timer = <span class="hljs-literal">None</span>, d2l.Timer()<br>    metric = d2l.Accumulator(<span class="hljs-number">2</span>)  <span class="hljs-comment"># 训练损失之和,词元数量</span><br>    <span class="hljs-keyword">for</span> X, Y <span class="hljs-keyword">in</span> train_iter:<br>        <span class="hljs-keyword">if</span> state <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> use_random_iter:<br>            <span class="hljs-comment"># 在第一次迭代或使用随机抽样时初始化state</span><br>            state = net.begin_state(batch_size=X.shape[<span class="hljs-number">0</span>], device=device)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module) <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(state, <span class="hljs-built_in">tuple</span>):<br>                <span class="hljs-comment"># state对于nn.GRU是个张量</span><br>                state.detach_()<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># state对于nn.LSTM或对于我们从零开始实现的模型是个张量</span><br>                <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> state:<br>                    s.detach_()<br>        y = Y.T.reshape(-<span class="hljs-number">1</span>)<br>        X, y = X.to(device), y.to(device)<br>        y_hat, state = net(X, state)<br>        l = loss(y_hat, y.long()).mean()<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(updater, torch.optim.Optimizer):<br>            updater.zero_grad()<br>            l.backward()<br>            grad_clipping(net, <span class="hljs-number">1</span>)<br>            updater.step()<br>        <span class="hljs-keyword">else</span>:<br>            l.backward()<br>            grad_clipping(net, <span class="hljs-number">1</span>)<br>            <span class="hljs-comment"># 因为已经调用了mean函数</span><br>            updater(batch_size=<span class="hljs-number">1</span>)<br>        metric.add(l * y.numel(), y.numel())<br>    <span class="hljs-keyword">return</span> math.exp(metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">1</span>]), metric[<span class="hljs-number">1</span>] / timer.stop()<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>循环神经网络模型的训练函数既支持从零开始实现， 也可以使用高级API来实现。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_ch8</span>(<span class="hljs-params">net, train_iter, vocab, lr, num_epochs, device,</span><br><span class="hljs-params">              use_random_iter=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-string">"""训练模型（定义见第8章）"""</span><br>    loss = nn.CrossEntropyLoss()<br>    animator = d2l.Animator(xlabel=<span class="hljs-string">'epoch'</span>, ylabel=<span class="hljs-string">'perplexity'</span>,<br>                            legend=[<span class="hljs-string">'train'</span>], xlim=[<span class="hljs-number">10</span>, num_epochs])<br>    <span class="hljs-comment"># 初始化</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module):<br>        updater = torch.optim.SGD(net.parameters(), lr)<br>    <span class="hljs-keyword">else</span>:<br>        updater = <span class="hljs-keyword">lambda</span> batch_size: d2l.sgd(net.params, lr, batch_size)<br>    predict = <span class="hljs-keyword">lambda</span> prefix: predict_ch8(prefix, <span class="hljs-number">50</span>, net, vocab, device)<br>    <span class="hljs-comment"># 训练和预测</span><br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        ppl, speed = train_epoch_ch8(<br>            net, train_iter, loss, updater, device, use_random_iter)<br>        <span class="hljs-keyword">if</span> (epoch + <span class="hljs-number">1</span>) % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(predict(<span class="hljs-string">'time traveller'</span>))<br>            animator.add(epoch + <span class="hljs-number">1</span>, [ppl])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'困惑度 <span class="hljs-subst">{ppl:<span class="hljs-number">.1</span>f}</span>, <span class="hljs-subst">{speed:<span class="hljs-number">.1</span>f}</span> 词元/秒 <span class="hljs-subst">{<span class="hljs-built_in">str</span>(device)}</span>'</span>)<br>    <span class="hljs-built_in">print</span>(predict(<span class="hljs-string">'time traveller'</span>))<br>    <span class="hljs-built_in">print</span>(predict(<span class="hljs-string">'traveller'</span>))<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>我们训练循环神经网络模型。 因为我们在数据集中只使用了10000个词元， 所以模型需要更多的迭代周期来更好地收敛。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">num_epochs, lr = <span class="hljs-number">500</span>, <span class="hljs-number">1</span><br>train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>让我们检查一下使用随机抽样方法的结果。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">net = RNNModelScratch(<span class="hljs-built_in">len</span>(vocab), num_hiddens, d2l.try_gpu(), get_params,<br>                      init_rnn_state, rnn)<br>train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(),<br>          use_random_iter=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></tbody></table></figure>











<h1 id="RNN简洁实现"><a href="#RNN简洁实现" class="headerlink" title="RNN简洁实现"></a>RNN简洁实现</h1><ul>
<li>Dependencies:</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>batch_size, num_steps = <span class="hljs-number">32</span>, <span class="hljs-number">35</span><br>train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)<br></code></pre></td></tr></tbody></table></figure>

<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">num_hiddens = <span class="hljs-number">256</span><br>rnn_layer = nn.RNN(<span class="hljs-built_in">len</span>(vocab), num_hiddens)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>我们使用张量来初始化隐状态，它的形状是（隐藏层数，批量大小，隐藏单元数）。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">state = torch.zeros((<span class="hljs-number">1</span>, batch_size, num_hiddens))<br>state.shape<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。 需要强调的是，<code>rnn_layer</code>的“输出”（<code>Y</code>）不涉及输出层的计算： 它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.rand(size=(num_steps, batch_size, <span class="hljs-built_in">len</span>(vocab)))<br>Y, state_new = rnn_layer(X, state)<br>Y.shape, state_new.shape<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>我们为一个完整的循环神经网络模型定义了一个<code>RNNModel</code>类。 注意，<code>rnn_layer</code>只包含隐藏的循环层，我们还需要创建一个单独的输出层。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RNNModel</span>(nn.Module):<br>    <span class="hljs-string">"""循环神经网络模型"""</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, rnn_layer, vocab_size, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(RNNModel, <span class="hljs-variable language_">self</span>).__init__(**kwargs)<br>        <span class="hljs-variable language_">self</span>.rnn = rnn_layer<br>        <span class="hljs-variable language_">self</span>.vocab_size = vocab_size<br>        <span class="hljs-variable language_">self</span>.num_hiddens = <span class="hljs-variable language_">self</span>.rnn.hidden_size<br>        <span class="hljs-comment"># 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.rnn.bidirectional:<br>            <span class="hljs-variable language_">self</span>.num_directions = <span class="hljs-number">1</span><br>            <span class="hljs-variable language_">self</span>.linear = nn.Linear(<span class="hljs-variable language_">self</span>.num_hiddens, <span class="hljs-variable language_">self</span>.vocab_size)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.num_directions = <span class="hljs-number">2</span><br>            <span class="hljs-variable language_">self</span>.linear = nn.Linear(<span class="hljs-variable language_">self</span>.num_hiddens * <span class="hljs-number">2</span>, <span class="hljs-variable language_">self</span>.vocab_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs, state</span>):<br>        X = F.one_hot(inputs.T.long(), <span class="hljs-variable language_">self</span>.vocab_size)<br>        X = X.to(torch.float32)<br>        Y, state = <span class="hljs-variable language_">self</span>.rnn(X, state)<br>        <span class="hljs-comment"># 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)</span><br>        <span class="hljs-comment"># 它的输出形状是(时间步数*批量大小,词表大小)。</span><br>        output = <span class="hljs-variable language_">self</span>.linear(Y.reshape((-<span class="hljs-number">1</span>, Y.shape[-<span class="hljs-number">1</span>])))<br>        <span class="hljs-keyword">return</span> output, state<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">begin_state</span>(<span class="hljs-params">self, device, batch_size=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(<span class="hljs-variable language_">self</span>.rnn, nn.LSTM):<br>            <span class="hljs-comment"># nn.GRU以张量作为隐状态</span><br>            <span class="hljs-keyword">return</span>  torch.zeros((<span class="hljs-variable language_">self</span>.num_directions * <span class="hljs-variable language_">self</span>.rnn.num_layers,<br>                                 batch_size, <span class="hljs-variable language_">self</span>.num_hiddens),<br>                                device=device)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># nn.LSTM以元组作为隐状态</span><br>            <span class="hljs-keyword">return</span> (torch.zeros((<br>                <span class="hljs-variable language_">self</span>.num_directions * <span class="hljs-variable language_">self</span>.rnn.num_layers,<br>                batch_size, <span class="hljs-variable language_">self</span>.num_hiddens), device=device),<br>                    torch.zeros((<br>                        <span class="hljs-variable language_">self</span>.num_directions * <span class="hljs-variable language_">self</span>.rnn.num_layers,<br>                        batch_size, <span class="hljs-variable language_">self</span>.num_hiddens), device=device))<br></code></pre></td></tr></tbody></table></figure>



<h2 id="训练与预测"><a href="#训练与预测" class="headerlink" title="训练与预测"></a>训练与预测</h2><ul>
<li>我们基于一个具有随机权重的模型进行预测。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">device = d2l.try_gpu()<br>net = RNNModel(rnn_layer, vocab_size=<span class="hljs-built_in">len</span>(vocab))<br>net = net.to(device)<br>d2l.predict_ch8(<span class="hljs-string">'time traveller'</span>, <span class="hljs-number">10</span>, net, vocab, device)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>我们使用 <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html#sec-rnn-scratch">8.5节</a>中 定义的超参数调用<code>train_ch8</code>，并且使用高级API训练模型。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">num_epochs, lr = <span class="hljs-number">500</span>, <span class="hljs-number">1</span><br>d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)<br></code></pre></td></tr></tbody></table></figure>









<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/643560888">LLM面试题</a></li>
<li><a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/index.html">d2l</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/109217883">马尔可夫链</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1L44y1m768?p=3&amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd">序列模型 Q&amp;A</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Fo4y1Q79L/?p=2&amp;spm_id_from=pageDriver&amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd">文本预处理 Q&amp;A</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ZX4y1F7K3/?p=3&amp;spm_id_from=pageDriver&amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd">语音模型 Q&amp;A</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32829048">Introduction of n-gram</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1D64y1z7CA/?p=2&amp;spm_id_from=pageDriver&amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd">RNN Q&amp;A</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1kq4y1H7sw?p=3&amp;spm_id_from=pageDriver&amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd">RNN实现 Q&amp;A</a></li>
</ol>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/AI/" class="category-chain-item">AI</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/">#研0自学</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>D2L-9-Recurrent Neural Networks</div>
      <div>https://alexanderliu-creator.github.io/2023/08/07/d2l-9-recurrent-neural-networks/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Alexander Liu</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年8月7日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/08/07/d2l-8-computer-vision/" title="D2L-8-Computer Vision">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">D2L-8-Computer Vision</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/08/06/d2l-7-modern-convolutional-neural-networks/" title="D2L-7-Modern Convolutional Neural Networks">
                        <span class="hidden-mobile">D2L-7-Modern Convolutional Neural Networks</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.events.registerRefreshCallback(function() {
      if ('mermaid' in window) {
        mermaid.init();
      }
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
