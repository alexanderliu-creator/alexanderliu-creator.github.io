

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/tuzi.png">
  <link rel="icon" href="/img/tuzi.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Alexander Liu">
  <meta name="keywords" content="åˆ†å¸ƒå¼ç³»ç»Ÿ,åç«¯ç ”å‘,æ•°æ®ååŒ">
  
    <meta name="description" content="Self Attention &amp; Transformer æå®æ¯…ï¼ŒSelf-supervised Learing BERT GPT æå®æ¯…ã€‚å…ˆçœ‹è¿™äº›ï¼è¿™ä¸ªå¯¹äºåŸç†è®²çš„éå¸¸æ¸…æ¥šï¼Œäº†è§£äº†è¿™ä¸ªï¼Œå†å»å­¦ææ²çš„è¯¾ç¨‹ï¼Œå°±ååˆ†æ¸…æ™°äº†æ˜‚ï¼ï¼ï¼">
<meta property="og:type" content="article">
<meta property="og:title" content="D2L-11-Attention Mechanisms and Transformers">
<meta property="og:url" content="http://example.com/2023/08/16/d2l-11-attention-mechanisms-and-transformers/index.html">
<meta property="og:site_name" content="å…”ã®åšå®¢">
<meta property="og:description" content="Self Attention &amp; Transformer æå®æ¯…ï¼ŒSelf-supervised Learing BERT GPT æå®æ¯…ã€‚å…ˆçœ‹è¿™äº›ï¼è¿™ä¸ªå¯¹äºåŸç†è®²çš„éå¸¸æ¸…æ¥šï¼Œäº†è§£äº†è¿™ä¸ªï¼Œå†å»å­¦ææ²çš„è¯¾ç¨‹ï¼Œå°±ååˆ†æ¸…æ™°äº†æ˜‚ï¼ï¼ï¼">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202307231956594.jpg">
<meta property="article:published_time" content="2023-08-16T09:06:57.000Z">
<meta property="article:modified_time" content="2023-08-18T11:31:35.036Z">
<meta property="article:author" content="Alexander Liu">
<meta property="article:tag" content="ç ”0è‡ªå­¦">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202307231956594.jpg">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>D2L-11-Attention Mechanisms and Transformers - å…”ã®åšå®¢</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- ä¸»é¢˜ä¾èµ–çš„å›¾æ ‡åº“ï¼Œä¸è¦è‡ªè¡Œä¿®æ”¹ -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":1},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.2.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="å…”ã®åšå®¢" type="application/atom+xml">
</head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>å…”çš„åšå®¢</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                é¦–é¡µ
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                å½’æ¡£
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                åˆ†ç±»
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                æ ‡ç­¾
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                å…³äº
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                å‹é“¾
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/background_post.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="D2L-11-Attention Mechanisms and Transformers"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Alexander Liu
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-08-16 17:06" pubdate>
          2023å¹´8æœˆ16æ—¥ ä¸‹åˆ
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          53k å­—
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          440 åˆ†é’Ÿ
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> æ¬¡
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">D2L-11-Attention Mechanisms and Transformers</h1>
            
              <p class="note note-info">
                
                  
                    æœ¬æ–‡æœ€åæ›´æ–°äºï¼š7 å¤©å‰
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1v3411r78R?p=1&amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd">Self Attention &amp; Transformer æå®æ¯…</a>ï¼Œ<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1fL4y1z7Pi/?p=2&amp;spm_id_from=pageDriver&amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd">Self-supervised Learing BERT GPT æå®æ¯…</a>ã€‚å…ˆçœ‹è¿™äº›ï¼è¿™ä¸ªå¯¹äºåŸç†è®²çš„éå¸¸æ¸…æ¥šï¼Œäº†è§£äº†è¿™ä¸ªï¼Œå†å»å­¦ææ²çš„è¯¾ç¨‹ï¼Œå°±ååˆ†æ¸…æ™°äº†æ˜‚ï¼ï¼ï¼</p>
<span id="more"></span>

<h1 id="æ³¨æ„åŠ›æœºåˆ¶"><a href="#æ³¨æ„åŠ›æœºåˆ¶" class="headerlink" title="æ³¨æ„åŠ›æœºåˆ¶"></a>æ³¨æ„åŠ›æœºåˆ¶</h1><h2 id="æŸ¥è¯¢ã€é”®å’Œå€¼"><a href="#æŸ¥è¯¢ã€é”®å’Œå€¼" class="headerlink" title="æŸ¥è¯¢ã€é”®å’Œå€¼"></a>æŸ¥è¯¢ã€é”®å’Œå€¼</h2><ul>
<li>å·ç§¯ã€å…¨è¿æ¥å’Œæ± åŒ–å±‚éƒ½åªè€ƒè™‘ä¸éšæ„çº¿ç´¢ã€‚</li>
<li>æ³¨æ„åŠ›æœºåˆ¶åˆ™æ˜¾ç¤ºåœ°è€ƒè™‘éšæ„çº¿ç´¢<ul>
<li>éšæ„çº¿ç´¢è¢«ç§°ä¹‹ä¸ºæŸ¥è¯¢(Query)</li>
<li>æ¯ä¸ªè¾“å…¥æ˜¯ä¸€ä¸ªå€¼(Value)å’Œä¸éšæ„çº¿ç´¢(Key)çš„å¯¹</li>
<li>é€šè¿‡æ³¨æ„åŠ›æ± åŒ–å±‚æ¥åå‘æ€§åœ°é€‰æ‹©æŸäº›è¾“å…¥</li>
</ul>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308161712603.png" srcset="/img/loading.gif" lazyload alt="image-20230816171211573"></p>
<blockquote>
<p> ç»™å®šä»»ä½•æŸ¥è¯¢ï¼Œæ³¨æ„åŠ›æœºåˆ¶é€šè¿‡<em>æ³¨æ„åŠ›æ±‡èš</em>ï¼ˆattention poolingï¼‰ å°†é€‰æ‹©å¼•å¯¼è‡³<em>æ„Ÿå®˜è¾“å…¥</em>ï¼ˆsensory inputsï¼Œä¾‹å¦‚ä¸­é—´ç‰¹å¾è¡¨ç¤ºï¼‰ã€‚ åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œè¿™äº›æ„Ÿå®˜è¾“å…¥è¢«ç§°ä¸º<em>å€¼</em>ï¼ˆvalueï¼‰ã€‚ æ›´é€šä¿—çš„è§£é‡Šï¼Œæ¯ä¸ªå€¼éƒ½ä¸ä¸€ä¸ª<em>é”®</em>ï¼ˆkeyï¼‰é…å¯¹ï¼Œ è¿™å¯ä»¥æƒ³è±¡ä¸ºæ„Ÿå®˜è¾“å…¥çš„éè‡ªä¸»æç¤ºã€‚ å¦‚ <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/attention-cues.html#fig-qkv">å›¾10.1.3</a>æ‰€ç¤ºï¼Œå¯ä»¥é€šè¿‡è®¾è®¡æ³¨æ„åŠ›æ±‡èšçš„æ–¹å¼ï¼Œ ä¾¿äºç»™å®šçš„æŸ¥è¯¢ï¼ˆè‡ªä¸»æ€§æç¤ºï¼‰ä¸é”®ï¼ˆéè‡ªä¸»æ€§æç¤ºï¼‰è¿›è¡ŒåŒ¹é…ï¼Œ è¿™å°†å¼•å¯¼å¾—å‡ºæœ€åŒ¹é…çš„å€¼ï¼ˆæ„Ÿå®˜è¾“å…¥ï¼‰ã€‚</p>
</blockquote>
<h2 id="éå‚æ³¨æ„åŠ›æ± åŒ–å±‚"><a href="#éå‚æ³¨æ„åŠ›æ± åŒ–å±‚" class="headerlink" title="éå‚æ³¨æ„åŠ›æ± åŒ–å±‚"></a>éå‚æ³¨æ„åŠ›æ± åŒ–å±‚</h2><blockquote>
<p>ç»™å®šæ•°æ®$(x_i, y_i)$</p>
</blockquote>
<h3 id="å¹³å‡æ± åŒ–å±‚"><a href="#å¹³å‡æ± åŒ–å±‚" class="headerlink" title="å¹³å‡æ± åŒ–å±‚"></a>å¹³å‡æ± åŒ–å±‚</h3><p>$$<br>f(x) = \frac{1}{n}\sum_{i=1}^n y_i,<br>$$</p>
<blockquote>
<p>æœ€ç®€å•çš„ä¼°è®¡å™¨ï¼Œè¿™ä¸ªä¼°è®¡å™¨ç¡®å®ä¸å¤Ÿèªæ˜ã€‚ çœŸå®å‡½æ•°fï¼ˆâ€œTruthâ€ï¼‰å’Œé¢„æµ‹å‡½æ•°ï¼ˆâ€œPredâ€ï¼‰ç›¸å·®å¾ˆå¤§ã€‚</p>
</blockquote>
<h3 id="Nadaraya-Watsonæ ¸å›å½’"><a href="#Nadaraya-Watsonæ ¸å›å½’" class="headerlink" title="Nadaraya-Watsonæ ¸å›å½’"></a>Nadaraya-Watsonæ ¸å›å½’</h3><ul>
<li>æ ¹æ®è¾“å…¥çš„ä½ç½®å¯¹è¾“å‡º$y_i$è¿›è¡ŒåŠ æƒï¼š</li>
</ul>
<p>$$<br>f(x) = \sum_{i=1}^n \frac{K(x - x_i)}{\sum_{j=1}^n K(x - x_j)} y_i,<br>$$</p>
<blockquote>
<p>Kæ˜¯Kernelæ˜‚ï¼ï¼ï¼ä¸Šè¿°ä¼°è®¡å™¨è¢«ç§°ä¸º <em>Nadaraya-Watsonæ ¸å›å½’</em>ï¼ˆNadaraya-Watson kernel regressionï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥ä» <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/attention-cues.html#fig-qkv">å›¾10.1.3</a>ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶æ¡†æ¶çš„è§’åº¦ é‡å†™ <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html#equation-eq-nadaraya-watson">(10.2.3)</a>ï¼Œ æˆä¸ºä¸€ä¸ªæ›´åŠ é€šç”¨çš„<em>æ³¨æ„åŠ›æ±‡èš</em>ï¼ˆattention poolingï¼‰å…¬å¼ï¼š</p>
</blockquote>
<p>$$<br>f(x) = \sum_{i=1}^n \alpha(x, x_i) y_i,<br>$$</p>
<blockquote>
<p>$x$æ˜¯æŸ¥è¯¢ï¼Œ$(x_i, y_i)$æ˜¯key-valueå¯¹ï¼Œæ³¨æ„åŠ›æ±‡èšæ˜¯$y_i$çš„åŠ æƒå¹³å‡ã€‚å°†æŸ¥è¯¢$x$å’Œé”®$x_i$ä¹‹é—´çš„å…³ç³»å»ºæ¨¡ä¸º <em>æ³¨æ„åŠ›æƒé‡</em>ï¼ˆattention weightï¼‰$\alpha(x, x_i)$ï¼Œ å¦‚ <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html#equation-eq-attn-pooling">(10.2.4)</a>æ‰€ç¤ºï¼Œ è¿™ä¸ªæƒé‡å°†è¢«åˆ†é…ç»™æ¯ä¸€ä¸ªå¯¹åº”å€¼$y_i$ã€‚ å¯¹äºä»»ä½•æŸ¥è¯¢ï¼Œæ¨¡å‹åœ¨æ‰€æœ‰é”®å€¼å¯¹æ³¨æ„åŠ›æƒé‡éƒ½æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ¦‚ç‡åˆ†å¸ƒï¼š å®ƒä»¬æ˜¯éè´Ÿçš„ï¼Œå¹¶ä¸”æ€»å’Œä¸º1ã€‚</p>
</blockquote>
<ul>
<li>ä¸‹é¢è€ƒè™‘ä¸€ä¸ª<em>é«˜æ–¯æ ¸</em>ï¼ˆGaussian kernelï¼‰ï¼Œå…¶å®šä¹‰ä¸ºï¼š</li>
</ul>
<p>$$<br>K(u) = \frac{1}{\sqrt{2\pi}} \exp(-\frac{u^2}{2}).<br>$$</p>
<blockquote>
<p>å°†é«˜æ–¯æ ¸ä»£å…¥ <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html#equation-eq-attn-pooling">(10.2.4)</a>å’Œ <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/nadaraya-waston.html#equation-eq-nadaraya-watson">(10.2.3)</a>å¯ä»¥å¾—åˆ°ï¼š</p>
</blockquote>
<p>$$<br>\begin{split}\begin{aligned} f(x) &amp;=\sum_{i=1}^n \alpha(x, x_i) y_i\ &amp;= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x - x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)} y_i \&amp;= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x - x_i)^2\right) y_i. \end{aligned}\end{split}<br>$$</p>
<h2 id="å‚æ•°åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶"><a href="#å‚æ•°åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶" class="headerlink" title="å‚æ•°åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶"></a>å‚æ•°åŒ–çš„æ³¨æ„åŠ›æœºåˆ¶</h2><ul>
<li>åœ¨ä¹‹å‰çš„åŸºç¡€ä¸Šï¼Œå¯ä»¥å¼•å…¥å¯ä»¥å­¦ä¹ çš„$\omega$ï¼Œæœ‰ï¼š</li>
</ul>
<p>$$<br>\begin{split}\begin{aligned}f(x) &amp;= \sum_{i=1}^n \alpha(x, x_i) y_i \&amp;= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}((x - x_i)w)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}((x - x_j)w)^2\right)} y_i \&amp;= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right) y_i.\end{aligned}\end{split}<br>$$</p>
<h2 id="ä»£ç "><a href="#ä»£ç " class="headerlink" title="ä»£ç "></a>ä»£ç </h2><h3 id="Dependencies"><a href="#Dependencies" class="headerlink" title="Dependencies"></a>Dependencies</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br></code></pre></td></tr></tbody></table></figure>

<h3 id="ç”Ÿæˆæ•°æ®é›†"><a href="#ç”Ÿæˆæ•°æ®é›†" class="headerlink" title="ç”Ÿæˆæ•°æ®é›†"></a>ç”Ÿæˆæ•°æ®é›†</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">n_train = <span class="hljs-number">50</span><br>x_train, _ = torch.sort(torch.rand(n_train) * <span class="hljs-number">5</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> * torch.sin(x) + x**<span class="hljs-number">0.8</span><br><br>y_train = f(x_train) + torch.normal(<span class="hljs-number">0.0</span>, <span class="hljs-number">0.5</span>, (n_train,))<br>x_test = torch.arange(<span class="hljs-number">0</span>, <span class="hljs-number">5</span>, <span class="hljs-number">0.1</span>)<br>y_truth = f(x_test)<br>n_test = <span class="hljs-built_in">len</span>(x_test)<br>n_test<br></code></pre></td></tr></tbody></table></figure>

<h3 id="ç»˜å›¾"><a href="#ç»˜å›¾" class="headerlink" title="ç»˜å›¾"></a>ç»˜å›¾</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_kernel_reg</span>(<span class="hljs-params">y_hat</span>):<br>    d2l.plot(x_test, [y_truth, y_hat], <span class="hljs-string">'x'</span>, <span class="hljs-string">'y'</span>, legend=[<span class="hljs-string">'Truth'</span>, <span class="hljs-string">'Pred'</span>],<br>             xlim=[<span class="hljs-number">0</span>, <span class="hljs-number">5</span>], ylim=[-<span class="hljs-number">1</span>, <span class="hljs-number">5</span>])<br>    d2l.plt.plot(x_train, y_train, <span class="hljs-string">'o'</span>, alpha=<span class="hljs-number">0.5</span>);<br><br>y_hat = torch.repeat_interleave(y_train.mean(), n_test)<br>plot_kernel_reg(y_hat)<br></code></pre></td></tr></tbody></table></figure>

<h3 id="éå‚æ•°æ³¨æ„åŠ›æ±‡èš"><a href="#éå‚æ•°æ³¨æ„åŠ›æ±‡èš" class="headerlink" title="éå‚æ•°æ³¨æ„åŠ›æ±‡èš"></a>éå‚æ•°æ³¨æ„åŠ›æ±‡èš</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">X_repeat = x_test.repeat_interleave(n_train).reshape((-<span class="hljs-number">1</span>, n_train))<br>attention_weights = nn.functional.softmax(-(X_repeat - x_train)**<span class="hljs-number">2</span> / <span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>)<br>y_hat = torch.matmul(attention_weights, y_train)<br>plot_kernel_reg(y_hat)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>éå‚çš„å¥½å¤„ï¼Œç±»ä¼¼äºknnï¼Œä¸éœ€è¦å­¦ä¹ æï¼</p>
</blockquote>
<h3 id="çƒ­åŠ›å›¾åº·åº·æ³¨æ„åŠ›æƒé‡"><a href="#çƒ­åŠ›å›¾åº·åº·æ³¨æ„åŠ›æƒé‡" class="headerlink" title="çƒ­åŠ›å›¾åº·åº·æ³¨æ„åŠ›æƒé‡"></a>çƒ­åŠ›å›¾åº·åº·æ³¨æ„åŠ›æƒé‡</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">d2l.show_heatmaps(<br>    attention_weights.unsqueeze(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>),<br>    xlabel=<span class="hljs-string">'Sorted training inputs'</span>, ylabel=<span class="hljs-string">'Sorted testing inputs'</span>)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>å¸¦å‚æ•°æ³¨æ„åŠ›æ±‡èšå‡å®šä¸¤ä¸ªå¼ é‡çš„å½¢çŠ¶åˆ†åˆ«æ˜¯ (ğ‘›,ğ‘,ğ‘)å’Œ (ğ‘›,ğ‘,ğ‘)ï¼Œå®ƒä»¬çš„æ‰¹é‡çŸ©é˜µä¹˜æ³•è¾“å‡ºçš„å½¢çŠ¶ä¸º (ğ‘›,ğ‘,ğ‘)</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>))<br>Y = torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>))<br>torch.bmm(X, Y).shape<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<p>ä½¿ç”¨å°æ‰¹é‡çŸ©é˜µä¹˜æ³•æ¥è®¡ç®—å°æ‰¹é‡æ•°æ®ä¸­çš„åŠ æƒå¹³å‡å€¼</p>
</blockquote>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">weights = torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">10</span>)) * <span class="hljs-number">0.1</span><br>values = torch.arange(<span class="hljs-number">20.0</span>).reshape((<span class="hljs-number">2</span>, <span class="hljs-number">10</span>))<br>torch.bmm(weights.unsqueeze(<span class="hljs-number">1</span>), values.unsqueeze(-<span class="hljs-number">1</span>))<br></code></pre></td></tr></tbody></table></figure>

<h3 id="å¸¦å‚æ•°çš„æ³¨æ„åŠ›æ±‡èš"><a href="#å¸¦å‚æ•°çš„æ³¨æ„åŠ›æ±‡èš" class="headerlink" title="å¸¦å‚æ•°çš„æ³¨æ„åŠ›æ±‡èš"></a>å¸¦å‚æ•°çš„æ³¨æ„åŠ›æ±‡èš</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">NWKernelRegression</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>().__init__(**kwargs)<br>        self.w = nn.Parameter(torch.rand((<span class="hljs-number">1</span>,), requires_grad=<span class="hljs-literal">True</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, queries, keys, values</span>):<br>        queries = queries.repeat_interleave(keys.shape[<span class="hljs-number">1</span>]).reshape(<br>            (-<span class="hljs-number">1</span>, keys.shape[<span class="hljs-number">1</span>]))<br>        self.attention_weights = nn.functional.softmax(<br>            -((queries - keys) * self.w)**<span class="hljs-number">2</span> / <span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> torch.bmm(self.attention_weights.unsqueeze(<span class="hljs-number">1</span>),<br>                         values.unsqueeze(-<span class="hljs-number">1</span>)).reshape(-<span class="hljs-number">1</span>)<br></code></pre></td></tr></tbody></table></figure>

<h3 id="å°†è®­ç»ƒæ•°æ®é›†è½¬æ¢ä¸ºé”®å’Œå€¼"><a href="#å°†è®­ç»ƒæ•°æ®é›†è½¬æ¢ä¸ºé”®å’Œå€¼" class="headerlink" title="å°†è®­ç»ƒæ•°æ®é›†è½¬æ¢ä¸ºé”®å’Œå€¼"></a>å°†è®­ç»ƒæ•°æ®é›†è½¬æ¢ä¸ºé”®å’Œå€¼</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">X_tile = x_train.repeat((n_train, <span class="hljs-number">1</span>))<br>Y_tile = y_train.repeat((n_train, <span class="hljs-number">1</span>))<br>keys = X_tile[(<span class="hljs-number">1</span> - torch.eye(n_train)).<span class="hljs-built_in">type</span>(torch.<span class="hljs-built_in">bool</span>)].reshape(<br>    (n_train, -<span class="hljs-number">1</span>))<br>values = Y_tile[(<span class="hljs-number">1</span> - torch.eye(n_train)).<span class="hljs-built_in">type</span>(torch.<span class="hljs-built_in">bool</span>)].reshape(<br>    (n_train, -<span class="hljs-number">1</span>))<br></code></pre></td></tr></tbody></table></figure>

<h3 id="è®­ç»ƒå¸¦å‚æ•°çš„æ³¨æ„åŠ›æ±‡èšæ¨¡å‹"><a href="#è®­ç»ƒå¸¦å‚æ•°çš„æ³¨æ„åŠ›æ±‡èšæ¨¡å‹" class="headerlink" title="è®­ç»ƒå¸¦å‚æ•°çš„æ³¨æ„åŠ›æ±‡èšæ¨¡å‹"></a>è®­ç»ƒå¸¦å‚æ•°çš„æ³¨æ„åŠ›æ±‡èšæ¨¡å‹</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">net = NWKernelRegression()<br>loss = nn.MSELoss(reduction=<span class="hljs-string">'none'</span>)<br>trainer = torch.optim.SGD(net.parameters(), lr=<span class="hljs-number">0.5</span>)<br>animator = d2l.Animator(xlabel=<span class="hljs-string">'epoch'</span>, ylabel=<span class="hljs-string">'loss'</span>, xlim=[<span class="hljs-number">1</span>, <span class="hljs-number">5</span>])<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>):<br>    trainer.zero_grad()<br>    l = loss(net(x_train, keys, values), y_train) / <span class="hljs-number">2</span><br>    l.<span class="hljs-built_in">sum</span>().backward()<br>    trainer.step()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'epoch <span class="hljs-subst">{epoch + <span class="hljs-number">1</span>}</span>, loss <span class="hljs-subst">{<span class="hljs-built_in">float</span>(l.<span class="hljs-built_in">sum</span>()):<span class="hljs-number">.6</span>f}</span>'</span>)<br>    animator.add(epoch + <span class="hljs-number">1</span>, <span class="hljs-built_in">float</span>(l.<span class="hljs-built_in">sum</span>()))<br></code></pre></td></tr></tbody></table></figure>

<h3 id="é¢„æµ‹ç»“æœç»˜åˆ¶"><a href="#é¢„æµ‹ç»“æœç»˜åˆ¶" class="headerlink" title="é¢„æµ‹ç»“æœç»˜åˆ¶"></a>é¢„æµ‹ç»“æœç»˜åˆ¶</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">keys = x_train.repeat((n_test, <span class="hljs-number">1</span>))<br>values = y_train.repeat((n_test, <span class="hljs-number">1</span>))<br>y_hat = net(x_test, keys, values).unsqueeze(<span class="hljs-number">1</span>).detach()<br>plot_kernel_reg(y_hat)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>æ›²çº¿åœ¨æ³¨æ„åŠ›æƒé‡è¾ƒå¤§çš„åŒºåŸŸå˜å¾—æ›´ä¸å¹³æ»‘</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">d2l.show_heatmaps(<br>    net.attention_weights.unsqueeze(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>),<br>    xlabel=<span class="hljs-string">'Sorted training inputs'</span>, ylabel=<span class="hljs-string">'Sorted testing inputs'</span>)<br></code></pre></td></tr></tbody></table></figure>





<h2 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h2><ul>
<li>å¿ƒç†å­¦è®¤ä¸ºäººé€šè¿‡éšæ„çº¿ç´¢å’Œä¸éšæ„çº¿ç´¢é€‰æ‹©æ³¨æ„ç‚¹</li>
<li>æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œé€šè¿‡query (éšæ„çº¿ç´¢)å’Œkey (ä¸éšæ„çº¿ç´¢)æ¥æœ‰åå‘æ€§çš„é€‰æ‹©è¾“å…¥<ul>
<li>å¯ä»¥ä¸€èˆ¬çš„å†™ä½œ$f(x) = \sum_{i=1} \alpha(x, x_i)y_i$è¿™é‡Œ$\alpha(x, x_i)$æ˜¯æ³¨æ„åŠ›æƒé‡</li>
<li>æ—©åœ¨60å¹´ä»£å°±æœ‰éå‚æ•°çš„æ³¨æ„åŠ›æœºåˆ¶</li>
<li>æ¥ä¸‹æ¥æˆ‘ä»¬ä¼šä»‹ç»å¤šä¸ªä¸åŒçš„æƒé‡è®¾è®¡</li>
</ul>
</li>
</ul>
<h1 id="æ³¨æ„åŠ›åˆ†æ•°"><a href="#æ³¨æ„åŠ›åˆ†æ•°" class="headerlink" title="æ³¨æ„åŠ›åˆ†æ•°"></a>æ³¨æ„åŠ›åˆ†æ•°</h1><h2 id="æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°"><a href="#æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°" class="headerlink" title="æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°"></a>æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°</h2><ul>
<li>ç”±äºæ³¨æ„åŠ›æƒé‡æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œ å› æ­¤åŠ æƒå’Œå…¶æœ¬è´¨ä¸Šæ˜¯åŠ æƒå¹³å‡å€¼ã€‚</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308161931710.svg" srcset="/img/loading.gif" lazyload alt="../_images/attention-output.svg"></p>
<ul>
<li>å‡è®¾æœ‰ä¸€ä¸ªæŸ¥è¯¢$\mathbf{q} \in \mathbb{R}^q$å’Œ$m$ä¸ªâ€œé”®ï¼å€¼â€å¯¹$(\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)$ï¼Œ å…¶ä¸­$\mathbf{k}_i \in \mathbb{R}^k, \mathbf{v}_i \in \mathbb{R}^v$ã€‚ æ³¨æ„åŠ›æ±‡èšå‡½æ•°få°±è¢«è¡¨ç¤ºæˆå€¼çš„åŠ æƒå’Œï¼š</li>
</ul>
<p>$$<br>f(\mathbf{q}, (\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}<em>m)) = \sum</em>{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i \in \mathbb{R}^v,<br>$$</p>
<ul>
<li>å…¶ä¸­æŸ¥è¯¢$q$å’Œé”®$\mathbf{k}_i$çš„æ³¨æ„åŠ›æƒé‡ï¼ˆæ ‡é‡ï¼‰ æ˜¯é€šè¿‡æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°$\alpha$å°†ä¸¤ä¸ªå‘é‡æ˜ å°„æˆæ ‡é‡ï¼Œ å†ç»è¿‡softmaxè¿ç®—å¾—åˆ°çš„ï¼š</li>
</ul>
<p>$$<br>\alpha(\mathbf{q}, \mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q}, \mathbf{k}_i)) = \frac{\exp(a(\mathbf{q}, \mathbf{k}<em>i))}{\sum</em>{j=1}^m \exp(a(\mathbf{q}, \mathbf{k}_j))} \in \mathbb{R}.<br>$$</p>
<h2 id="åŠ æ€§æ³¨æ„åŠ›"><a href="#åŠ æ€§æ³¨æ„åŠ›" class="headerlink" title="åŠ æ€§æ³¨æ„åŠ›"></a>åŠ æ€§æ³¨æ„åŠ›</h2><ul>
<li>å½“æŸ¥è¯¢å’Œé”®æ˜¯ä¸åŒé•¿åº¦çš„çŸ¢é‡æ—¶ï¼Œå¯ä»¥ä½¿ç”¨åŠ æ€§æ³¨æ„åŠ›ä½œä¸ºè¯„åˆ†å‡½æ•°ã€‚ ç»™å®šæŸ¥è¯¢$\mathbf{q} \in \mathbb{R}^q$å’Œé”®$\mathbf{k} \in \mathbb{R}^k$ï¼Œ <em>åŠ æ€§æ³¨æ„åŠ›</em>ï¼ˆadditive attentionï¼‰çš„è¯„åˆ†å‡½æ•°ä¸º:</li>
</ul>
<p>$$<br>a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R},<br>$$</p>
<blockquote>
<p>å…¶ä¸­å¯å­¦ä¹ çš„å‚æ•°æ˜¯$\mathbf W_q\in\mathbb R^{h\times q}$ã€$\mathbf W_k\in\mathbb R^{h\times k}$å’Œ$\mathbf w_v\in\mathbb R^{h}$ã€‚ å¦‚ <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html#equation-eq-additive-attn">(10.3.3)</a>æ‰€ç¤ºï¼Œ å°†æŸ¥è¯¢å’Œé”®è¿ç»“èµ·æ¥åè¾“å…¥åˆ°ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ä¸­ï¼Œ æ„ŸçŸ¥æœºåŒ…å«ä¸€ä¸ªéšè—å±‚ï¼Œå…¶éšè—å•å…ƒæ•°æ˜¯ä¸€ä¸ªè¶…å‚æ•°hã€‚ é€šè¿‡ä½¿ç”¨tanhä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œå¹¶ä¸”ç¦ç”¨åç½®é¡¹ã€‚</p>
<p><strong>ç­‰ä»·äºå°†keyå’Œvalueåˆå¹¶èµ·æ¥ï¼Œæ”¾å…¥åˆ°ä¸€ä¸ªéšè—å¤§å°ä¸ºhï¼Œè¾“å‡ºä¸º1çš„å•éšè—å±‚MLP</strong></p>
</blockquote>
<h2 id="ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"><a href="#ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›" class="headerlink" title="ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"></a>ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›</h2><ul>
<li>ä½¿ç”¨ç‚¹ç§¯å¯ä»¥å¾—åˆ°è®¡ç®—æ•ˆç‡æ›´é«˜çš„è¯„åˆ†å‡½æ•°ï¼Œ ä½†æ˜¯ç‚¹ç§¯æ“ä½œè¦æ±‚æŸ¥è¯¢å’Œé”®å…·æœ‰ç›¸åŒçš„é•¿åº¦dã€‚ å‡è®¾æŸ¥è¯¢å’Œé”®çš„æ‰€æœ‰å…ƒç´ éƒ½æ˜¯ç‹¬ç«‹çš„éšæœºå˜é‡ï¼Œ å¹¶ä¸”éƒ½æ»¡è¶³é›¶å‡å€¼å’Œå•ä½æ–¹å·®ï¼Œ é‚£ä¹ˆä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯çš„å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸ºdã€‚ ä¸ºç¡®ä¿æ— è®ºå‘é‡é•¿åº¦å¦‚ä½•ï¼Œ ç‚¹ç§¯çš„æ–¹å·®åœ¨ä¸è€ƒè™‘å‘é‡é•¿åº¦çš„æƒ…å†µä¸‹ä»ç„¶æ˜¯1ï¼Œ æˆ‘ä»¬å†å°†ç‚¹ç§¯é™¤ä»¥$\sqrt{d}$ï¼Œ åˆ™<em>ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›</em>ï¼ˆscaled dot-product attentionï¼‰è¯„åˆ†å‡½æ•°ä¸ºï¼š</li>
</ul>
<p>$$<br>a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}  /\sqrt{d}.<br>$$</p>
<blockquote>
<p>ç”±äºqå’Œkçš„shapeä¸€è‡´ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯åšä¸ªå†…ç§¯ï¼å¾ˆæ–¹ä¾¿æ˜‚ï¼</p>
</blockquote>
<ul>
<li>åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä»å°æ‰¹é‡çš„è§’åº¦æ¥è€ƒè™‘æé«˜æ•ˆç‡ï¼Œ ä¾‹å¦‚åŸºäºnä¸ªæŸ¥è¯¢å’Œmä¸ªé”®ï¼å€¼å¯¹è®¡ç®—æ³¨æ„åŠ›ï¼Œ å…¶ä¸­æŸ¥è¯¢å’Œé”®çš„é•¿åº¦ä¸ºdï¼Œå€¼çš„é•¿åº¦ä¸ºvã€‚ æŸ¥è¯¢$\mathbf Q\in\mathbb R^{n\times d}$ã€ é”®$\mathbf K\in\mathbb R^{m\times d}$å’Œå€¼$\mathbf V\in\mathbb R^{m\times v}$çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æ˜¯ï¼š</li>
</ul>
<p>$$<br>\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}.<br>$$</p>
<h2 id="æ€»ç»“-1"><a href="#æ€»ç»“-1" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h2><ul>
<li>æ³¨æ„åŠ›åˆ†æ•°æ˜¯queryå’Œkeyçš„ç›¸ä¼¼åº¦ï¼Œæ³¨æ„åŠ›æƒé‡æ˜¯åˆ†æ•°çš„softmaxç»“æœã€‚</li>
<li>ä¸¤ç§å¸¸è§çš„åˆ†æ•°è®¡ç®—ï¼š<ul>
<li>å°†queryå’Œkeyåˆå¹¶èµ·æ¥è¿›å…¥ä¸€ä¸ªå•è¾“å‡ºå•éšè—å±‚çš„MLP</li>
<li>ç›´æ¥å°†queryå’Œkeyåšå†…ç§¯</li>
</ul>
</li>
</ul>
<h2 id="ä»£ç -1"><a href="#ä»£ç -1" class="headerlink" title="ä»£ç "></a>ä»£ç </h2><h3 id="Dependencies-1"><a href="#Dependencies-1" class="headerlink" title="Dependencies"></a>Dependencies</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br></code></pre></td></tr></tbody></table></figure>



<h3 id="æ©è”½softmaxæ“ä½œ"><a href="#æ©è”½softmaxæ“ä½œ" class="headerlink" title="æ©è”½softmaxæ“ä½œ"></a>æ©è”½softmaxæ“ä½œ</h3><ul>
<li>softmaxæ“ä½œç”¨äºè¾“å‡ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒä½œä¸ºæ³¨æ„åŠ›æƒé‡ã€‚ åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¹¶éæ‰€æœ‰çš„å€¼éƒ½åº”è¯¥è¢«çº³å…¥åˆ°æ³¨æ„åŠ›æ±‡èšä¸­ã€‚ ä¾‹å¦‚ï¼Œä¸ºäº†åœ¨ <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html#sec-machine-translation">9.5èŠ‚</a>ä¸­é«˜æ•ˆå¤„ç†å°æ‰¹é‡æ•°æ®é›†ï¼Œ æŸäº›æ–‡æœ¬åºåˆ—è¢«å¡«å……äº†æ²¡æœ‰æ„ä¹‰çš„ç‰¹æ®Šè¯å…ƒã€‚ ä¸ºäº†ä»…å°†æœ‰æ„ä¹‰çš„è¯å…ƒä½œä¸ºå€¼æ¥è·å–æ³¨æ„åŠ›æ±‡èšï¼Œ å¯ä»¥æŒ‡å®šä¸€ä¸ªæœ‰æ•ˆåºåˆ—é•¿åº¦ï¼ˆå³è¯å…ƒçš„ä¸ªæ•°ï¼‰ï¼Œ ä»¥ä¾¿åœ¨è®¡ç®—softmaxæ—¶è¿‡æ»¤æ‰è¶…å‡ºæŒ‡å®šèŒƒå›´çš„ä½ç½®ã€‚ ä¸‹é¢çš„<code>masked_softmax</code>å‡½æ•° å®ç°äº†è¿™æ ·çš„<em>æ©è”½softmaxæ“ä½œ</em>ï¼ˆmasked softmax operationï¼‰ï¼Œ å…¶ä¸­ä»»ä½•è¶…å‡ºæœ‰æ•ˆé•¿åº¦çš„ä½ç½®éƒ½è¢«æ©è”½å¹¶ç½®ä¸º0ã€‚</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">masked_softmax</span>(<span class="hljs-params">X, valid_lens</span>):<br>    <span class="hljs-string">"""é€šè¿‡åœ¨æœ€åä¸€ä¸ªè½´ä¸Šæ©è”½å…ƒç´ æ¥æ‰§è¡Œsoftmaxæ“ä½œ"""</span><br>    <span class="hljs-comment"># X:3Då¼ é‡ï¼Œvalid_lens:1Dæˆ–2Då¼ é‡</span><br>    <span class="hljs-keyword">if</span> valid_lens <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-keyword">return</span> nn.functional.softmax(X, dim=-<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">else</span>:<br>        shape = X.shape<br>        <span class="hljs-keyword">if</span> valid_lens.dim() == <span class="hljs-number">1</span>:<br>            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="hljs-number">1</span>])<br>        <span class="hljs-keyword">else</span>:<br>            valid_lens = valid_lens.reshape(-<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># æœ€åä¸€è½´ä¸Šè¢«æ©è”½çš„å…ƒç´ ä½¿ç”¨ä¸€ä¸ªéå¸¸å¤§çš„è´Ÿå€¼æ›¿æ¢ï¼Œä»è€Œå…¶softmaxè¾“å‡ºä¸º0</span><br>        X = d2l.sequence_mask(X.reshape(-<span class="hljs-number">1</span>, shape[-<span class="hljs-number">1</span>]), valid_lens,<br>                              value=-<span class="hljs-number">1e6</span>)<br>        <span class="hljs-keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>ä¸ºäº†æ¼”ç¤ºæ­¤å‡½æ•°æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œ è€ƒè™‘ç”±ä¸¤ä¸ª2Ã—4çŸ©é˜µè¡¨ç¤ºçš„æ ·æœ¬ï¼Œ è¿™ä¸¤ä¸ªæ ·æœ¬çš„æœ‰æ•ˆé•¿åº¦åˆ†åˆ«ä¸º2å’Œ3ã€‚ ç»è¿‡æ©è”½softmaxæ“ä½œï¼Œè¶…å‡ºæœ‰æ•ˆé•¿åº¦çš„å€¼éƒ½è¢«æ©è”½ä¸º0ã€‚</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">masked_softmax(torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>), torch.tensor([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]))<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>ä¹Ÿå¯ä»¥ä½¿ç”¨äºŒç»´å¼ é‡ï¼Œä¸ºçŸ©é˜µæ ·æœ¬ä¸­çš„æ¯ä¸€è¡ŒæŒ‡å®šæœ‰æ•ˆé•¿åº¦ã€‚</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">masked_softmax(torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>), torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">4</span>]]))<br></code></pre></td></tr></tbody></table></figure>



<h3 id="åŠ æ€§æ³¨æ„åŠ›-1"><a href="#åŠ æ€§æ³¨æ„åŠ›-1" class="headerlink" title="åŠ æ€§æ³¨æ„åŠ›"></a>åŠ æ€§æ³¨æ„åŠ›</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">AdditiveAttention</span>(nn.Module):<br>    <span class="hljs-string">"""åŠ æ€§æ³¨æ„åŠ›"""</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, key_size, query_size, num_hiddens, dropout, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(AdditiveAttention, self).__init__(**kwargs)<br>        self.W_k = nn.Linear(key_size, num_hiddens, bias=<span class="hljs-literal">False</span>)<br>        self.W_q = nn.Linear(query_size, num_hiddens, bias=<span class="hljs-literal">False</span>)<br>        self.w_v = nn.Linear(num_hiddens, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        self.dropout = nn.Dropout(dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, queries, keys, values, valid_lens</span>):<br>        queries, keys = self.W_q(queries), self.W_k(keys)<br>        <span class="hljs-comment"># åœ¨ç»´åº¦æ‰©å±•åï¼Œ</span><br>        <span class="hljs-comment"># queriesçš„å½¢çŠ¶ï¼š(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°ï¼Œ1ï¼Œnum_hidden)</span><br>        <span class="hljs-comment"># keyçš„å½¢çŠ¶ï¼š(batch_sizeï¼Œ1ï¼Œâ€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œnum_hiddens)</span><br>        <span class="hljs-comment"># ä½¿ç”¨å¹¿æ’­æ–¹å¼è¿›è¡Œæ±‚å’Œ</span><br>        features = queries.unsqueeze(<span class="hljs-number">2</span>) + keys.unsqueeze(<span class="hljs-number">1</span>)<br>        features = torch.tanh(features)<br>        <span class="hljs-comment"># self.w_vä»…æœ‰ä¸€ä¸ªè¾“å‡ºï¼Œå› æ­¤ä»å½¢çŠ¶ä¸­ç§»é™¤æœ€åé‚£ä¸ªç»´åº¦ã€‚</span><br>        <span class="hljs-comment"># scoresçš„å½¢çŠ¶ï¼š(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°ï¼Œâ€œé”®-å€¼â€å¯¹çš„ä¸ªæ•°)</span><br>        scores = self.w_v(features).squeeze(-<span class="hljs-number">1</span>)<br>        self.attention_weights = masked_softmax(scores, valid_lens)<br>        <span class="hljs-comment"># valuesçš„å½¢çŠ¶ï¼š(batch_sizeï¼Œâ€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œå€¼çš„ç»´åº¦)</span><br>        <span class="hljs-keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)<br></code></pre></td></tr></tbody></table></figure>

<blockquote>
<ul>
<li>forwardé‡Œé¢æœ‰ç‚¹å¤æ‚ï¼š<ul>
<li>queriesçš„å½¢çŠ¶ï¼š(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°ï¼Œ1ï¼Œnum_hidden)</li>
<li>featuresçš„å½¢çŠ¶ï¼š(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°ï¼Œâ€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œnum_hidden)</li>
<li>scoresçš„å½¢çŠ¶ï¼š(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°ï¼Œâ€œé”®-å€¼â€å¯¹çš„ä¸ªæ•°)</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li>usage</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">queries, keys = torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, (<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">20</span>)), torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">10</span>, <span class="hljs-number">2</span>))<br><span class="hljs-comment"># valuesçš„å°æ‰¹é‡ï¼Œä¸¤ä¸ªå€¼çŸ©é˜µæ˜¯ç›¸åŒçš„</span><br>values = torch.arange(<span class="hljs-number">40</span>, dtype=torch.float32).reshape(<span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">4</span>).repeat(<br>    <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>valid_lens = torch.tensor([<span class="hljs-number">2</span>, <span class="hljs-number">6</span>])<br><br>attention = AdditiveAttention(key_size=<span class="hljs-number">2</span>, query_size=<span class="hljs-number">20</span>, num_hiddens=<span class="hljs-number">8</span>,<br>                              dropout=<span class="hljs-number">0.1</span>)<br>attention.<span class="hljs-built_in">eval</span>()<br>attention(queries, keys, values, valid_lens)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>plot</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">d2l.show_heatmaps(attention.attention_weights.reshape((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">10</span>)),<br>                  xlabel=<span class="hljs-string">'Keys'</span>, ylabel=<span class="hljs-string">'Queries'</span>)<br></code></pre></td></tr></tbody></table></figure>



<h3 id="ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›-1"><a href="#ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›-1" class="headerlink" title="ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"></a>ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DotProductAttention</span>(nn.Module):<br>    <span class="hljs-string">"""ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dropout, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(DotProductAttention, self).__init__(**kwargs)<br>        self.dropout = nn.Dropout(dropout)<br><br>    <span class="hljs-comment"># queriesçš„å½¢çŠ¶ï¼š(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°ï¼Œd)</span><br>    <span class="hljs-comment"># keysçš„å½¢çŠ¶ï¼š(batch_sizeï¼Œâ€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œd)</span><br>    <span class="hljs-comment"># valuesçš„å½¢çŠ¶ï¼š(batch_sizeï¼Œâ€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œå€¼çš„ç»´åº¦)</span><br>    <span class="hljs-comment"># valid_lensçš„å½¢çŠ¶:(batch_sizeï¼Œ)æˆ–è€…(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°)</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, queries, keys, values, valid_lens=<span class="hljs-literal">None</span></span>):<br>        d = queries.shape[-<span class="hljs-number">1</span>]<br>        <span class="hljs-comment"># è®¾ç½®transpose_b=Trueä¸ºäº†äº¤æ¢keysçš„æœ€åä¸¤ä¸ªç»´åº¦</span><br>        scores = torch.bmm(queries, keys.transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>)) / math.sqrt(d)<br>        self.attention_weights = masked_softmax(scores, valid_lens)<br>        <span class="hljs-keyword">return</span> torch.bmm(self.dropout(self.attention_weights), values)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>usage</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">queries = torch.normal(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, (<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>))<br>attention = DotProductAttention(dropout=<span class="hljs-number">0.5</span>)<br>attention.<span class="hljs-built_in">eval</span>()<br>attention(queries, keys, values, valid_lens)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>plot</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">d2l.show_heatmaps(attention.attention_weights.reshape((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">10</span>)),<br>                  xlabel=<span class="hljs-string">'Keys'</span>, ylabel=<span class="hljs-string">'Queries'</span>)<br></code></pre></td></tr></tbody></table></figure>



<h1 id="Bahdanau-æ³¨æ„åŠ›"><a href="#Bahdanau-æ³¨æ„åŠ›" class="headerlink" title="Bahdanau æ³¨æ„åŠ›"></a>Bahdanau æ³¨æ„åŠ›</h1><blockquote>
<p>åœ¨ä¸ºç»™å®šæ–‡æœ¬åºåˆ—ç”Ÿæˆæ‰‹å†™çš„æŒ‘æˆ˜ä¸­ï¼Œ Gravesè®¾è®¡äº†ä¸€ç§å¯å¾®æ³¨æ„åŠ›æ¨¡å‹ï¼Œ å°†æ–‡æœ¬å­—ç¬¦ä¸æ›´é•¿çš„ç¬”è¿¹å¯¹é½ï¼Œ å…¶ä¸­å¯¹é½æ–¹å¼ä»…å‘ä¸€ä¸ªæ–¹å‘ç§»åŠ¨ã€‚ å—å­¦ä¹ å¯¹é½æƒ³æ³•çš„å¯å‘ï¼ŒBahdanauç­‰äººæå‡ºäº†ä¸€ä¸ªæ²¡æœ‰ä¸¥æ ¼å•å‘å¯¹é½é™åˆ¶çš„ å¯å¾®æ³¨æ„åŠ›æ¨¡å‹ (<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id6">Bahdanau <em>et al.</em>, 2014</a>)ã€‚ åœ¨é¢„æµ‹è¯å…ƒæ—¶ï¼Œå¦‚æœä¸æ˜¯æ‰€æœ‰è¾“å…¥è¯å…ƒéƒ½ç›¸å…³ï¼Œæ¨¡å‹å°†ä»…å¯¹é½ï¼ˆæˆ–å‚ä¸ï¼‰è¾“å…¥åºåˆ—ä¸­ä¸å½“å‰é¢„æµ‹ç›¸å…³çš„éƒ¨åˆ†ã€‚è¿™æ˜¯é€šè¿‡å°†ä¸Šä¸‹æ–‡å˜é‡è§†ä¸ºæ³¨æ„åŠ›é›†ä¸­çš„è¾“å‡ºæ¥å®ç°çš„ã€‚</p>
</blockquote>
<h2 id="æ¨¡å‹"><a href="#æ¨¡å‹" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308162147999.svg" srcset="/img/loading.gif" lazyload alt="../_images/seq2seq-attention-details.svg"></p>
<ul>
<li>ç¼–ç å™¨æ¯æ¬¡çš„è¾“å‡ºä½œä¸ºkeyå’Œvalue</li>
<li>è§£ç å™¨RNNå¯¹äºä¸Šä¸€ä¸ªè¯çš„è¾“å‡ºæ˜¯query</li>
<li>æ³¨æ„åŠ›çš„è¾“å‡ºå’Œä¸‹ä¸€ä¸ªè¯çš„è¯åµŒå…¥åˆå¹¶è¿›å…¥è§£ç å™¨</li>
</ul>
<h2 id="æ€»ç»“-2"><a href="#æ€»ç»“-2" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h2><ul>
<li>Seq2Seqä¸­é€šè¿‡éšçŠ¶æ€åœ¨ç¼–ç å™¨å’Œè§£ç å™¨ä¸­ä¼ é€’ä¿¡æ¯ã€‚</li>
<li>æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æ ¹æ®è§£ç å™¨RNNçš„è¾“å‡ºæ¥åŒ¹é…åˆ°åˆé€‚çš„ç¼–ç å™¨RNNçš„è¾“å‡ºæ¥æ›´æœ‰æ•ˆçš„ä¼ é€’ä¿¡æ¯ã€‚</li>
</ul>
<h2 id="ä»£ç -2"><a href="#ä»£ç -2" class="headerlink" title="ä»£ç "></a>ä»£ç </h2><h3 id="Dependencies-2"><a href="#Dependencies-2" class="headerlink" title="Dependencies"></a>Dependencies</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br></code></pre></td></tr></tbody></table></figure>



<h3 id="æ³¨æ„åŠ›æœºåˆ¶è§£ç å™¨"><a href="#æ³¨æ„åŠ›æœºåˆ¶è§£ç å™¨" class="headerlink" title="æ³¨æ„åŠ›æœºåˆ¶è§£ç å™¨"></a>æ³¨æ„åŠ›æœºåˆ¶è§£ç å™¨</h3><ul>
<li>æ¥å£ï¼š</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">AttentionDecoder</span>(d2l.Decoder):<br>    <span class="hljs-string">"""å¸¦æœ‰æ³¨æ„åŠ›æœºåˆ¶è§£ç å™¨çš„åŸºæœ¬æ¥å£"""</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(AttentionDecoder, self).__init__(**kwargs)<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">attention_weights</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">raise</span> NotImplementedError<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li><p>åˆå§‹åŒ–è§£ç å™¨çš„çŠ¶æ€ï¼Œéœ€è¦ä¸‹é¢çš„è¾“å…¥ï¼š</p>
<ol>
<li>ç¼–ç å™¨åœ¨æ‰€æœ‰æ—¶é—´æ­¥çš„æœ€ç»ˆå±‚éšçŠ¶æ€ï¼Œå°†ä½œä¸ºæ³¨æ„åŠ›çš„é”®å’Œå€¼ï¼›</li>
<li>ä¸Šä¸€æ—¶é—´æ­¥çš„ç¼–ç å™¨å…¨å±‚éšçŠ¶æ€ï¼Œå°†ä½œä¸ºåˆå§‹åŒ–è§£ç å™¨çš„éšçŠ¶æ€ï¼›</li>
<li>ç¼–ç å™¨æœ‰æ•ˆé•¿åº¦ï¼ˆæ’é™¤åœ¨æ³¨æ„åŠ›æ± ä¸­å¡«å……è¯å…ƒï¼‰ã€‚</li>
</ol>
</li>
<li><p>åœ¨æ¯ä¸ªè§£ç æ—¶é—´æ­¥éª¤ä¸­ï¼Œè§£ç å™¨ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„æœ€ç»ˆå±‚éšçŠ¶æ€å°†ç”¨ä½œæŸ¥è¯¢ã€‚ å› æ­¤ï¼Œæ³¨æ„åŠ›è¾“å‡ºå’Œè¾“å…¥åµŒå…¥éƒ½è¿ç»“ä¸ºå¾ªç¯ç¥ç»ç½‘ç»œè§£ç å™¨çš„è¾“å…¥ã€‚</p>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Seq2SeqAttentionDecoder</span>(<span class="hljs-title class_ inherited__">AttentionDecoder</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, embed_size, num_hiddens, num_layers,</span><br><span class="hljs-params">                 dropout=<span class="hljs-number">0</span>, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(Seq2SeqAttentionDecoder, self).__init__(**kwargs)<br>        self.attention = d2l.AdditiveAttention(<br>            num_hiddens, num_hiddens, num_hiddens, dropout)<br>        self.embedding = nn.Embedding(vocab_size, embed_size)<br>        self.rnn = nn.GRU(<br>            embed_size + num_hiddens, num_hiddens, num_layers,<br>            dropout=dropout)<br>        self.dense = nn.Linear(num_hiddens, vocab_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_state</span>(<span class="hljs-params">self, enc_outputs, enc_valid_lens, *args</span>):<br>        <span class="hljs-comment"># outputsçš„å½¢çŠ¶ä¸º(batch_sizeï¼Œnum_stepsï¼Œnum_hiddens).</span><br>        <span class="hljs-comment"># hidden_stateçš„å½¢çŠ¶ä¸º(num_layersï¼Œbatch_sizeï¼Œnum_hiddens)</span><br>        outputs, hidden_state = enc_outputs<br>        <span class="hljs-keyword">return</span> (outputs.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>), hidden_state, enc_valid_lens)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, state</span>):<br>        <span class="hljs-comment"># enc_outputsçš„å½¢çŠ¶ä¸º(batch_size,num_steps,num_hiddens).</span><br>        <span class="hljs-comment"># hidden_stateçš„å½¢çŠ¶ä¸º(num_layers,batch_size,</span><br>        <span class="hljs-comment"># num_hiddens)</span><br>        enc_outputs, hidden_state, enc_valid_lens = state<br>        <span class="hljs-comment"># è¾“å‡ºXçš„å½¢çŠ¶ä¸º(num_steps,batch_size,embed_size)</span><br>        X = self.embedding(X).permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br>        outputs, self._attention_weights = [], []<br>        <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X:<br>            <span class="hljs-comment"># queryçš„å½¢çŠ¶ä¸º(batch_size,1,num_hiddens)</span><br>            query = torch.unsqueeze(hidden_state[-<span class="hljs-number">1</span>], dim=<span class="hljs-number">1</span>)<br>            <span class="hljs-comment"># contextçš„å½¢çŠ¶ä¸º(batch_size,1,num_hiddens)</span><br>            context = self.attention(<br>                query, enc_outputs, enc_outputs, enc_valid_lens)<br>            <span class="hljs-comment"># åœ¨ç‰¹å¾ç»´åº¦ä¸Šè¿ç»“</span><br>            x = torch.cat((context, torch.unsqueeze(x, dim=<span class="hljs-number">1</span>)), dim=-<span class="hljs-number">1</span>)<br>            <span class="hljs-comment"># å°†xå˜å½¢ä¸º(1,batch_size,embed_size+num_hiddens)</span><br>            out, hidden_state = self.rnn(x.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>), hidden_state)<br>            outputs.append(out)<br>            self._attention_weights.append(self.attention.attention_weights)<br>        <span class="hljs-comment"># å…¨è¿æ¥å±‚å˜æ¢åï¼Œoutputsçš„å½¢çŠ¶ä¸º</span><br>        <span class="hljs-comment"># (num_steps,batch_size,vocab_size)</span><br>        outputs = self.dense(torch.cat(outputs, dim=<span class="hljs-number">0</span>))<br>        <span class="hljs-keyword">return</span> outputs.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>), [enc_outputs, hidden_state,<br>                                          enc_valid_lens]<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">attention_weights</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self._attention_weights<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>ä½¿ç”¨åŒ…å«7ä¸ªæ—¶é—´æ­¥çš„4ä¸ªåºåˆ—è¾“å…¥çš„å°æ‰¹é‡æµ‹è¯•Bahdanauæ³¨æ„åŠ›è§£ç å™¨ã€‚</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">encoder = d2l.Seq2SeqEncoder(vocab_size=<span class="hljs-number">10</span>, embed_size=<span class="hljs-number">8</span>, num_hiddens=<span class="hljs-number">16</span>,<br>                             num_layers=<span class="hljs-number">2</span>)<br>encoder.<span class="hljs-built_in">eval</span>()<br>decoder = Seq2SeqAttentionDecoder(vocab_size=<span class="hljs-number">10</span>, embed_size=<span class="hljs-number">8</span>, num_hiddens=<span class="hljs-number">16</span>,<br>                                  num_layers=<span class="hljs-number">2</span>)<br>decoder.<span class="hljs-built_in">eval</span>()<br>X = torch.zeros((<span class="hljs-number">4</span>, <span class="hljs-number">7</span>), dtype=torch.long)  <span class="hljs-comment"># (batch_size,num_steps)</span><br>state = decoder.init_state(encoder(X), <span class="hljs-literal">None</span>)<br>output, state = decoder(X, state)<br>output.shape, <span class="hljs-built_in">len</span>(state), state[<span class="hljs-number">0</span>].shape, <span class="hljs-built_in">len</span>(state[<span class="hljs-number">1</span>]), state[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>].shape<br></code></pre></td></tr></tbody></table></figure>





<h3 id="è®­ç»ƒ"><a href="#è®­ç»ƒ" class="headerlink" title="è®­ç»ƒ"></a>è®­ç»ƒ</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">embed_size, num_hiddens, num_layers, dropout = <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0.1</span><br>batch_size, num_steps = <span class="hljs-number">64</span>, <span class="hljs-number">10</span><br>lr, num_epochs, device = <span class="hljs-number">0.005</span>, <span class="hljs-number">250</span>, d2l.try_gpu()<br><br>train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)<br>encoder = d2l.Seq2SeqEncoder(<br>    <span class="hljs-built_in">len</span>(src_vocab), embed_size, num_hiddens, num_layers, dropout)<br>decoder = Seq2SeqAttentionDecoder(<br>    <span class="hljs-built_in">len</span>(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)<br>net = d2l.EncoderDecoder(encoder, decoder)<br>d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>æ¨¡å‹è®­ç»ƒåï¼Œæˆ‘ä»¬ç”¨å®ƒå°†å‡ ä¸ªè‹±è¯­å¥å­ç¿»è¯‘æˆæ³•è¯­å¹¶è®¡ç®—å®ƒä»¬çš„BLEUåˆ†æ•°ã€‚</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">engs = [<span class="hljs-string">'go .'</span>, <span class="hljs-string">"i lost ."</span>, <span class="hljs-string">'he\'s calm .'</span>, <span class="hljs-string">'i\'m home .'</span>]<br>fras = [<span class="hljs-string">'va !'</span>, <span class="hljs-string">'j\'ai perdu .'</span>, <span class="hljs-string">'il est calme .'</span>, <span class="hljs-string">'je suis chez moi .'</span>]<br><span class="hljs-keyword">for</span> eng, fra <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(engs, fras):<br>    translation, dec_attention_weight_seq = d2l.predict_seq2seq(<br>        net, eng, src_vocab, tgt_vocab, num_steps, device, <span class="hljs-literal">True</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'<span class="hljs-subst">{eng}</span> =&gt; <span class="hljs-subst">{translation}</span>, '</span>,<br>          <span class="hljs-string">f'bleu <span class="hljs-subst">{d2l.bleu(translation, fra, k=<span class="hljs-number">2</span>):<span class="hljs-number">.3</span>f}</span>'</span>)<br></code></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">attention_weights = torch.cat([step[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>][<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> dec_attention_weight_seq], <span class="hljs-number">0</span>).reshape((<br>    <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, num_steps))<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>è®­ç»ƒç»“æŸåï¼Œä¸‹é¢é€šè¿‡å¯è§†åŒ–æ³¨æ„åŠ›æƒé‡ ä¼šå‘ç°ï¼Œæ¯ä¸ªæŸ¥è¯¢éƒ½ä¼šåœ¨é”®å€¼å¯¹ä¸Šåˆ†é…ä¸åŒçš„æƒé‡ï¼Œè¿™è¯´æ˜ åœ¨æ¯ä¸ªè§£ç æ­¥ä¸­ï¼Œè¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†è¢«é€‰æ‹©æ€§åœ°èšé›†åœ¨æ³¨æ„åŠ›æ± ä¸­ã€‚</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># åŠ ä¸Šä¸€ä¸ªåŒ…å«åºåˆ—ç»“æŸè¯å…ƒ</span><br>d2l.show_heatmaps(<br>    attention_weights[:, :, :, :<span class="hljs-built_in">len</span>(engs[-<span class="hljs-number">1</span>].split()) + <span class="hljs-number">1</span>].cpu(),<br>    xlabel=<span class="hljs-string">'Key positions'</span>, ylabel=<span class="hljs-string">'Query positions'</span>)<br></code></pre></td></tr></tbody></table></figure>







<h1 id="è‡ªæ³¨æ„åŠ›å’Œä½ç½®ç¼–ç "><a href="#è‡ªæ³¨æ„åŠ›å’Œä½ç½®ç¼–ç " class="headerlink" title="è‡ªæ³¨æ„åŠ›å’Œä½ç½®ç¼–ç "></a>è‡ªæ³¨æ„åŠ›å’Œä½ç½®ç¼–ç </h1><h2 id="è‡ªæ³¨æ„åŠ›"><a href="#è‡ªæ³¨æ„åŠ›" class="headerlink" title="è‡ªæ³¨æ„åŠ›"></a>è‡ªæ³¨æ„åŠ›</h2><ul>
<li>ç»™å®šä¸€ä¸ªç”±è¯å…ƒç»„æˆçš„è¾“å…¥åºåˆ—$\mathbf{x}_1, \ldots, \mathbf{x}_n$ï¼Œ å…¶ä¸­ä»»æ„$\mathbf{x}_i \in \mathbb{R}^d\ (1 \leq i \leq n)$ã€‚ è¯¥åºåˆ—çš„è‡ªæ³¨æ„åŠ›è¾“å‡ºä¸ºä¸€ä¸ªé•¿åº¦ç›¸åŒçš„åºåˆ— $\mathbf{y}_1, \ldots, \mathbf{y}_n$ï¼Œå…¶ä¸­ï¼š</li>
</ul>
<p>$$<br>\mathbf{y}_i = f(\mathbf{x}_i, (\mathbf{x}_1, \mathbf{x}_1), \ldots, (\mathbf{x}_n, \mathbf{x}_n)) \in \mathbb{R}^d<br>$$</p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308171149320.svg" srcset="/img/loading.gif" lazyload alt="../_images/cnn-rnn-self-attention.svg"></p>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308171151483.png" srcset="/img/loading.gif" lazyload alt="image-20230817115110321"></p>
<blockquote>
<p>RNNçš„å¹¶è¡Œåº¦æ˜¯è¿™é‡Œæœ€å·®çš„æ˜‚ï¼</p>
</blockquote>
<h2 id="ä½ç½®ç¼–ç "><a href="#ä½ç½®ç¼–ç " class="headerlink" title="ä½ç½®ç¼–ç "></a>ä½ç½®ç¼–ç </h2><ul>
<li>åœ¨å¤„ç†è¯å…ƒåºåˆ—æ—¶ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œæ˜¯é€ä¸ªçš„é‡å¤åœ°å¤„ç†è¯å…ƒçš„ï¼Œ è€Œè‡ªæ³¨æ„åŠ›åˆ™å› ä¸ºå¹¶è¡Œè®¡ç®—è€Œæ”¾å¼ƒäº†é¡ºåºæ“ä½œã€‚ ä¸ºäº†ä½¿ç”¨åºåˆ—çš„é¡ºåºä¿¡æ¯ï¼Œé€šè¿‡åœ¨è¾“å…¥è¡¨ç¤ºä¸­æ·»åŠ  <em>ä½ç½®ç¼–ç </em>ï¼ˆpositional encodingï¼‰æ¥æ³¨å…¥ç»å¯¹çš„æˆ–ç›¸å¯¹çš„ä½ç½®ä¿¡æ¯ã€‚ ä½ç½®ç¼–ç å¯ä»¥é€šè¿‡å­¦ä¹ å¾—åˆ°ä¹Ÿå¯ä»¥ç›´æ¥å›ºå®šå¾—åˆ°ã€‚</li>
<li>å‡è®¾è¾“å…¥è¡¨ç¤º$\mathbf{X} \in \mathbb{R}^{n \times d}$åŒ…å«ä¸€ä¸ªåºåˆ—ä¸­nä¸ªè¯å…ƒçš„dç»´åµŒå…¥è¡¨ç¤ºã€‚ ä½ç½®ç¼–ç ä½¿ç”¨ç›¸åŒå½¢çŠ¶çš„ä½ç½®åµŒå…¥çŸ©é˜µ$\mathbf{P} \in \mathbb{R}^{n \times d}$è¾“å‡º$\mathbf{X} + \mathbf{P}$ï¼Œ çŸ©é˜µç¬¬iè¡Œã€ç¬¬2jåˆ—å’Œ2j+1åˆ—ä¸Šçš„å…ƒç´ ä¸ºï¼š</li>
</ul>
<p>$$<br>\begin{split}\begin{aligned} p_{i, 2j} &amp;= \sin\left(\frac{i}{10000^{2j/d}}\right),\p_{i, 2j+1} &amp;= \cos\left(\frac{i}{10000^{2j/d}}\right).\end{aligned}\end{split}<br>$$</p>
<ul>
<li>åœ¨ä½ç½®åµŒå…¥çŸ©é˜µ$P$ä¸­ï¼Œ è¡Œä»£è¡¨è¯å…ƒåœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼Œåˆ—ä»£è¡¨ä½ç½®ç¼–ç çš„ä¸åŒç»´åº¦ã€‚ ä»ä¸‹é¢çš„ä¾‹å­ä¸­å¯ä»¥çœ‹åˆ°ä½ç½®åµŒå…¥çŸ©é˜µçš„ç¬¬6åˆ—å’Œç¬¬7åˆ—çš„é¢‘ç‡é«˜äºç¬¬8åˆ—å’Œç¬¬9åˆ—ã€‚ ç¬¬6åˆ—å’Œç¬¬7åˆ—ä¹‹é—´çš„åç§»é‡ï¼ˆç¬¬8åˆ—å’Œç¬¬9åˆ—ç›¸åŒï¼‰æ˜¯ç”±äºæ­£å¼¦å‡½æ•°å’Œä½™å¼¦å‡½æ•°çš„äº¤æ›¿ã€‚</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308171554620.svg" srcset="/img/loading.gif" lazyload alt="../_images/output_self-attention-and-positional-encoding_d76d5a_52_0.svg"></p>
<blockquote>
<p>ç›´æ¥æŠŠä½ç½®ç¼–ç æ”¾å…¥æ•°æ®ï¼Œè€Œä¸æ˜¯åœ¨æ¨¡å‹è¿›è¡Œä¿®æ”¹ or å¤šåŠ ä¸€ä¸ªä½ç½®çš„ç»´åº¦å†concatåŸå§‹æ•°æ®ã€‚</p>
</blockquote>
<h3 id="ç»å¯¹ä½ç½®ä¿¡æ¯"><a href="#ç»å¯¹ä½ç½®ä¿¡æ¯" class="headerlink" title="ç»å¯¹ä½ç½®ä¿¡æ¯"></a>ç»å¯¹ä½ç½®ä¿¡æ¯</h3><ul>
<li>åœ¨äºŒè¿›åˆ¶è¡¨ç¤ºä¸­ï¼Œè¾ƒé«˜æ¯”ç‰¹ä½çš„äº¤æ›¿é¢‘ç‡ä½äºè¾ƒä½æ¯”ç‰¹ä½ï¼ˆä¾‹å¦‚00, 01, 10, 11ï¼Œé«˜ä½äº¤æ›¿ä¸€æ¬¡ï¼Œä½ä½äº¤æ›¿äº†ä¸¤æ¬¡ï¼‰ï¼Œ ä¸ä¸‹é¢çš„çƒ­å›¾æ‰€ç¤ºç›¸ä¼¼ï¼Œåªæ˜¯ä½ç½®ç¼–ç é€šè¿‡ä½¿ç”¨ä¸‰è§’å‡½æ•°åœ¨ç¼–ç ç»´åº¦ä¸Šé™ä½é¢‘ç‡ã€‚ ç”±äºè¾“å‡ºæ˜¯æµ®ç‚¹æ•°ï¼Œå› æ­¤æ­¤ç±»è¿ç»­è¡¨ç¤ºæ¯”äºŒè¿›åˆ¶è¡¨ç¤ºæ³•æ›´èŠ‚çœç©ºé—´ã€‚</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308171619542.svg" srcset="/img/loading.gif" lazyload alt="../_images/output_self-attention-and-positional-encoding_d76d5a_82_0.svg"></p>
<h3 id="ç›¸å¯¹ä½ç½®ä¿¡æ¯"><a href="#ç›¸å¯¹ä½ç½®ä¿¡æ¯" class="headerlink" title="ç›¸å¯¹ä½ç½®ä¿¡æ¯"></a>ç›¸å¯¹ä½ç½®ä¿¡æ¯</h3><ul>
<li>é™¤äº†æ•è·ç»å¯¹ä½ç½®ä¿¡æ¯ä¹‹å¤–ï¼Œä¸Šè¿°çš„ä½ç½®ç¼–ç è¿˜å…è®¸æ¨¡å‹å­¦ä¹ å¾—åˆ°è¾“å…¥åºåˆ—ä¸­ç›¸å¯¹ä½ç½®ä¿¡æ¯ã€‚ è¿™æ˜¯å› ä¸ºå¯¹äºä»»ä½•ç¡®å®šçš„ä½ç½®åç§»$\delta$ï¼Œä½ç½®$i + \delta$å¤„ çš„ä½ç½®ç¼–ç å¯ä»¥çº¿æ€§æŠ•å½±ä½ç½®$i$å¤„çš„ä½ç½®ç¼–ç æ¥è¡¨ç¤ºã€‚è¿™ç§æŠ•å½±çš„æ•°å­¦è§£é‡Šæ˜¯ï¼Œä»¤$\omega_j = 1/10000^{2j/d}$ï¼Œ å¯¹äºä»»ä½•ç¡®å®šçš„ä½ç½®åç§»$\delta$ï¼Œ <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html#equation-eq-positional-encoding-def">(10.6.2)</a>ä¸­çš„ä»»ä½•ä¸€å¯¹$(p_{i, 2j}, p_{i, 2j+1})$éƒ½å¯ä»¥çº¿æ€§æŠ•å½±åˆ°$(p_{i+\delta, 2j}, p_{i+\delta, 2j+1})$ï¼š</li>
</ul>
<p>$$<br>\begin{split}\begin{aligned}<br>&amp;\begin{bmatrix} \cos(\delta \omega_j) &amp; \sin(\delta \omega_j) \  -\sin(\delta \omega_j) &amp; \cos(\delta \omega_j) \ \end{bmatrix}<br>\begin{bmatrix} p_{i, 2j} \  p_{i, 2j+1} \ \end{bmatrix}\<br>=&amp;\begin{bmatrix} \cos(\delta \omega_j) \sin(i \omega_j) + \sin(\delta \omega_j) \cos(i \omega_j) \  -\sin(\delta \omega_j) \sin(i \omega_j) + \cos(\delta \omega_j) \cos(i \omega_j) \ \end{bmatrix}\<br>=&amp;\begin{bmatrix} \sin\left((i+\delta) \omega_j\right) \  \cos\left((i+\delta) \omega_j\right) \ \end{bmatrix}\<br>=&amp;<br>\begin{bmatrix} p_{i+\delta, 2j} \  p_{i+\delta, 2j+1} \ \end{bmatrix},<br>\end{aligned}\end{split}<br>$$</p>
<blockquote>
<p>2Ã—2æŠ•å½±çŸ©é˜µä¸ä¾èµ–äºä»»ä½•ä½ç½®çš„ç´¢å¼•iã€‚</p>
</blockquote>
<h2 id="ä»£ç å®ç°"><a href="#ä»£ç å®ç°" class="headerlink" title="ä»£ç å®ç°"></a>ä»£ç å®ç°</h2><h3 id="Dependencies-3"><a href="#Dependencies-3" class="headerlink" title="Dependencies"></a>Dependencies</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br></code></pre></td></tr></tbody></table></figure>



<h3 id="ä½ç½®ç¼–ç -1"><a href="#ä½ç½®ç¼–ç -1" class="headerlink" title="ä½ç½®ç¼–ç "></a>ä½ç½®ç¼–ç </h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PositionalEncoding</span>(nn.Module):<br>    <span class="hljs-string">"""ä½ç½®ç¼–ç """</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_hiddens, dropout, max_len=<span class="hljs-number">1000</span></span>):<br>        <span class="hljs-built_in">super</span>(PositionalEncoding, self).__init__()<br>        self.dropout = nn.Dropout(dropout)<br>        <span class="hljs-comment"># åˆ›å»ºä¸€ä¸ªè¶³å¤Ÿé•¿çš„P</span><br>        self.P = torch.zeros((<span class="hljs-number">1</span>, max_len, num_hiddens))<br>        X = torch.arange(max_len, dtype=torch.float32).reshape(<br>            -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>) / torch.<span class="hljs-built_in">pow</span>(<span class="hljs-number">10000</span>, torch.arange(<br>            <span class="hljs-number">0</span>, num_hiddens, <span class="hljs-number">2</span>, dtype=torch.float32) / num_hiddens)<br>        self.P[:, :, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>] = torch.sin(X)<br>        self.P[:, :, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>] = torch.cos(X)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        X = X + self.P[:, :X.shape[<span class="hljs-number">1</span>], :].to(X.device)<br>        <span class="hljs-keyword">return</span> self.dropout(X)<br></code></pre></td></tr></tbody></table></figure>



<h3 id="ç»˜åˆ¶ä½ç½®ç¼–ç ç¤ºæ„å›¾"><a href="#ç»˜åˆ¶ä½ç½®ç¼–ç ç¤ºæ„å›¾" class="headerlink" title="ç»˜åˆ¶ä½ç½®ç¼–ç ç¤ºæ„å›¾"></a>ç»˜åˆ¶ä½ç½®ç¼–ç ç¤ºæ„å›¾</h3><ul>
<li>è¡Œä»£è¡¨æ ‡è®°åœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼Œåˆ—ä»£è¡¨ä½ç½®ç¼–ç çš„ä¸åŒç»´åº¦</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">encoding_dim, num_steps = <span class="hljs-number">32</span>, <span class="hljs-number">60</span><br>pos_encoding = PositionalEncoding(encoding_dim, <span class="hljs-number">0</span>)<br>pos_encoding.<span class="hljs-built_in">eval</span>()<br>X = pos_encoding(torch.zeros((<span class="hljs-number">1</span>, num_steps, encoding_dim)))<br>P = pos_encoding.P[:, :X.shape[<span class="hljs-number">1</span>], :]<br>d2l.plot(torch.arange(num_steps), P[<span class="hljs-number">0</span>, :, <span class="hljs-number">6</span>:<span class="hljs-number">10</span>].T, xlabel=<span class="hljs-string">'Row (position)'</span>,<br>         figsize=(<span class="hljs-number">6</span>, <span class="hljs-number">2.5</span>), legend=[<span class="hljs-string">"Col %d"</span> % d <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> torch.arange(<span class="hljs-number">6</span>, <span class="hljs-number">10</span>)])<br></code></pre></td></tr></tbody></table></figure>



<h3 id="äºŒè¿›åˆ¶è¡¨ç¤º"><a href="#äºŒè¿›åˆ¶è¡¨ç¤º" class="headerlink" title="äºŒè¿›åˆ¶è¡¨ç¤º"></a>äºŒè¿›åˆ¶è¡¨ç¤º</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">8</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'<span class="hljs-subst">{i}</span> in binary is <span class="hljs-subst">{i:&gt;03b}</span>'</span>)<br></code></pre></td></tr></tbody></table></figure>



<h3 id="åœ¨ç¼–ç ç»´åº¦ä¸Šé™ä½é¢‘ç‡"><a href="#åœ¨ç¼–ç ç»´åº¦ä¸Šé™ä½é¢‘ç‡" class="headerlink" title="åœ¨ç¼–ç ç»´åº¦ä¸Šé™ä½é¢‘ç‡"></a>åœ¨ç¼–ç ç»´åº¦ä¸Šé™ä½é¢‘ç‡</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">P = P[<span class="hljs-number">0</span>, :, :].unsqueeze(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)<br>d2l.show_heatmaps(P, xlabel=<span class="hljs-string">'Column (encoding dimension)'</span>,<br>                  ylabel=<span class="hljs-string">'Row (position)'</span>, figsize=(<span class="hljs-number">3.5</span>, <span class="hljs-number">4</span>), cmap=<span class="hljs-string">'Blues'</span>)<br></code></pre></td></tr></tbody></table></figure>





<h1 id="å¤šå¤´æ³¨æ„åŠ›"><a href="#å¤šå¤´æ³¨æ„åŠ›" class="headerlink" title="å¤šå¤´æ³¨æ„åŠ›"></a>å¤šå¤´æ³¨æ„åŠ›</h1><ul>
<li>åœ¨å®è·µä¸­ï¼Œå½“ç»™å®šç›¸åŒçš„æŸ¥è¯¢ã€é”®å’Œå€¼çš„é›†åˆæ—¶ï¼Œ æˆ‘ä»¬å¸Œæœ›æ¨¡å‹å¯ä»¥åŸºäºç›¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ åˆ°ä¸åŒçš„è¡Œä¸ºï¼Œ ç„¶åå°†ä¸åŒçš„è¡Œä¸ºä½œä¸ºçŸ¥è¯†ç»„åˆèµ·æ¥ï¼Œ æ•è·åºåˆ—å†…å„ç§èŒƒå›´çš„ä¾èµ–å…³ç³» ï¼ˆä¾‹å¦‚ï¼ŒçŸ­è·ç¦»ä¾èµ–å’Œé•¿è·ç¦»ä¾èµ–å…³ç³»ï¼‰ã€‚ å› æ­¤ï¼Œå…è®¸æ³¨æ„åŠ›æœºåˆ¶ç»„åˆä½¿ç”¨æŸ¥è¯¢ã€é”®å’Œå€¼çš„ä¸åŒ<em>å­ç©ºé—´è¡¨ç¤º</em>ï¼ˆrepresentation subspacesï¼‰å¯èƒ½æ˜¯æœ‰ç›Šçš„ã€‚</li>
<li>ä¸ºæ­¤ï¼Œä¸å…¶åªä½¿ç”¨å•ç‹¬ä¸€ä¸ªæ³¨æ„åŠ›æ±‡èšï¼Œ æˆ‘ä»¬å¯ä»¥ç”¨ç‹¬ç«‹å­¦ä¹ å¾—åˆ°çš„hç»„ä¸åŒçš„ <em>çº¿æ€§æŠ•å½±</em>ï¼ˆlinear projectionsï¼‰æ¥å˜æ¢æŸ¥è¯¢ã€é”®å’Œå€¼ã€‚ ç„¶åï¼Œè¿™hç»„å˜æ¢åçš„æŸ¥è¯¢ã€é”®å’Œå€¼å°†å¹¶è¡Œåœ°é€åˆ°æ³¨æ„åŠ›æ±‡èšä¸­ã€‚ æœ€åï¼Œå°†è¿™hä¸ªæ³¨æ„åŠ›æ±‡èšçš„è¾“å‡ºæ‹¼æ¥åœ¨ä¸€èµ·ï¼Œ å¹¶ä¸”é€šè¿‡å¦ä¸€ä¸ªå¯ä»¥å­¦ä¹ çš„çº¿æ€§æŠ•å½±è¿›è¡Œå˜æ¢ï¼Œ ä»¥äº§ç”Ÿæœ€ç»ˆè¾“å‡ºã€‚ è¿™ç§è®¾è®¡è¢«ç§°ä¸º<em>å¤šå¤´æ³¨æ„åŠ›</em>ï¼ˆmultihead attentionï¼‰ (<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id174">Vaswani <em>et al.</em>, 2017</a>)ã€‚ å¯¹äºhä¸ªæ³¨æ„åŠ›æ±‡èšè¾“å‡ºï¼Œæ¯ä¸€ä¸ªæ³¨æ„åŠ›æ±‡èšéƒ½è¢«ç§°ä½œä¸€ä¸ª<em>å¤´</em>ï¼ˆheadï¼‰ã€‚ <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/multihead-attention.html#fig-multi-head-attention">å›¾10.5.1</a> å±•ç¤ºäº†ä½¿ç”¨å…¨è¿æ¥å±‚æ¥å®ç°å¯å­¦ä¹ çš„çº¿æ€§å˜æ¢çš„å¤šå¤´æ³¨æ„åŠ›ã€‚</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308171700733.svg" srcset="/img/loading.gif" lazyload alt="../_images/multi-head-attention.svg"></p>
<blockquote>
<p>è¯´ç™½äº†å°±æ˜¯æƒ³æŠ½å–ä¸åŒçš„ç‰¹å¾ï¼Œå­¦åˆ°æ›´å¤šçš„ä¸œè¥¿ã€‚å¯¹åŒä¸€ä¸ªkey, value, queryæŠ½å–ä¸åŒçš„ä¿¡æ¯ï¼Œä¾‹å¦‚çŸ­è·ç¦»å’Œé•¿è·ç¦»å…³ç³»ã€‚å¤šå¤´æ³¨æ„åŠ›ä½¿ç”¨hä¸ªç‹¬ç«‹çš„æ³¨æ„åŠ›æ± åŒ–ï¼Œåˆå¹¶å„ä¸ªå¤´å¹¶å¾—åˆ°æœ€ç»ˆè¾“å‡ºã€‚</p>
</blockquote>
<h2 id="æ¨¡å‹-1"><a href="#æ¨¡å‹-1" class="headerlink" title="æ¨¡å‹"></a>æ¨¡å‹</h2><ul>
<li>ç»™å®šæŸ¥è¯¢$\mathbf{q} \in \mathbb{R}^{d_q}$ã€ é”®$\mathbf{k} \in \mathbb{R}^{d_k}$å’Œå€¼$\mathbf{v} \in \mathbb{R}^{d_v}$ï¼Œ æ¯ä¸ªæ³¨æ„åŠ›å¤´$i = 1, \ldots, h$çš„è®¡ç®—æ–¹æ³•ä¸ºï¼š</li>
</ul>
<p>$$<br>\mathbf{h}_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v},<br>$$</p>
<ul>
<li>å…¶ä¸­ï¼Œå¯å­¦ä¹ çš„å‚æ•°åŒ…æ‹¬$\mathbf W_i^{(q)}\in\mathbb R^{p_q\times d_q}$ã€ $\mathbf W_i^{(k)}\in\mathbb R^{p_k\times d_k}$å’Œ$\mathbf W_i^{(v)}\in\mathbb R^{p_v\times d_v}$ï¼Œ ä»¥åŠä»£è¡¨æ³¨æ„åŠ›æ±‡èšçš„å‡½æ•°$f$ã€‚$f$å¯ä»¥æ˜¯ <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/attention-scoring-functions.html#sec-attention-scoring-functions">10.3èŠ‚</a>ä¸­çš„ åŠ æ€§æ³¨æ„åŠ›å’Œç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ã€‚ å¤šå¤´æ³¨æ„åŠ›çš„è¾“å‡ºéœ€è¦ç»è¿‡å¦ä¸€ä¸ªçº¿æ€§è½¬æ¢ï¼Œ å®ƒå¯¹åº”ç€â„ä¸ªå¤´è¿ç»“åçš„ç»“æœï¼Œå› æ­¤å…¶å¯å­¦ä¹ å‚æ•°æ˜¯ $\mathbf W_o\in\mathbb R^{p_o\times h p_v}$ï¼š</li>
</ul>
<p>$$<br>\begin{split}\mathbf W_o \begin{bmatrix}\mathbf h_1\\vdots\\mathbf h_h\end{bmatrix} \in \mathbb{R}^{p_o}.\end{split}<br>$$</p>
<blockquote>
<p>åŸºäºè¿™ç§è®¾è®¡ï¼Œæ¯ä¸ªå¤´éƒ½å¯èƒ½ä¼šå…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ï¼Œ å¯ä»¥è¡¨ç¤ºæ¯”ç®€å•åŠ æƒå¹³å‡å€¼æ›´å¤æ‚çš„å‡½æ•°ã€‚</p>
</blockquote>
<h2 id="æœ‰æ©ç çš„å¤šå¤´æ³¨æ„åŠ›"><a href="#æœ‰æ©ç çš„å¤šå¤´æ³¨æ„åŠ›" class="headerlink" title="æœ‰æ©ç çš„å¤šå¤´æ³¨æ„åŠ›"></a>æœ‰æ©ç çš„å¤šå¤´æ³¨æ„åŠ›</h2><ul>
<li>è§£ç å™¨åœ¨å¯¹äºåºåˆ—ä¸­çš„å…ƒç´ è¾“å‡ºæ—¶ï¼Œåªè€ƒè™‘ä¹‹å‰çš„å…ƒç´ ï¼Œä¸è€ƒè™‘ä¹‹åçš„å…ƒç´ ã€‚</li>
<li>å¯ä»¥é€šè¿‡æ©ç æ¥å®ç°ï¼šè®¡ç®—$x_i$çš„æ—¶å€™ï¼Œå‡è£…å½“å‰çš„åºåˆ—é•¿åº¦ä¸ºi</li>
</ul>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><ul>
<li><p>ä»å®è§‚è§’åº¦æ¥çœ‹ï¼ŒTransformerçš„ç¼–ç å™¨æ˜¯ç”±å¤šä¸ªç›¸åŒçš„å±‚å åŠ è€Œæˆçš„ï¼Œæ¯ä¸ªå±‚éƒ½æœ‰ä¸¤ä¸ªå­å±‚ï¼ˆå­å±‚è¡¨ç¤ºä¸ºsublayerï¼‰ã€‚ç¬¬ä¸€ä¸ªå­å±‚æ˜¯<em>å¤šå¤´è‡ªæ³¨æ„åŠ›</em>ï¼ˆmulti-head self-attentionï¼‰æ±‡èšï¼›ç¬¬äºŒä¸ªå­å±‚æ˜¯<em>åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ</em>ï¼ˆpositionwise feed-forward networkï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨è®¡ç®—ç¼–ç å™¨çš„è‡ªæ³¨æ„åŠ›æ—¶ï¼ŒæŸ¥è¯¢ã€é”®å’Œå€¼éƒ½æ¥è‡ªå‰ä¸€ä¸ªç¼–ç å™¨å±‚çš„è¾“å‡ºã€‚å— <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_convolutional-modern/resnet.html#sec-resnet">7.6èŠ‚</a>ä¸­æ®‹å·®ç½‘ç»œçš„å¯å‘ï¼Œæ¯ä¸ªå­å±‚éƒ½é‡‡ç”¨äº†<em>æ®‹å·®è¿æ¥</em>ï¼ˆresidual connectionï¼‰ã€‚åœ¨Transformerä¸­ï¼Œå¯¹äºåºåˆ—ä¸­ä»»ä½•ä½ç½®çš„ä»»ä½•è¾“å…¥$\mathbf{x} \in \mathbb{R}^d$ï¼Œéƒ½è¦æ±‚æ»¡è¶³$\mathrm{sublayer}(\mathbf{x}) \in \mathbb{R}^d$ï¼Œä»¥ä¾¿æ®‹å·®è¿æ¥æ»¡è¶³$\mathbf{x} + \mathrm{sublayer}(\mathbf{x}) \in \mathbb{R}^d$ã€‚åœ¨æ®‹å·®è¿æ¥çš„åŠ æ³•è®¡ç®—ä¹‹åï¼Œç´§æ¥ç€åº”ç”¨<em>å±‚è§„èŒƒåŒ–</em>ï¼ˆlayer normalizationï¼‰ (<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id5">Ba <em>et al.</em>, 2016</a>)ã€‚å› æ­¤ï¼Œè¾“å…¥åºåˆ—å¯¹åº”çš„æ¯ä¸ªä½ç½®ï¼ŒTransformerç¼–ç å™¨éƒ½å°†è¾“å‡ºä¸€ä¸ªdç»´è¡¨ç¤ºå‘é‡ã€‚</p>
</li>
<li><p>Transformerè§£ç å™¨ä¹Ÿæ˜¯ç”±å¤šä¸ªç›¸åŒçš„å±‚å åŠ è€Œæˆçš„ï¼Œå¹¶ä¸”å±‚ä¸­ä½¿ç”¨äº†æ®‹å·®è¿æ¥å’Œå±‚è§„èŒƒåŒ–ã€‚é™¤äº†ç¼–ç å™¨ä¸­æè¿°çš„ä¸¤ä¸ªå­å±‚ä¹‹å¤–ï¼Œè§£ç å™¨è¿˜åœ¨è¿™ä¸¤ä¸ªå­å±‚ä¹‹é—´æ’å…¥äº†ç¬¬ä¸‰ä¸ªå­å±‚ï¼Œç§°ä¸º<em>ç¼–ç å™¨ï¼è§£ç å™¨æ³¨æ„åŠ›</em>ï¼ˆencoder-decoder attentionï¼‰å±‚ã€‚åœ¨ç¼–ç å™¨ï¼è§£ç å™¨æ³¨æ„åŠ›ä¸­ï¼ŒæŸ¥è¯¢æ¥è‡ªå‰ä¸€ä¸ªè§£ç å™¨å±‚çš„è¾“å‡ºï¼Œè€Œé”®å’Œå€¼æ¥è‡ªæ•´ä¸ªç¼–ç å™¨çš„è¾“å‡ºã€‚åœ¨è§£ç å™¨è‡ªæ³¨æ„åŠ›ä¸­ï¼ŒæŸ¥è¯¢ã€é”®å’Œå€¼éƒ½æ¥è‡ªä¸Šä¸€ä¸ªè§£ç å™¨å±‚çš„è¾“å‡ºã€‚ä½†æ˜¯ï¼Œè§£ç å™¨ä¸­çš„æ¯ä¸ªä½ç½®åªèƒ½è€ƒè™‘è¯¥ä½ç½®ä¹‹å‰çš„æ‰€æœ‰ä½ç½®ã€‚è¿™ç§<em>æ©è”½</em>ï¼ˆmaskedï¼‰æ³¨æ„åŠ›ä¿ç•™äº†<em>è‡ªå›å½’</em>ï¼ˆauto-regressiveï¼‰å±æ€§ï¼Œç¡®ä¿é¢„æµ‹ä»…ä¾èµ–äºå·²ç”Ÿæˆçš„è¾“å‡ºè¯å…ƒã€‚</p>
</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308171653966.svg" srcset="/img/loading.gif" lazyload alt="../_images/transformer.svg"></p>
<blockquote>
<ul>
<li>åŸºäºEncoder-Decoderæ¶æ„æ¥å¤„ç†åºåˆ—å¯¹</li>
<li>Transformeræ˜¯çº¯åŸºäºæ³¨æ„åŠ›</li>
</ul>
</blockquote>
<h2 id="åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ"><a href="#åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ" class="headerlink" title="åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ"></a>åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ</h2><ul>
<li>åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œå¯¹åºåˆ—ä¸­çš„æ‰€æœ‰ä½ç½®çš„è¡¨ç¤ºè¿›è¡Œå˜æ¢æ—¶ä½¿ç”¨çš„æ˜¯åŒä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ï¼Œè¿™å°±æ˜¯ç§°å‰é¦ˆç½‘ç»œæ˜¯<em>åŸºäºä½ç½®çš„</em>ï¼ˆpositionwiseï¼‰çš„åŸå› ã€‚åœ¨ä¸‹é¢çš„å®ç°ä¸­ï¼Œè¾“å…¥<code>X</code>çš„å½¢çŠ¶ï¼ˆæ‰¹é‡å¤§å°ï¼Œæ—¶é—´æ­¥æ•°æˆ–åºåˆ—é•¿åº¦ï¼Œéšå•å…ƒæ•°æˆ–ç‰¹å¾ç»´åº¦ï¼‰å°†è¢«ä¸€ä¸ªä¸¤å±‚çš„æ„ŸçŸ¥æœºè½¬æ¢æˆå½¢çŠ¶ä¸ºï¼ˆæ‰¹é‡å¤§å°ï¼Œæ—¶é—´æ­¥æ•°ï¼Œ<code>ffn_num_outputs</code>ï¼‰çš„è¾“å‡ºå¼ é‡ã€‚</li>
</ul>
<blockquote>
<ul>
<li>å°†è¾“å…¥å½¢çŠ¶ç”±(b, n, d)å˜æ¢ä¸º(bn, d)</li>
<li>ä½œç”¨ä¸¤ä¸ªå…¨è¿æ¥å±‚</li>
<li>å°†è¾“å‡ºå½¢çŠ¶ç”±(bn, d)å˜æ¢ä¸º(b, n, d)</li>
<li>ç­‰ä»·äºä¸¤å±‚æ ¸çª—å£ä¸º1çš„ä¸€ç»´å·ç§¯å±‚</li>
</ul>
<p>FFNå­˜åœ¨çš„åŸå› æ˜¯ï¼šFFNçš„è¾“å…¥ï¼Œlinearå‡½æ•°åªèƒ½å†™ input_embed, å’Œoutput_embedè¿™ä¸¤ä¸ªå‚æ•°, æ‰€ä»¥å¾—åˆ‡æ¢æˆäºŒç»´çš„å½¢å¼è¾“å…¥ã€‚</p>
</blockquote>
<h2 id="æ®‹å·®è¿æ¥å’Œå±‚è§„èŒƒåŒ–"><a href="#æ®‹å·®è¿æ¥å’Œå±‚è§„èŒƒåŒ–" class="headerlink" title="æ®‹å·®è¿æ¥å’Œå±‚è§„èŒƒåŒ–"></a>æ®‹å·®è¿æ¥å’Œå±‚è§„èŒƒåŒ–</h2><ul>
<li><em>åŠ æ³•å’Œè§„èŒƒåŒ–</em>ï¼ˆadd&amp;normï¼‰ç»„ä»¶ã€‚æ­£å¦‚åœ¨æœ¬èŠ‚å¼€å¤´æ‰€è¿°ï¼Œè¿™æ˜¯ç”±æ®‹å·®è¿æ¥å’Œç´§éšå…¶åçš„å±‚è§„èŒƒåŒ–ç»„æˆçš„ã€‚ä¸¤è€…éƒ½æ˜¯æ„å»ºæœ‰æ•ˆçš„æ·±åº¦æ¶æ„çš„å…³é”®ã€‚ä¸€ä¸ªå°æ‰¹é‡çš„æ ·æœ¬å†…åŸºäºæ‰¹é‡è§„èŒƒåŒ–å¯¹æ•°æ®è¿›è¡Œé‡æ–°ä¸­å¿ƒåŒ–å’Œé‡æ–°ç¼©æ”¾çš„è°ƒæ•´ã€‚å±‚è§„èŒƒåŒ–å’Œæ‰¹é‡è§„èŒƒåŒ–çš„ç›®æ ‡ç›¸åŒï¼Œä½†å±‚è§„èŒƒåŒ–æ˜¯åŸºäºç‰¹å¾ç»´åº¦è¿›è¡Œè§„èŒƒåŒ–ã€‚å°½ç®¡æ‰¹é‡è§„èŒƒåŒ–åœ¨è®¡ç®—æœºè§†è§‰ä¸­è¢«å¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­ï¼ˆè¾“å…¥é€šå¸¸æ˜¯å˜é•¿åºåˆ—ï¼‰æ‰¹é‡è§„èŒƒåŒ–é€šå¸¸ä¸å¦‚å±‚è§„èŒƒåŒ–çš„æ•ˆæœå¥½ã€‚</li>
</ul>
<blockquote>
<ul>
<li>æ¯ä¸ªå¥å­çš„é•¿åº¦ä¸ä¸€æ ·ï¼Œä¸èƒ½ç”¨bnï¼Œå¾—ç”¨lnã€‚æ¯”æ–¹è¯´ä¸€è¡Œæ˜¯ä¸€é¡¹æ•°æ®ï¼Œé‚£ä¹ˆbatch normå°±æ˜¯å¯¹ä¸€åˆ—è¿›è¡Œå½’ä¸€åŒ–ï¼Œlnå°±æ˜¯å¯¹æ‰€æœ‰æ•°æ®é¡¹çš„æŸä¸€åˆ—ç‰¹å¾å½’ä¸€åŒ–ã€‚</li>
<li>æ¯å¥è¯æœ‰lenä¸ªè¯ï¼Œæ¯ä¸ªè¯ç”±dä¸ªç‰¹å¾è¡¨ç¤ºï¼ŒBNæ˜¯å¯¹æ‰€æœ‰å¥å­æ‰€æœ‰è¯çš„æŸä¸€ç‰¹å¾åšå½’ä¸€åŒ–ï¼ŒLNæ˜¯å¯¹æŸä¸€å¥è¯çš„æ‰€æœ‰è¯æ‰€æœ‰ç‰¹å¾åšå½’ä¸€åŒ–</li>
</ul>
</blockquote>
<ul>
<li>æ‰¹é‡å½’ä¸€åŒ–å¯¹æ¯ä¸ªç‰¹å¾/é€šé“é‡Œå…ƒç´ è¿›è¡Œå½’ä¸€åŒ–ï¼Œä¸é€‚åˆåºåˆ—é•¿åº¦ä¼šå˜çš„NLPåº”ç”¨ã€‚ï¼ˆé¢„æµ‹åºåˆ—çš„é•¿çŸ­ï¼Œå¯èƒ½ç›´æ¥å½±å“å½’ä¸€åŒ–çš„æ•ˆæœï¼Œä¸ç¨³å®šã€‚å¯¹äºæ¯ä¸ªæ ·æœ¬æ¥åšçš„è¯ï¼Œå’Œé•¿çŸ­æ— å…³ï¼Œå°±ç¨³å®šå¾ˆå¤šäº†æ˜‚ã€‚ï¼‰</li>
<li>å±‚å½’ä¸€åŒ–å¯¹æ¯ä¸ªæ ·æœ¬é‡Œçš„å…ƒç´ è¿›è¡Œå½’ä¸€åŒ–</li>
</ul>
<h2 id="ä¿¡æ¯ä¼ é€’"><a href="#ä¿¡æ¯ä¼ é€’" class="headerlink" title="ä¿¡æ¯ä¼ é€’"></a>ä¿¡æ¯ä¼ é€’</h2><ul>
<li>ç¼–ç å™¨ä¸­çš„è¾“å‡º$y_1, \dots, y_n$</li>
<li>å°†å…¶ä½œä¸ºè§£ç å™¨ä¸­ç¬¬$i$ä¸ªTransformerå—ä¸­å¤šå¤´æ³¨æ„åŠ›çš„keyå’Œvalueï¼Œå®ƒçš„queryæ¥è‡ªç›®æ ‡åºåˆ—ã€‚</li>
<li>æ„å‘³ç€ç¼–ç å™¨å’Œè§£ç å™¨ä¸­å—çš„ä¸ªæ•°å’Œè¾“å‡ºç»´åº¦éƒ½æ˜¯ä¸€æ ·çš„</li>
</ul>
<h2 id="é¢„æµ‹"><a href="#é¢„æµ‹" class="headerlink" title="é¢„æµ‹"></a>é¢„æµ‹</h2><ul>
<li>é¢„æµ‹ç¬¬$t+1$ä¸ªè¾“å‡ºæ—¶</li>
<li>è§£ç å™¨è¾“å…¥å‰tä¸ªé¢„æµ‹å€¼<ul>
<li>åœ¨è‡ªæ³¨æ„åŠ›ä¸­ï¼Œå‰tä¸ªé¢„æµ‹å€¼ä½œä¸ºkeyå’Œvalueï¼Œå¼Ÿtä¸ªé¢„æµ‹å€¼è¿˜ä½œä¸ºquery</li>
</ul>
</li>
</ul>
<blockquote>
<p>è®­ç»ƒæ—¶ï¼Œç¬¬ä¸€ä¸ªmask-å¤šå¤´K,Væ¥è‡ªæœ¬èº«çš„Qï¼Œç¬¬äºŒä¸ªattentionçš„K,Væ‰æ˜¯æ¥è‡ªencoderã€‚</p>
</blockquote>
<h2 id="æ€»ç»“-3"><a href="#æ€»ç»“-3" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h2><ul>
<li>Transformeræ˜¯ä¸€ä¸ªçº¯ä½¿ç”¨æ³¨æ„åŠ›çš„Encoder-Decoderã€‚</li>
<li>Encoderå’ŒDecoderéƒ½æœ‰nä¸ªTransformerå—ã€‚</li>
<li>æ¯ä¸ªå—é‡Œä½¿ç”¨å¤šå¤´ï¼ˆè‡ªï¼‰æ³¨æ„åŠ›ï¼ŒåŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œï¼Œå’Œå±‚å½’ä¸€åŒ–ã€‚</li>
</ul>
<h2 id="ä»£ç å®ç°-1"><a href="#ä»£ç å®ç°-1" class="headerlink" title="ä»£ç å®ç°"></a>ä»£ç å®ç°</h2><h3 id="Dependencies-4"><a href="#Dependencies-4" class="headerlink" title="Dependencies"></a>Dependencies</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br></code></pre></td></tr></tbody></table></figure>

<h3 id="å¤šå¤´æ³¨æ„åŠ›-1"><a href="#å¤šå¤´æ³¨æ„åŠ›-1" class="headerlink" title="å¤šå¤´æ³¨æ„åŠ›"></a>å¤šå¤´æ³¨æ„åŠ›</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiHeadAttention</span>(nn.Module):<br>    <span class="hljs-string">"""å¤šå¤´æ³¨æ„åŠ›"""</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, key_size, query_size, value_size, num_hiddens,</span><br><span class="hljs-params">                 num_heads, dropout, bias=<span class="hljs-literal">False</span>, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(MultiHeadAttention, self).__init__(**kwargs)<br>        self.num_heads = num_heads<br>        self.attention = d2l.DotProductAttention(dropout)<br>        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)<br>        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)<br>        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)<br>        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, queries, keys, values, valid_lens</span>):<br>        <span class="hljs-comment"># queriesï¼Œkeysï¼Œvaluesçš„å½¢çŠ¶:</span><br>        <span class="hljs-comment"># (batch_sizeï¼ŒæŸ¥è¯¢æˆ–è€…â€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œnum_hiddens)</span><br>        <span class="hljs-comment"># valid_lensã€€çš„å½¢çŠ¶:</span><br>        <span class="hljs-comment"># (batch_sizeï¼Œ)æˆ–(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°)</span><br>        <span class="hljs-comment"># ç»è¿‡å˜æ¢åï¼Œè¾“å‡ºçš„queriesï¼Œkeysï¼Œvaluesã€€çš„å½¢çŠ¶:</span><br>        <span class="hljs-comment"># (batch_size*num_headsï¼ŒæŸ¥è¯¢æˆ–è€…â€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œ</span><br>        <span class="hljs-comment"># num_hiddens/num_heads)</span><br>        queries = transpose_qkv(self.W_q(queries), self.num_heads)<br>        keys = transpose_qkv(self.W_k(keys), self.num_heads)<br>        values = transpose_qkv(self.W_v(values), self.num_heads)<br><br>        <span class="hljs-keyword">if</span> valid_lens <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-comment"># åœ¨è½´0ï¼Œå°†ç¬¬ä¸€é¡¹ï¼ˆæ ‡é‡æˆ–è€…çŸ¢é‡ï¼‰å¤åˆ¶num_headsæ¬¡ï¼Œ</span><br>            <span class="hljs-comment"># ç„¶åå¦‚æ­¤å¤åˆ¶ç¬¬äºŒé¡¹ï¼Œç„¶åè¯¸å¦‚æ­¤ç±»ã€‚</span><br>            valid_lens = torch.repeat_interleave(<br>                valid_lens, repeats=self.num_heads, dim=<span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># outputçš„å½¢çŠ¶:(batch_size*num_headsï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°ï¼Œ</span><br>        <span class="hljs-comment"># num_hiddens/num_heads)</span><br>        output = self.attention(queries, keys, values, valid_lens)<br><br>        <span class="hljs-comment"># output_concatçš„å½¢çŠ¶:(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°ï¼Œnum_hiddens)</span><br>        output_concat = transpose_output(output, self.num_heads)<br>        <span class="hljs-keyword">return</span> self.W_o(output_concat)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>ä¸ºäº†èƒ½å¤Ÿä½¿å¤šä¸ªå¤´å¹¶è¡Œè®¡ç®—ï¼Œ ä¸Šé¢çš„<code>MultiHeadAttention</code>ç±»å°†ä½¿ç”¨ä¸‹é¢å®šä¹‰çš„ä¸¤ä¸ªè½¬ç½®å‡½æ•°ã€‚ å…·ä½“æ¥è¯´ï¼Œ<code>transpose_output</code>å‡½æ•°åè½¬äº†<code>transpose_qkv</code>å‡½æ•°çš„æ“ä½œã€‚</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">transpose_qkv</span>(<span class="hljs-params">X, num_heads</span>):<br>    <span class="hljs-string">"""ä¸ºäº†å¤šæ³¨æ„åŠ›å¤´çš„å¹¶è¡Œè®¡ç®—è€Œå˜æ¢å½¢çŠ¶"""</span><br>    <span class="hljs-comment"># è¾“å…¥Xçš„å½¢çŠ¶:(batch_sizeï¼ŒæŸ¥è¯¢æˆ–è€…â€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œnum_hiddens)</span><br>    <span class="hljs-comment"># è¾“å‡ºXçš„å½¢çŠ¶:(batch_sizeï¼ŒæŸ¥è¯¢æˆ–è€…â€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œnum_headsï¼Œ</span><br>    <span class="hljs-comment"># num_hiddens/num_heads)</span><br>    X = X.reshape(X.shape[<span class="hljs-number">0</span>], X.shape[<span class="hljs-number">1</span>], num_heads, -<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># è¾“å‡ºXçš„å½¢çŠ¶:(batch_sizeï¼Œnum_headsï¼ŒæŸ¥è¯¢æˆ–è€…â€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°,</span><br>    <span class="hljs-comment"># num_hiddens/num_heads)</span><br>    X = X.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br><br>    <span class="hljs-comment"># æœ€ç»ˆè¾“å‡ºçš„å½¢çŠ¶:(batch_size*num_heads,æŸ¥è¯¢æˆ–è€…â€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°,</span><br>    <span class="hljs-comment"># num_hiddens/num_heads)</span><br>    <span class="hljs-keyword">return</span> X.reshape(-<span class="hljs-number">1</span>, X.shape[<span class="hljs-number">2</span>], X.shape[<span class="hljs-number">3</span>])<br><br><br><span class="hljs-comment">#@save</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">transpose_output</span>(<span class="hljs-params">X, num_heads</span>):<br>    <span class="hljs-string">"""é€†è½¬transpose_qkvå‡½æ•°çš„æ“ä½œ"""</span><br>    X = X.reshape(-<span class="hljs-number">1</span>, num_heads, X.shape[<span class="hljs-number">1</span>], X.shape[<span class="hljs-number">2</span>])<br>    X = X.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>)<br>    <span class="hljs-keyword">return</span> X.reshape(X.shape[<span class="hljs-number">0</span>], X.shape[<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>ä¸‹é¢ä½¿ç”¨é”®å’Œå€¼ç›¸åŒçš„å°ä¾‹å­æ¥æµ‹è¯•æˆ‘ä»¬ç¼–å†™çš„<code>MultiHeadAttention</code>ç±»ã€‚ å¤šå¤´æ³¨æ„åŠ›è¾“å‡ºçš„å½¢çŠ¶æ˜¯ï¼ˆ<code>batch_size</code>ï¼Œ<code>num_queries</code>ï¼Œ<code>num_hiddens</code>ï¼‰ã€‚</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">num_hiddens, num_heads = <span class="hljs-number">100</span>, <span class="hljs-number">5</span><br>attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,<br>                               num_hiddens, num_heads, <span class="hljs-number">0.5</span>)<br>attention.<span class="hljs-built_in">eval</span>()<br></code></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size, num_queries = <span class="hljs-number">2</span>, <span class="hljs-number">4</span><br>num_kvpairs, valid_lens =  <span class="hljs-number">6</span>, torch.tensor([<span class="hljs-number">3</span>, <span class="hljs-number">2</span>])<br>X = torch.ones((batch_size, num_queries, num_hiddens))<br>Y = torch.ones((batch_size, num_kvpairs, num_hiddens))<br>attention(X, Y, Y, valid_lens).shape<br></code></pre></td></tr></tbody></table></figure>











<h3 id="åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ-1"><a href="#åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ-1" class="headerlink" title="åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ"></a>åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PositionWiseFFN</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,</span><br><span class="hljs-params">                 **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(PositionWiseFFN, self).__init__(**kwargs)<br>        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)<br>        self.relu = nn.ReLU()<br>        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> self.dense2(self.relu(self.dense1(X)))<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>æ”¹å˜å¼ é‡çš„æœ€é‡Œå±‚ç»´åº¦çš„å°ºå¯¸</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">ffn = PositionWiseFFN(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>)<br>ffn.<span class="hljs-built_in">eval</span>()<br>ffn(torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)))[<span class="hljs-number">0</span>]<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>å¯¹æ¯”ä¸åŒç»´åº¦çš„å±‚å½’ä¸€åŒ–å’Œæ‰¹é‡å½’ä¸€åŒ–çš„æ•ˆæœ</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">ln = nn.LayerNorm(<span class="hljs-number">2</span>)<br>bn = nn.BatchNorm1d(<span class="hljs-number">2</span>)<br>X = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]], dtype=torch.float32)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">'layer norm:'</span>, ln(X), <span class="hljs-string">'\nbatch norm:'</span>, bn(X))<br></code></pre></td></tr></tbody></table></figure>



<h3 id="ä½¿ç”¨æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–"><a href="#ä½¿ç”¨æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–" class="headerlink" title="ä½¿ç”¨æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–"></a>ä½¿ç”¨æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">AddNorm</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, normalized_shape, dropout, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(AddNorm, self).__init__(**kwargs)<br>        self.dropout = nn.Dropout(dropout)<br>        self.ln = nn.LayerNorm(normalized_shape)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, Y</span>):<br>        <span class="hljs-keyword">return</span> self.ln(self.dropout(Y) + X)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>åŠ æ³•æ“ä½œåè¾“å‡ºå¼ é‡çš„å½¢çŠ¶ç›¸åŒ</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">add_norm = AddNorm([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>], <span class="hljs-number">0.5</span>)<br>add_norm.<span class="hljs-built_in">eval</span>()<br>add_norm(torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>)), torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))).shape<br></code></pre></td></tr></tbody></table></figure>



<h3 id="Transformerç¼–ç å™¨"><a href="#Transformerç¼–ç å™¨" class="headerlink" title="Transformerç¼–ç å™¨"></a>Transformerç¼–ç å™¨</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">EncoderBlock</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, key_size, query_size, value_size, num_hiddens,</span><br><span class="hljs-params">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="hljs-params">                 dropout, use_bias=<span class="hljs-literal">False</span>, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(EncoderBlock, self).__init__(**kwargs)<br>        self.attention = d2l.MultiHeadAttention(key_size, query_size,<br>                                                value_size, num_hiddens,<br>                                                num_heads, dropout, use_bias)<br>        self.addnorm1 = AddNorm(norm_shape, dropout)<br>        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,<br>                                   num_hiddens)<br>        self.addnorm2 = AddNorm(norm_shape, dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, valid_lens</span>):<br>        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))<br>        <span class="hljs-keyword">return</span> self.addnorm2(Y, self.ffn(Y))<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>Transformerç¼–ç å™¨ä¸­çš„ä»»ä½•å±‚éƒ½ä¸ä¼šæ”¹å˜å…¶è¾“å…¥çš„å½¢çŠ¶</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">100</span>, <span class="hljs-number">24</span>))<br>valid_lens = torch.tensor([<span class="hljs-number">3</span>, <span class="hljs-number">2</span>])<br>encoder_blk = EncoderBlock(<span class="hljs-number">24</span>, <span class="hljs-number">24</span>, <span class="hljs-number">24</span>, <span class="hljs-number">24</span>, [<span class="hljs-number">100</span>, <span class="hljs-number">24</span>], <span class="hljs-number">24</span>, <span class="hljs-number">48</span>, <span class="hljs-number">8</span>, <span class="hljs-number">0.5</span>)<br>encoder_blk.<span class="hljs-built_in">eval</span>()<br>encoder_blk(X, valid_lens).shape<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>Transformerç¼–ç å™¨ï¼š</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerEncoder</span>(d2l.Encoder):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, key_size, query_size, value_size,</span><br><span class="hljs-params">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="hljs-params">                 num_heads, num_layers, dropout, use_bias=<span class="hljs-literal">False</span>, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(TransformerEncoder, self).__init__(**kwargs)<br>        self.num_hiddens = num_hiddens<br>        self.embedding = nn.Embedding(vocab_size, num_hiddens)<br>        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)<br>        self.blks = nn.Sequential()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers):<br>            self.blks.add_module(<br>                <span class="hljs-string">"block"</span> + <span class="hljs-built_in">str</span>(i),<br>                EncoderBlock(key_size, query_size, value_size, num_hiddens,<br>                             norm_shape, ffn_num_input, ffn_num_hiddens,<br>                             num_heads, dropout, use_bias))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, valid_lens, *args</span>):<br>        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))<br>        self.attention_weights = [<span class="hljs-literal">None</span>] * <span class="hljs-built_in">len</span>(self.blks)<br>        <span class="hljs-keyword">for</span> i, blk <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.blks):<br>            X = blk(X, valid_lens)<br>            self.attention_weights[<br>                i] = blk.attention.attention.attention_weights<br>        <span class="hljs-keyword">return</span> X<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>åˆ›å»ºä¸€ä¸ªä¸¤å±‚çš„Transformerç¼–ç å™¨</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">encoder = TransformerEncoder(<span class="hljs-number">200</span>, <span class="hljs-number">24</span>, <span class="hljs-number">24</span>, <span class="hljs-number">24</span>, <span class="hljs-number">24</span>, [<span class="hljs-number">100</span>, <span class="hljs-number">24</span>], <span class="hljs-number">24</span>, <span class="hljs-number">48</span>, <span class="hljs-number">8</span>, <span class="hljs-number">2</span>,<br>                             <span class="hljs-number">0.5</span>)<br>encoder.<span class="hljs-built_in">eval</span>()<br>encoder(torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">100</span>), dtype=torch.long), valid_lens).shape<br></code></pre></td></tr></tbody></table></figure>



<h3 id="Transformerè§£ç å™¨"><a href="#Transformerè§£ç å™¨" class="headerlink" title="Transformerè§£ç å™¨"></a>Transformerè§£ç å™¨</h3><ul>
<li>Transformerè§£ç å™¨ä¹Ÿæ˜¯ç”±å¤šä¸ªç›¸åŒçš„å±‚ç»„æˆï¼š</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DecoderBlock</span>(nn.Module):<br>    <span class="hljs-string">"""è§£ç å™¨ä¸­ç¬¬ i ä¸ªå—"""</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, key_size, query_size, value_size, num_hiddens,</span><br><span class="hljs-params">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span><br><span class="hljs-params">                 dropout, i, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(DecoderBlock, self).__init__(**kwargs)<br>        self.i = i<br>        self.attention1 = d2l.MultiHeadAttention(key_size, query_size,<br>                                                 value_size, num_hiddens,<br>                                                 num_heads, dropout)<br>        self.addnorm1 = AddNorm(norm_shape, dropout)<br>        self.attention2 = d2l.MultiHeadAttention(key_size, query_size,<br>                                                 value_size, num_hiddens,<br>                                                 num_heads, dropout)<br>        self.addnorm2 = AddNorm(norm_shape, dropout)<br>        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,<br>                                   num_hiddens)<br>        self.addnorm3 = AddNorm(norm_shape, dropout)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, state</span>):<br>        enc_outputs, enc_valid_lens = state[<span class="hljs-number">0</span>], state[<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> state[<span class="hljs-number">2</span>][self.i] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            key_values = X<br>        <span class="hljs-keyword">else</span>:<br>            key_values = torch.cat((state[<span class="hljs-number">2</span>][self.i], X), axis=<span class="hljs-number">1</span>)<br>        state[<span class="hljs-number">2</span>][self.i] = key_values<br>        <span class="hljs-keyword">if</span> self.training:<br>            batch_size, num_steps, _ = X.shape<br>            dec_valid_lens = torch.arange(<span class="hljs-number">1</span>, num_steps + <span class="hljs-number">1</span>,<br>                                          device=X.device).repeat(<br>                                              batch_size, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">else</span>:<br>            dec_valid_lens = <span class="hljs-literal">None</span><br><br>        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)<br>        Y = self.addnorm1(X, X2)<br>        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)<br>        Z = self.addnorm2(Y, Y2)<br>        <span class="hljs-keyword">return</span> self.addnorm3(Z, self.ffn(Z)), state<br><br></code></pre></td></tr></tbody></table></figure>

<ul>
<li><p>ç¼–ç å™¨å’Œè§£ç å™¨çš„ç‰¹å¾ç»´åº¦éƒ½æ˜¯num_hiddensï¼š</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">decoder_blk = DecoderBlock(<span class="hljs-number">24</span>, <span class="hljs-number">24</span>, <span class="hljs-number">24</span>, <span class="hljs-number">24</span>, [<span class="hljs-number">100</span>, <span class="hljs-number">24</span>], <span class="hljs-number">24</span>, <span class="hljs-number">48</span>, <span class="hljs-number">8</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0</span>)<br>decoder_blk.<span class="hljs-built_in">eval</span>()<br>X = torch.ones((<span class="hljs-number">2</span>, <span class="hljs-number">100</span>, <span class="hljs-number">24</span>))<br>state = [encoder_blk(X, valid_lens), valid_lens, [<span class="hljs-literal">None</span>]]<br>decoder_blk(X, state)[<span class="hljs-number">0</span>].shape<br>torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">100</span>, <span class="hljs-number">24</span>])<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>Transformerè§£ç å™¨ï¼š</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TransformerDecoder</span>(d2l.AttentionDecoder):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, key_size, query_size, value_size,</span><br><span class="hljs-params">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="hljs-params">                 num_heads, num_layers, dropout, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(TransformerDecoder, self).__init__(**kwargs)<br>        self.num_hiddens = num_hiddens<br>        self.num_layers = num_layers<br>        self.embedding = nn.Embedding(vocab_size, num_hiddens)<br>        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)<br>        self.blks = nn.Sequential()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers):<br>            self.blks.add_module(<br>                <span class="hljs-string">"block"</span> + <span class="hljs-built_in">str</span>(i),<br>                DecoderBlock(key_size, query_size, value_size, num_hiddens,<br>                             norm_shape, ffn_num_input, ffn_num_hiddens,<br>                             num_heads, dropout, i))<br>        self.dense = nn.Linear(num_hiddens, vocab_size)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_state</span>(<span class="hljs-params">self, enc_outputs, enc_valid_lens, *args</span>):<br>        <span class="hljs-keyword">return</span> [enc_outputs, enc_valid_lens, [<span class="hljs-literal">None</span>] * self.num_layers]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, state</span>):<br>        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))<br>        self._attention_weights = [[<span class="hljs-literal">None</span>] * <span class="hljs-built_in">len</span>(self.blks) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>)]<br>        <span class="hljs-keyword">for</span> i, blk <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.blks):<br>            X, state = blk(X, state)<br>            self._attention_weights[<span class="hljs-number">0</span>][<br>                i] = blk.attention1.attention.attention_weights<br>            self._attention_weights[<span class="hljs-number">1</span>][<br>                i] = blk.attention2.attention.attention_weights<br>        <span class="hljs-keyword">return</span> self.dense(X), state<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">attention_weights</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> self._attention_weights<br><br></code></pre></td></tr></tbody></table></figure></li>
</ul>
<h3 id="è®­ç»ƒ-1"><a href="#è®­ç»ƒ-1" class="headerlink" title="è®­ç»ƒ"></a>è®­ç»ƒ</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">num_hiddens, num_layers, dropout, batch_size, num_steps = <span class="hljs-number">32</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">10</span><br>lr, num_epochs, device = <span class="hljs-number">0.005</span>, <span class="hljs-number">200</span>, d2l.try_gpu()<br>ffn_num_input, ffn_num_hiddens, num_heads = <span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">4</span><br>key_size, query_size, value_size = <span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span><br>norm_shape = [<span class="hljs-number">32</span>]<br><br>train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)<br><br>encoder = TransformerEncoder(<span class="hljs-built_in">len</span>(src_vocab), key_size, query_size, value_size,<br>                             num_hiddens, norm_shape, ffn_num_input,<br>                             ffn_num_hiddens, num_heads, num_layers, dropout)<br>decoder = TransformerDecoder(<span class="hljs-built_in">len</span>(tgt_vocab), key_size, query_size, value_size,<br>                             num_hiddens, norm_shape, ffn_num_input,<br>                             ffn_num_hiddens, num_heads, num_layers, dropout)<br>net = d2l.EncoderDecoder(encoder, decoder)<br>d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)<br></code></pre></td></tr></tbody></table></figure>



<h3 id="é¢„æµ‹-1"><a href="#é¢„æµ‹-1" class="headerlink" title="é¢„æµ‹"></a>é¢„æµ‹</h3><ul>
<li>å°†ä¸€äº›è‹±è¯­å¥å­ç¿»è¯‘æˆæ³•è¯­ï¼š</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">engs = [<span class="hljs-string">'go .'</span>, <span class="hljs-string">"i lost ."</span>, <span class="hljs-string">'he\'s calm .'</span>, <span class="hljs-string">'i\'m home .'</span>]<br>fras = [<span class="hljs-string">'va !'</span>, <span class="hljs-string">'j\'ai perdu .'</span>, <span class="hljs-string">'il est calme .'</span>, <span class="hljs-string">'je suis chez moi .'</span>]<br><span class="hljs-keyword">for</span> eng, fra <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(engs, fras):<br>    translation, dec_attention_weight_seq = d2l.predict_seq2seq(<br>        net, eng, src_vocab, tgt_vocab, num_steps, device, <span class="hljs-literal">True</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'<span class="hljs-subst">{eng}</span> =&gt; <span class="hljs-subst">{translation}</span>, '</span>,<br>          <span class="hljs-string">f'bleu <span class="hljs-subst">{d2l.bleu(translation, fra, k=<span class="hljs-number">2</span>):<span class="hljs-number">.3</span>f}</span>'</span>)<br></code></pre></td></tr></tbody></table></figure>



<h3 id="å¯è§†åŒ–"><a href="#å¯è§†åŒ–" class="headerlink" title="å¯è§†åŒ–"></a>å¯è§†åŒ–</h3><ul>
<li>å¯è§†åŒ–Transformer çš„æ³¨æ„åŠ›æƒé‡ï¼š</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">enc_attention_weights = torch.cat(net.encoder.attention_weights, <span class="hljs-number">0</span>).reshape(<br>    (num_layers, num_heads, -<span class="hljs-number">1</span>, num_steps))<br>enc_attention_weights.shape<br></code></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">d2l.show_heatmaps(enc_attention_weights.cpu(), xlabel=<span class="hljs-string">'Key positions'</span>,<br>                  ylabel=<span class="hljs-string">'Query positions'</span>,<br>                  titles=[<span class="hljs-string">'Head %d'</span> % i<br>                          <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>)], figsize=(<span class="hljs-number">7</span>, <span class="hljs-number">3.5</span>))<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>ä¸ºäº†å¯è§†åŒ–è§£ç å™¨çš„è‡ªæ³¨æ„åŠ›æƒé‡å’Œâ€œç¼–ç å™¨ï¼è§£ç å™¨â€çš„æ³¨æ„åŠ›æƒé‡ï¼Œæˆ‘ä»¬éœ€è¦å®Œæˆæ›´å¤šçš„æ•°æ®æ“ä½œå·¥ä½œ</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">dec_attention_weights_2d = [<br>    head[<span class="hljs-number">0</span>].tolist() <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> dec_attention_weight_seq <span class="hljs-keyword">for</span> attn <span class="hljs-keyword">in</span> step<br>    <span class="hljs-keyword">for</span> blk <span class="hljs-keyword">in</span> attn <span class="hljs-keyword">for</span> head <span class="hljs-keyword">in</span> blk]<br>dec_attention_weights_filled = torch.tensor(<br>    pd.DataFrame(dec_attention_weights_2d).fillna(<span class="hljs-number">0.0</span>).values)<br>dec_attention_weights = dec_attention_weights_filled.reshape(<br>    (-<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, num_layers, num_heads, num_steps))<br>dec_self_attention_weights, dec_inter_attention_weights = \<br>    dec_attention_weights.permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>)<br>dec_self_attention_weights.shape, dec_inter_attention_weights.shape<br></code></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">d2l.show_heatmaps(<br>    dec_self_attention_weights[:, :, :, :<span class="hljs-built_in">len</span>(translation.split()) + <span class="hljs-number">1</span>],<br>    xlabel=<span class="hljs-string">'Key positions'</span>, ylabel=<span class="hljs-string">'Query positions'</span>,<br>    titles=[<span class="hljs-string">'Head %d'</span> % i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>)], figsize=(<span class="hljs-number">7</span>, <span class="hljs-number">3.5</span>))<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>è¾“å‡ºåºåˆ—çš„æŸ¥è¯¢ä¸ä¼šä¸è¾“å…¥åºåˆ—ä¸­å¡«å……ä½ç½®çš„æ ‡è®°è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">d2l.show_heatmaps(dec_inter_attention_weights, xlabel=<span class="hljs-string">'Key positions'</span>,<br>                  ylabel=<span class="hljs-string">'Query positions'</span>,<br>                  titles=[<span class="hljs-string">'Head %d'</span> % i<br>                          <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>)], figsize=(<span class="hljs-number">7</span>, <span class="hljs-number">3.5</span>))<br></code></pre></td></tr></tbody></table></figure>



<h1 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h1><h2 id="Transfer-Learning-in-NLP"><a href="#Transfer-Learning-in-NLP" class="headerlink" title="Transfer Learning in NLP"></a>Transfer Learning in NLP</h2><ul>
<li>ä½¿ç”¨é¢„è®­ç»ƒå¥½çš„æ¨¡å‹æ¥æŠ½å–è¯ã€å¥å­çš„ç‰¹å¾<ul>
<li>ä¾‹å¦‚word2vecæˆ–è¯­è¨€æ¨¡å‹</li>
</ul>
</li>
<li><strong>ä¸æ›´æ–°é¢„è®­ç»ƒå¥½çš„æ¨¡å‹</strong></li>
<li>éœ€è¦æ„å»ºæ–°çš„ç½‘ç»œæ¥æŠ“å–æ–°ä»»åŠ¡éœ€è¦çš„ä¿¡æ¯<ul>
<li>Word2vecå¿½ç•¥äº†æ—¶åºä¿¡æ¯ï¼Œè¯­è¨€æ¨¡å‹åªçœ‹äº†ä¸€ä¸ªæ–¹å‘</li>
</ul>
</li>
</ul>
<h2 id="Bertçš„åŠ¨æœº"><a href="#Bertçš„åŠ¨æœº" class="headerlink" title="Bertçš„åŠ¨æœº"></a>Bertçš„åŠ¨æœº</h2><ul>
<li>åŸºäºfine tuneçš„NLPæ¨¡å‹</li>
<li>é¢„è®­ç»ƒçš„æ¨¡å‹æŠ½å–äº†è¶³å¤Ÿå¤šçš„ä¿¡æ¯</li>
<li>æ–°çš„ä»»åŠ¡åªéœ€è¦å¢åŠ ä¸€ä¸ªç®€å•çš„è¾“å‡ºå±‚</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308172131963.png" srcset="/img/loading.gif" lazyload alt="image-20230817213133734"></p>
<h2 id="Bertæ¶æ„"><a href="#Bertæ¶æ„" class="headerlink" title="Bertæ¶æ„"></a>Bertæ¶æ„</h2><ul>
<li>åªæœ‰ç¼–ç å™¨çš„Transformer</li>
<li>ä¸¤ä¸ªç‰ˆæœ¬:<ul>
<li>Base: #blocks= 12, hidden size= 768, #heads= 12, #parameters= 1 10M</li>
<li>Large: #blocks= 24, hidden size= 1024, #heads= 16, #parameter = 340M</li>
</ul>
</li>
<li>åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šè®­ç»ƒ &gt; 3B è¯</li>
</ul>
<h2 id="å¯¹è¾“å…¥çš„ä¿®æ”¹"><a href="#å¯¹è¾“å…¥çš„ä¿®æ”¹" class="headerlink" title="å¯¹è¾“å…¥çš„ä¿®æ”¹"></a>å¯¹è¾“å…¥çš„ä¿®æ”¹</h2><ul>
<li>æ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ªå¥å­å¯¹</li>
<li>åŠ å…¥é¢å¤–çš„ç‰‡æ®µåµŒå…¥</li>
<li>ä½ç½®ç¼–ç å¯å­¦ä¹ </li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308172140964.png" srcset="/img/loading.gif" lazyload alt="image-20230817214029711"></p>
<h2 id="ä»£ç å®ç°-2"><a href="#ä»£ç å®ç°-2" class="headerlink" title="ä»£ç å®ç°"></a>ä»£ç å®ç°</h2><h3 id="Dependencies-5"><a href="#Dependencies-5" class="headerlink" title="Dependencies"></a>Dependencies</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br></code></pre></td></tr></tbody></table></figure>



<h3 id="Input-Representation"><a href="#Input-Representation" class="headerlink" title="Input Representation"></a>Input Representation</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_tokens_and_segments</span>(<span class="hljs-params">tokens_a, tokens_b=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">"""Get tokens of the BERT input sequence and their segment IDs."""</span><br>    tokens = [<span class="hljs-string">'&lt;cls&gt;'</span>] + tokens_a + [<span class="hljs-string">'&lt;sep&gt;'</span>]<br>    segments = [<span class="hljs-number">0</span>] * (<span class="hljs-built_in">len</span>(tokens_a) + <span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">if</span> tokens_b <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        tokens += tokens_b + [<span class="hljs-string">'&lt;sep&gt;'</span>]<br>        segments += [<span class="hljs-number">1</span>] * (<span class="hljs-built_in">len</span>(tokens_b) + <span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> tokens, segments<br></code></pre></td></tr></tbody></table></figure>



<h3 id="BERTEncoder"><a href="#BERTEncoder" class="headerlink" title="BERTEncoder"></a>BERTEncoder</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BERTEncoder</span>(nn.Module):<br>    <span class="hljs-string">"""BERT encoder."""</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, num_hiddens, norm_shape, ffn_num_input,</span><br><span class="hljs-params">                 ffn_num_hiddens, num_heads, num_layers, dropout,</span><br><span class="hljs-params">                 max_len=<span class="hljs-number">1000</span>, key_size=<span class="hljs-number">768</span>, query_size=<span class="hljs-number">768</span>, value_size=<span class="hljs-number">768</span>,</span><br><span class="hljs-params">                 **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(BERTEncoder, self).__init__(**kwargs)<br>        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)<br>        self.segment_embedding = nn.Embedding(<span class="hljs-number">2</span>, num_hiddens)<br>        self.blks = nn.Sequential()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers):<br>            self.blks.add_module(<span class="hljs-string">f"<span class="hljs-subst">{i}</span>"</span>, d2l.EncoderBlock(<br>                key_size, query_size, value_size, num_hiddens, norm_shape,<br>                ffn_num_input, ffn_num_hiddens, num_heads, dropout, <span class="hljs-literal">True</span>))<br>        self.pos_embedding = nn.Parameter(torch.randn(<span class="hljs-number">1</span>, max_len,<br>                                                      num_hiddens))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, tokens, segments, valid_lens</span>):<br>        X = self.token_embedding(tokens) + self.segment_embedding(segments)<br>        X = X + self.pos_embedding.data[:, :X.shape[<span class="hljs-number">1</span>], :]<br>        <span class="hljs-keyword">for</span> blk <span class="hljs-keyword">in</span> self.blks:<br>            X = blk(X, valid_lens)<br>        <span class="hljs-keyword">return</span> X<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>Inference of <code>BERTEncoder</code></li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">vocab_size, num_hiddens, ffn_num_hiddens, num_heads = <span class="hljs-number">10000</span>, <span class="hljs-number">768</span>, <span class="hljs-number">1024</span>, <span class="hljs-number">4</span><br>norm_shape, ffn_num_input, num_layers, dropout = [<span class="hljs-number">768</span>], <span class="hljs-number">768</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0.2</span><br>encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,<br>                      ffn_num_hiddens, num_heads, num_layers, dropout)<br><br>tokens = torch.randint(<span class="hljs-number">0</span>, vocab_size, (<span class="hljs-number">2</span>, <span class="hljs-number">8</span>))<br>segments = torch.tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])<br>encoded_X = encoder(tokens, segments, <span class="hljs-literal">None</span>)<br>encoded_X.shape<br></code></pre></td></tr></tbody></table></figure>



<h3 id="Masked-Language-Modeling"><a href="#Masked-Language-Modeling" class="headerlink" title="Masked Language Modeling"></a>Masked Language Modeling</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MaskLM</span>(nn.Module):<br>    <span class="hljs-string">"""The masked language model task of BERT."""</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, num_hiddens, num_inputs=<span class="hljs-number">768</span>, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(MaskLM, self).__init__(**kwargs)<br>        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),<br>                                 nn.ReLU(),<br>                                 nn.LayerNorm(num_hiddens),<br>                                 nn.Linear(num_hiddens, vocab_size))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, pred_positions</span>):<br>        num_pred_positions = pred_positions.shape[<span class="hljs-number">1</span>]<br>        pred_positions = pred_positions.reshape(-<span class="hljs-number">1</span>)<br>        batch_size = X.shape[<span class="hljs-number">0</span>]<br>        batch_idx = torch.arange(<span class="hljs-number">0</span>, batch_size)<br>        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)<br>        masked_X = X[batch_idx, pred_positions]<br>        masked_X = masked_X.reshape((batch_size, num_pred_positions, -<span class="hljs-number">1</span>))<br>        mlm_Y_hat = self.mlp(masked_X)<br>        <span class="hljs-keyword">return</span> mlm_Y_hat<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>The forward inference of MaskLM<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">mlm = MaskLM(vocab_size, num_hiddens)<br>mlm_positions = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>], [<span class="hljs-number">6</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>]])<br>mlm_Y_hat = mlm(encoded_X, mlm_positions)<br>mlm_Y_hat.shape<br><span class="hljs-comment"># result torch.Size([2, 3, 10000])</span><br></code></pre></td></tr></tbody></table></figure></li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">mlm_Y = torch.tensor([[<span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>], [<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>]])<br>loss = nn.CrossEntropyLoss(reduction=<span class="hljs-string">'none'</span>)<br>mlm_l = loss(mlm_Y_hat.reshape((-<span class="hljs-number">1</span>, vocab_size)), mlm_Y.reshape(-<span class="hljs-number">1</span>))<br>mlm_l.shape<br><span class="hljs-comment"># result torch.Size([6])</span><br></code></pre></td></tr></tbody></table></figure>



<h3 id="Next-Sentence-Prediction"><a href="#Next-Sentence-Prediction" class="headerlink" title="Next Sentence Prediction"></a>Next Sentence Prediction</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">NextSentencePred</span>(nn.Module):<br>    <span class="hljs-string">"""The next sentence prediction task of BERT."""</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_inputs, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(NextSentencePred, self).__init__(**kwargs)<br>        self.output = nn.Linear(num_inputs, <span class="hljs-number">2</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> self.output(X)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>The forward inference of an <code>NextSentencePred</code></li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">encoded_X = torch.flatten(encoded_X, start_dim=<span class="hljs-number">1</span>)<br>nsp = NextSentencePred(encoded_X.shape[-<span class="hljs-number">1</span>])<br>nsp_Y_hat = nsp(encoded_X)<br>nsp_Y_hat.shape<br><span class="hljs-comment"># result torch.Size([2, 2])</span><br></code></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">nsp_y = torch.tensor([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br>nsp_l = loss(nsp_Y_hat, nsp_y)<br>nsp_l.shape<br><br><span class="hljs-comment"># result torch.Size([2])</span><br></code></pre></td></tr></tbody></table></figure>



<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BERTModel</span>(nn.Module):<br>    <span class="hljs-string">"""The BERT model."""</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab_size, num_hiddens, norm_shape, ffn_num_input,</span><br><span class="hljs-params">                 ffn_num_hiddens, num_heads, num_layers, dropout,</span><br><span class="hljs-params">                 max_len=<span class="hljs-number">1000</span>, key_size=<span class="hljs-number">768</span>, query_size=<span class="hljs-number">768</span>, value_size=<span class="hljs-number">768</span>,</span><br><span class="hljs-params">                 hid_in_features=<span class="hljs-number">768</span>, mlm_in_features=<span class="hljs-number">768</span>,</span><br><span class="hljs-params">                 nsp_in_features=<span class="hljs-number">768</span></span>):<br>        <span class="hljs-built_in">super</span>(BERTModel, self).__init__()<br>        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,<br>                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,<br>                    dropout, max_len=max_len, key_size=key_size,<br>                    query_size=query_size, value_size=value_size)<br>        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),<br>                                    nn.Tanh())<br>        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)<br>        self.nsp = NextSentencePred(nsp_in_features)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, tokens, segments, valid_lens=<span class="hljs-literal">None</span>, pred_positions=<span class="hljs-literal">None</span></span>):<br>        encoded_X = self.encoder(tokens, segments, valid_lens)<br>        <span class="hljs-keyword">if</span> pred_positions <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            mlm_Y_hat = self.mlm(encoded_X, pred_positions)<br>        <span class="hljs-keyword">else</span>:<br>            mlm_Y_hat = <span class="hljs-literal">None</span><br>        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, <span class="hljs-number">0</span>, :]))<br>        <span class="hljs-keyword">return</span> encoded_X, mlm_Y_hat, nsp_Y_hat<br></code></pre></td></tr></tbody></table></figure>



<h2 id="The-Dataset-for-Pretraining-BERT"><a href="#The-Dataset-for-Pretraining-BERT" class="headerlink" title="The Dataset for Pretraining BERT"></a>The Dataset for Pretraining BERT</h2><ul>
<li>Dependencies:</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li><p>The WikiText-2 dataset: </p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">d2l.DATA_HUB[<span class="hljs-string">'wikitext-2'</span>] = (<br>    <span class="hljs-string">'https://s3.amazonaws.com/research.metamind.io/wikitext/'</span><br>    <span class="hljs-string">'wikitext-2-v1.zip'</span>, <span class="hljs-string">'3c914d17d80b1459be871a5039ac23e752a53cbe'</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_read_wiki</span>(<span class="hljs-params">data_dir</span>):<br>    file_name = os.path.join(data_dir, <span class="hljs-string">'wiki.train.tokens'</span>)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_name, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:<br>        lines = f.readlines()<br>    paragraphs = [line.strip().lower().split(<span class="hljs-string">' . '</span>)<br>                  <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(line.split(<span class="hljs-string">' . '</span>)) &gt;= <span class="hljs-number">2</span>]<br>    random.shuffle(paragraphs)<br>    <span class="hljs-keyword">return</span> paragraphs<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>Generating the Next Sentence Prediction Task: </p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_next_sentence</span>(<span class="hljs-params">sentence, next_sentence, paragraphs</span>):<br>    <span class="hljs-keyword">if</span> random.random() &lt; <span class="hljs-number">0.5</span>:<br>        is_next = <span class="hljs-literal">True</span><br>    <span class="hljs-keyword">else</span>:<br>        next_sentence = random.choice(random.choice(paragraphs))<br>        is_next = <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">return</span> sentence, next_sentence, is_next<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_nsp_data_from_paragraph</span>(<span class="hljs-params">paragraph, paragraphs, vocab, max_len</span>):<br>    nsp_data_from_paragraph = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(paragraph) - <span class="hljs-number">1</span>):<br>        tokens_a, tokens_b, is_next = _get_next_sentence(<br>            paragraph[i], paragraph[i + <span class="hljs-number">1</span>], paragraphs)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(tokens_a) + <span class="hljs-built_in">len</span>(tokens_b) + <span class="hljs-number">3</span> &gt; max_len:<br>            <span class="hljs-keyword">continue</span><br>        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)<br>        nsp_data_from_paragraph.append((tokens, segments, is_next))<br>    <span class="hljs-keyword">return</span> nsp_data_from_paragraph<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>Generating the Masked Language Modeling Task</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_replace_mlm_tokens</span>(<span class="hljs-params">tokens, candidate_pred_positions, num_mlm_preds,</span><br><span class="hljs-params">                        vocab</span>):<br>    mlm_input_tokens = [token <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> tokens]<br>    pred_positions_and_labels = []<br>    random.shuffle(candidate_pred_positions)<br>    <span class="hljs-keyword">for</span> mlm_pred_position <span class="hljs-keyword">in</span> candidate_pred_positions:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(pred_positions_and_labels) &gt;= num_mlm_preds:<br>            <span class="hljs-keyword">break</span><br>        masked_token = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> random.random() &lt; <span class="hljs-number">0.8</span>:<br>            masked_token = <span class="hljs-string">'&lt;mask&gt;'</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">if</span> random.random() &lt; <span class="hljs-number">0.5</span>:<br>                masked_token = tokens[mlm_pred_position]<br>            <span class="hljs-keyword">else</span>:<br>                masked_token = random.randint(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(vocab) - <span class="hljs-number">1</span>)<br>        mlm_input_tokens[mlm_pred_position] = masked_token<br>        pred_positions_and_labels.append(<br>            (mlm_pred_position, tokens[mlm_pred_position]))<br>    <span class="hljs-keyword">return</span> mlm_input_tokens, pred_positions_and_labels<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_mlm_data_from_tokens</span>(<span class="hljs-params">tokens, vocab</span>):<br>    candidate_pred_positions = []<br>    <span class="hljs-keyword">for</span> i, token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tokens):<br>        <span class="hljs-keyword">if</span> token <span class="hljs-keyword">in</span> [<span class="hljs-string">'&lt;cls&gt;'</span>, <span class="hljs-string">'&lt;sep&gt;'</span>]:<br>            <span class="hljs-keyword">continue</span><br>        candidate_pred_positions.append(i)<br>    num_mlm_preds = <span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">round</span>(<span class="hljs-built_in">len</span>(tokens) * <span class="hljs-number">0.15</span>))<br>    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(<br>        tokens, candidate_pred_positions, num_mlm_preds, vocab)<br>    pred_positions_and_labels = <span class="hljs-built_in">sorted</span>(pred_positions_and_labels,<br>                                       key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">0</span>])<br>    pred_positions = [v[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> pred_positions_and_labels]<br>    mlm_pred_labels = [v[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> pred_positions_and_labels]<br>    <span class="hljs-keyword">return</span> vocab[mlm_input_tokens], pred_positions,vocab[mlm_pred_labels]<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>Append the special â€œ<mask>â€ tokens to the inputs</mask></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_pad_bert_inputs</span>(<span class="hljs-params">examples, max_len, vocab</span>):<br>    max_num_mlm_preds = <span class="hljs-built_in">round</span>(max_len * <span class="hljs-number">0.15</span>)<br>    all_token_ids, all_segments, valid_lens,  = [], [], []<br>    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []<br>    nsp_labels = []<br>    <span class="hljs-keyword">for</span> (token_ids, pred_positions, mlm_pred_label_ids, segments,<br>         is_next) <span class="hljs-keyword">in</span> examples:<br>        all_token_ids.append(torch.tensor(token_ids + [vocab[<span class="hljs-string">'&lt;pad&gt;'</span>]] * (<br>            max_len - <span class="hljs-built_in">len</span>(token_ids)), dtype=torch.long))<br>        all_segments.append(torch.tensor(segments + [<span class="hljs-number">0</span>] * (<br>            max_len - <span class="hljs-built_in">len</span>(segments)), dtype=torch.long))<br>        valid_lens.append(torch.tensor(<span class="hljs-built_in">len</span>(token_ids), dtype=torch.float32))<br>        all_pred_positions.append(torch.tensor(pred_positions + [<span class="hljs-number">0</span>] * (<br>            max_num_mlm_preds - <span class="hljs-built_in">len</span>(pred_positions)), dtype=torch.long))<br>        all_mlm_weights.append(<br>            torch.tensor([<span class="hljs-number">1.0</span>] * <span class="hljs-built_in">len</span>(mlm_pred_label_ids) + [<span class="hljs-number">0.0</span>] * (<br>                max_num_mlm_preds - <span class="hljs-built_in">len</span>(pred_positions)),<br>                dtype=torch.float32))<br>        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [<span class="hljs-number">0</span>] * (<br>            max_num_mlm_preds - <span class="hljs-built_in">len</span>(mlm_pred_label_ids)), dtype=torch.long))<br>        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))<br>    <span class="hljs-keyword">return</span> (all_token_ids, all_segments, valid_lens, all_pred_positions,<br>            all_mlm_weights, all_mlm_labels, nsp_labels)<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>The WikiText-2 dataset for pretraining BERT</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">_WikiTextDataset</span>(torch.utils.data.Dataset):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, paragraphs, max_len</span>):<br>        paragraphs = [d2l.tokenize(<br>            paragraph, token=<span class="hljs-string">'word'</span>) <span class="hljs-keyword">for</span> paragraph <span class="hljs-keyword">in</span> paragraphs]<br>        sentences = [sentence <span class="hljs-keyword">for</span> paragraph <span class="hljs-keyword">in</span> paragraphs<br>                     <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> paragraph]<br>        self.vocab = d2l.Vocab(sentences, min_freq=<span class="hljs-number">5</span>, reserved_tokens=[<br>            <span class="hljs-string">'&lt;pad&gt;'</span>, <span class="hljs-string">'&lt;mask&gt;'</span>, <span class="hljs-string">'&lt;cls&gt;'</span>, <span class="hljs-string">'&lt;sep&gt;'</span>])<br>        examples = []<br>        <span class="hljs-keyword">for</span> paragraph <span class="hljs-keyword">in</span> paragraphs:<br>            examples.extend(_get_nsp_data_from_paragraph(<br>                paragraph, paragraphs, self.vocab, max_len))<br>        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)<br>                      + (segments, is_next))<br>                     <span class="hljs-keyword">for</span> tokens, segments, is_next <span class="hljs-keyword">in</span> examples]<br>        (self.all_token_ids, self.all_segments, self.valid_lens,<br>         self.all_pred_positions, self.all_mlm_weights,<br>         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(<br>            examples, max_len, self.vocab)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> (self.all_token_ids[idx], self.all_segments[idx],<br>                self.valid_lens[idx], self.all_pred_positions[idx],<br>                self.all_mlm_weights[idx], self.all_mlm_labels[idx],<br>                self.nsp_labels[idx])<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.all_token_ids)<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>Download and WikiText-2 dataset and generate pretraining examples</p>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data_wiki</span>(<span class="hljs-params">batch_size, max_len</span>):<br>    <span class="hljs-string">"""Load the WikiText-2 dataset."""</span><br>    num_workers = d2l.get_dataloader_workers()<br>    data_dir = d2l.download_extract(<span class="hljs-string">'wikitext-2'</span>, <span class="hljs-string">'wikitext-2'</span>)<br>    paragraphs = _read_wiki(data_dir)<br>    train_set = _WikiTextDataset(paragraphs, max_len)<br>    train_iter = torch.utils.data.DataLoader(train_set, batch_size,<br>                                        shuffle=<span class="hljs-literal">True</span>, num_workers=num_workers)<br>    <span class="hljs-keyword">return</span> train_iter, train_set.vocab<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>Print out the shapes of a minibatch of BERT pretraining examples</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size, max_len = <span class="hljs-number">512</span>, <span class="hljs-number">64</span><br>train_iter, vocab = load_data_wiki(batch_size, max_len)<br><br><span class="hljs-keyword">for</span> (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,<br>     mlm_Y, nsp_y) <span class="hljs-keyword">in</span> train_iter:<br>    <span class="hljs-built_in">print</span>(tokens_X.shape, segments_X.shape, valid_lens_x.shape,<br>          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,<br>          nsp_y.shape)<br>    <span class="hljs-keyword">break</span><br></code></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">len</span>(vocab)<br></code></pre></td></tr></tbody></table></figure>



<h2 id="Pretraining-BERT"><a href="#Pretraining-BERT" class="headerlink" title="Pretraining BERT"></a>Pretraining BERT</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>batch_size, max_len = <span class="hljs-number">512</span>, <span class="hljs-number">64</span><br>train_iter, vocab = d2l.load_data_wiki(batch_size, max_len)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>A small BERT, using 2 layers, 128 hidden units, and 2 self-attention heads</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">net = d2l.BERTModel(<span class="hljs-built_in">len</span>(vocab), num_hiddens=<span class="hljs-number">128</span>, norm_shape=[<span class="hljs-number">128</span>],<br>                    ffn_num_input=<span class="hljs-number">128</span>, ffn_num_hiddens=<span class="hljs-number">256</span>, num_heads=<span class="hljs-number">2</span>,<br>                    num_layers=<span class="hljs-number">2</span>, dropout=<span class="hljs-number">0.2</span>, key_size=<span class="hljs-number">128</span>, query_size=<span class="hljs-number">128</span>,<br>                    value_size=<span class="hljs-number">128</span>, hid_in_features=<span class="hljs-number">128</span>, mlm_in_features=<span class="hljs-number">128</span>,<br>                    nsp_in_features=<span class="hljs-number">128</span>)<br>devices = d2l.try_all_gpus()<br>loss = nn.CrossEntropyLoss()<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>Computes the loss for both the masked language modeling and next sentence prediction tasks</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_batch_loss_bert</span>(<span class="hljs-params">net, loss, vocab_size, tokens_X,</span><br><span class="hljs-params">                         segments_X, valid_lens_x,</span><br><span class="hljs-params">                         pred_positions_X, mlm_weights_X,</span><br><span class="hljs-params">                         mlm_Y, nsp_y</span>):<br>    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,<br>                                  valid_lens_x.reshape(-<span class="hljs-number">1</span>),<br>                                  pred_positions_X)<br>    mlm_l = loss(mlm_Y_hat.reshape(-<span class="hljs-number">1</span>, vocab_size), mlm_Y.reshape(-<span class="hljs-number">1</span>)) *\<br>    mlm_weights_X.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>    mlm_l = mlm_l.<span class="hljs-built_in">sum</span>() / (mlm_weights_X.<span class="hljs-built_in">sum</span>() + <span class="hljs-number">1e-8</span>)<br>    nsp_l = loss(nsp_Y_hat, nsp_y)<br>    l = mlm_l + nsp_l<br>    <span class="hljs-keyword">return</span> mlm_l, nsp_l, l<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>Pretrain BERT (<code>net</code>) on the WikiText-2 (<code>train_iter</code>) dataset</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_bert</span>(<span class="hljs-params">train_iter, net, loss, vocab_size, devices, num_steps</span>):<br>    net = nn.DataParallel(net, device_ids=devices).to(devices[<span class="hljs-number">0</span>])<br>    trainer = torch.optim.Adam(net.parameters(), lr=<span class="hljs-number">1e-3</span>)<br>    step, timer = <span class="hljs-number">0</span>, d2l.Timer()<br>    animator = d2l.Animator(xlabel=<span class="hljs-string">'step'</span>, ylabel=<span class="hljs-string">'loss'</span>,<br>                            xlim=[<span class="hljs-number">1</span>, num_steps], legend=[<span class="hljs-string">'mlm'</span>, <span class="hljs-string">'nsp'</span>])<br>    metric = d2l.Accumulator(<span class="hljs-number">4</span>)<br>    num_steps_reached = <span class="hljs-literal">False</span><br>    <span class="hljs-keyword">while</span> step &lt; num_steps <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> num_steps_reached:<br>        <span class="hljs-keyword">for</span> tokens_X, segments_X, valid_lens_x, pred_positions_X,\<br>            mlm_weights_X, mlm_Y, nsp_y <span class="hljs-keyword">in</span> train_iter:<br>            tokens_X = tokens_X.to(devices[<span class="hljs-number">0</span>])<br>            segments_X = segments_X.to(devices[<span class="hljs-number">0</span>])<br>            valid_lens_x = valid_lens_x.to(devices[<span class="hljs-number">0</span>])<br>            pred_positions_X = pred_positions_X.to(devices[<span class="hljs-number">0</span>])<br>            mlm_weights_X = mlm_weights_X.to(devices[<span class="hljs-number">0</span>])<br>            mlm_Y, nsp_y = mlm_Y.to(devices[<span class="hljs-number">0</span>]), nsp_y.to(devices[<span class="hljs-number">0</span>])<br>            trainer.zero_grad()<br>            timer.start()<br>            mlm_l, nsp_l, l = _get_batch_loss_bert(<br>                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,<br>                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)<br>            l.backward()<br>            trainer.step()<br>            metric.add(mlm_l, nsp_l, tokens_X.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>)<br>            timer.stop()<br>            animator.add(step + <span class="hljs-number">1</span>,<br>                         (metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">3</span>], metric[<span class="hljs-number">1</span>] / metric[<span class="hljs-number">3</span>]))<br>            step += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> step == num_steps:<br>                num_steps_reached = <span class="hljs-literal">True</span><br>                <span class="hljs-keyword">break</span><br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'MLM loss <span class="hljs-subst">{metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">3</span>]:<span class="hljs-number">.3</span>f}</span>, '</span><br>          <span class="hljs-string">f'NSP loss <span class="hljs-subst">{metric[<span class="hljs-number">1</span>] / metric[<span class="hljs-number">3</span>]:<span class="hljs-number">.3</span>f}</span>'</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'<span class="hljs-subst">{metric[<span class="hljs-number">2</span>] / timer.<span class="hljs-built_in">sum</span>():<span class="hljs-number">.1</span>f}</span> sentence pairs/sec on '</span><br>          <span class="hljs-string">f'<span class="hljs-subst">{<span class="hljs-built_in">str</span>(devices)}</span>'</span>)<br><br>train_bert(train_iter, net, loss, <span class="hljs-built_in">len</span>(vocab), devices, <span class="hljs-number">50</span>)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>Representing Text with BERT</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_bert_encoding</span>(<span class="hljs-params">net, tokens_a, tokens_b=<span class="hljs-literal">None</span></span>):<br>    tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)<br>    token_ids = torch.tensor(vocab[tokens], device=devices[<span class="hljs-number">0</span>]).unsqueeze(<span class="hljs-number">0</span>)<br>    segments = torch.tensor(segments, device=devices[<span class="hljs-number">0</span>]).unsqueeze(<span class="hljs-number">0</span>)<br>    valid_len = torch.tensor(<span class="hljs-built_in">len</span>(tokens), device=devices[<span class="hljs-number">0</span>]).unsqueeze(<span class="hljs-number">0</span>)<br>    encoded_X, _, _ = net(token_ids, segments, valid_len)<br>    <span class="hljs-keyword">return</span> encoded_X<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>Consider the sentence â€œa crane is flyingâ€</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">tokens_a = [<span class="hljs-string">'a'</span>, <span class="hljs-string">'crane'</span>, <span class="hljs-string">'is'</span>, <span class="hljs-string">'flying'</span>]<br>encoded_text = get_bert_encoding(net, tokens_a)<br>encoded_text_cls = encoded_text[:, <span class="hljs-number">0</span>, :]<br>encoded_text_crane = encoded_text[:, <span class="hljs-number">2</span>, :]<br>encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[<span class="hljs-number">0</span>][:<span class="hljs-number">3</span>]<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>Now consider a sentence pair â€œa crane driver cameâ€ and â€œhe just leftâ€</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">tokens_a, tokens_b = [<span class="hljs-string">'a'</span>, <span class="hljs-string">'crane'</span>, <span class="hljs-string">'driver'</span>, <span class="hljs-string">'came'</span>], [<span class="hljs-string">'he'</span>, <span class="hljs-string">'just'</span>, <span class="hljs-string">'left'</span>]<br>encoded_pair = get_bert_encoding(net, tokens_a, tokens_b)<br>encoded_pair_cls = encoded_pair[:, <span class="hljs-number">0</span>, :]<br>encoded_pair_crane = encoded_pair[:, <span class="hljs-number">2</span>, :]<br>encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[<span class="hljs-number">0</span>][:<span class="hljs-number">3</span>]<br></code></pre></td></tr></tbody></table></figure>







<h1 id="Bert-Fine-Tuning"><a href="#Bert-Fine-Tuning" class="headerlink" title="Bert Fine Tuning"></a>Bert Fine Tuning</h1><blockquote>
<p>ä¸Šé¢çš„è¯¾ç¨‹éƒ½æ˜¯é¡ºåºåœ¨d2lçš„è¯¾ç¨‹ä¸­ï¼ŒBert Fine Tuningåœ¨ä¸€ä¸ªæ–°çš„ä½ç½®ï¼š<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html">å¾®è°ƒBert D2L æ•™ç¨‹</a>ï¼Œå†²ï¼</p>
</blockquote>
<h2 id="å•æ–‡æœ¬åˆ†ç±»"><a href="#å•æ–‡æœ¬åˆ†ç±»" class="headerlink" title="å•æ–‡æœ¬åˆ†ç±»"></a>å•æ–‡æœ¬åˆ†ç±»</h2><p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308181605050.svg" srcset="/img/loading.gif" lazyload alt="../_images/bert-one-seq.svg"></p>
<ul>
<li><p><em>å•æ–‡æœ¬åˆ†ç±»</em>å°†å•ä¸ªæ–‡æœ¬åºåˆ—ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºå…¶åˆ†ç±»ç»“æœã€‚ é™¤äº†æˆ‘ä»¬åœ¨è¿™ä¸€ç« ä¸­æ¢è®¨çš„æƒ…æ„Ÿåˆ†æä¹‹å¤–ï¼Œè¯­è¨€å¯æ¥å—æ€§è¯­æ–™åº“ï¼ˆCorpus of Linguistic Acceptabilityï¼ŒCOLAï¼‰ä¹Ÿæ˜¯ä¸€ä¸ªå•æ–‡æœ¬åˆ†ç±»çš„æ•°æ®é›†ï¼Œå®ƒçš„è¦æ±‚åˆ¤æ–­ç»™å®šçš„å¥å­åœ¨è¯­æ³•ä¸Šæ˜¯å¦å¯ä»¥æ¥å—ã€‚</p>
</li>
<li><p>BERTè¾“å…¥åºåˆ—æ˜ç¡®åœ°è¡¨ç¤ºå•ä¸ªæ–‡æœ¬å’Œæ–‡æœ¬å¯¹ï¼Œå…¶ä¸­ç‰¹æ®Šåˆ†ç±»æ ‡è®°â€œ<cls>â€ç”¨äºåºåˆ—åˆ†ç±»ï¼Œè€Œç‰¹æ®Šåˆ†ç±»æ ‡è®°â€œ<sep>â€æ ‡è®°å•ä¸ªæ–‡æœ¬çš„ç»“æŸæˆ–åˆ†éš”æˆå¯¹æ–‡æœ¬ã€‚å¦‚ <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html#fig-bert-one-seq">å›¾15.6.1</a>æ‰€ç¤ºï¼Œåœ¨å•æ–‡æœ¬åˆ†ç±»åº”ç”¨ä¸­ï¼Œç‰¹æ®Šåˆ†ç±»æ ‡è®°â€œ<cls>â€çš„BERTè¡¨ç¤ºå¯¹æ•´ä¸ªè¾“å…¥æ–‡æœ¬åºåˆ—çš„ä¿¡æ¯è¿›è¡Œç¼–ç ã€‚ä½œä¸ºè¾“å…¥å•ä¸ªæ–‡æœ¬çš„è¡¨ç¤ºï¼Œå®ƒå°†è¢«é€å…¥åˆ°ç”±å…¨è¿æ¥ï¼ˆç¨ å¯†ï¼‰å±‚ç»„æˆçš„å°å¤šå±‚æ„ŸçŸ¥æœºä¸­ï¼Œä»¥è¾“å‡ºæ‰€æœ‰ç¦»æ•£æ ‡ç­¾å€¼çš„åˆ†å¸ƒã€‚</cls></sep></cls></p>
</li>
</ul>
<h2 id="æ–‡æœ¬å¯¹åˆ†ç±»æˆ–å›å½’"><a href="#æ–‡æœ¬å¯¹åˆ†ç±»æˆ–å›å½’" class="headerlink" title="æ–‡æœ¬å¯¹åˆ†ç±»æˆ–å›å½’"></a>æ–‡æœ¬å¯¹åˆ†ç±»æˆ–å›å½’</h2><p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308181607510.svg" srcset="/img/loading.gif" lazyload alt="../_images/bert-two-seqs.svg"></p>
<ul>
<li><p>æ–‡æœ¬å¯¹åˆ†ç±»æˆ–å›å½’åº”ç”¨çš„BERTå¾®è°ƒï¼Œå¦‚è‡ªç„¶è¯­è¨€æ¨æ–­å’Œè¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼æ€§ï¼ˆå‡è®¾è¾“å…¥æ–‡æœ¬å¯¹åˆ†åˆ«æœ‰ä¸¤ä¸ªè¯å…ƒå’Œä¸‰ä¸ªè¯å…ƒï¼‰</p>
</li>
<li><p>ä»¥ä¸€å¯¹æ–‡æœ¬ä½œä¸ºè¾“å…¥ä½†è¾“å‡ºè¿ç»­å€¼ï¼Œ<em>è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼åº¦</em>æ˜¯ä¸€ä¸ªæµè¡Œçš„â€œæ–‡æœ¬å¯¹å›å½’â€ä»»åŠ¡ã€‚ è¿™é¡¹ä»»åŠ¡è¯„ä¼°å¥å­çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚ä¾‹å¦‚ï¼Œåœ¨è¯­ä¹‰æ–‡æœ¬ç›¸ä¼¼åº¦åŸºå‡†æ•°æ®é›†ï¼ˆSemantic Textual Similarity Benchmarkï¼‰ä¸­ï¼Œå¥å­å¯¹çš„ç›¸ä¼¼åº¦å¾—åˆ†æ˜¯ä»0ï¼ˆæ— è¯­ä¹‰é‡å ï¼‰åˆ°5ï¼ˆè¯­ä¹‰ç­‰ä»·ï¼‰çš„åˆ†æ•°åŒºé—´ (<a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id21">Cer <em>et al.</em>, 2017</a>)ã€‚</p>
</li>
</ul>
<h2 id="æ–‡æœ¬æ ‡æ³¨"><a href="#æ–‡æœ¬æ ‡æ³¨" class="headerlink" title="æ–‡æœ¬æ ‡æ³¨"></a>æ–‡æœ¬æ ‡æ³¨</h2><p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308181621815.svg" srcset="/img/loading.gif" lazyload alt="../_images/bert-tagging.svg"></p>
<ul>
<li>æ–‡æœ¬æ ‡è®°åº”ç”¨çš„BERTå¾®è°ƒï¼Œå¦‚è¯æ€§æ ‡è®°ã€‚å‡è®¾è¾“å…¥çš„å•ä¸ªæ–‡æœ¬æœ‰å…­ä¸ªè¯å…ƒã€‚</li>
<li>è¯å…ƒçº§ä»»åŠ¡ï¼Œæ¯”å¦‚<em>æ–‡æœ¬æ ‡æ³¨</em>ï¼ˆtext taggingï¼‰ï¼Œå…¶ä¸­æ¯ä¸ªè¯å…ƒéƒ½è¢«åˆ†é…äº†ä¸€ä¸ªæ ‡ç­¾ã€‚åœ¨æ–‡æœ¬æ ‡æ³¨ä»»åŠ¡ä¸­ï¼Œ<em>è¯æ€§æ ‡æ³¨</em>ä¸ºæ¯ä¸ªå•è¯åˆ†é…è¯æ€§æ ‡è®°ï¼ˆä¾‹å¦‚ï¼Œå½¢å®¹è¯å’Œé™å®šè¯ï¼‰ã€‚</li>
</ul>
<h2 id="é—®ç­”"><a href="#é—®ç­”" class="headerlink" title="é—®ç­”"></a>é—®ç­”</h2><p><img src="https://cdn.jsdelivr.net/gh/alexanderliu-creator/blog_img/img/202308181628212.svg" srcset="/img/loading.gif" lazyload alt="../_images/bert-qa.svg"></p>
<ul>
<li>ä¸ºäº†å¾®è°ƒBERTè¿›è¡Œé—®ç­”ï¼Œåœ¨BERTçš„è¾“å…¥ä¸­ï¼Œå°†é—®é¢˜å’Œæ®µè½åˆ†åˆ«ä½œä¸ºç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªæ–‡æœ¬åºåˆ—ã€‚ä¸ºäº†é¢„æµ‹æ–‡æœ¬ç‰‡æ®µå¼€å§‹çš„ä½ç½®ï¼Œç›¸åŒçš„é¢å¤–çš„å…¨è¿æ¥å±‚å°†æŠŠæ¥è‡ªä½ç½®içš„ä»»ä½•è¯å…ƒçš„BERTè¡¨ç¤ºè½¬æ¢æˆæ ‡é‡åˆ†æ•°$s_i$ã€‚æ–‡ç« ä¸­æ‰€æœ‰è¯å…ƒçš„åˆ†æ•°è¿˜é€šè¿‡softmaxè½¬æ¢æˆæ¦‚ç‡åˆ†å¸ƒï¼Œä»è€Œä¸ºæ–‡ç« ä¸­çš„æ¯ä¸ªè¯å…ƒä½ç½®iåˆ†é…ä½œä¸ºæ–‡æœ¬ç‰‡æ®µå¼€å§‹çš„æ¦‚ç‡$p_i$ã€‚é¢„æµ‹æ–‡æœ¬ç‰‡æ®µçš„ç»“æŸä¸ä¸Šé¢ç›¸åŒï¼Œåªæ˜¯å…¶é¢å¤–çš„å…¨è¿æ¥å±‚ä¸­çš„å‚æ•°ä¸ç”¨äºé¢„æµ‹å¼€å§‹ä½ç½®çš„å‚æ•°æ— å…³ã€‚å½“é¢„æµ‹ç»“æŸæ—¶ï¼Œä½ç½®içš„è¯å…ƒç”±ç›¸åŒçš„å…¨è¿æ¥å±‚å˜æ¢æˆæ ‡é‡åˆ†æ•°$e_i$ã€‚ <a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html#fig-bert-qa">å›¾15.6.4</a>æè¿°äº†ç”¨äºé—®ç­”çš„å¾®è°ƒBERTã€‚</li>
<li>å¯¹äºé—®ç­”ï¼Œç›‘ç£å­¦ä¹ çš„è®­ç»ƒç›®æ ‡å°±åƒæœ€å¤§åŒ–çœŸå®å€¼çš„å¼€å§‹å’Œç»“æŸä½ç½®çš„å¯¹æ•°ä¼¼ç„¶ä¸€æ ·ç®€å•ã€‚å½“é¢„æµ‹ç‰‡æ®µæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ä»ä½ç½®iåˆ°ä½ç½®jçš„æœ‰æ•ˆç‰‡æ®µçš„åˆ†æ•°$s_i + e_j(i \leq j)$ï¼Œå¹¶è¾“å‡ºåˆ†æ•°æœ€é«˜çš„è·¨åº¦ã€‚</li>
</ul>
<h2 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li>ä¸‹æ¸¸ä»»åŠ¡ä¸åŒï¼Œä½¿ç”¨Bertå¾®è°ƒæ—¶ï¼Œåªéœ€è¦å¢åŠ è¾“å‡ºå±‚ã€‚</li>
<li>æ ¹æ®ä»»åŠ¡çš„ä¸åŒï¼Œè¾“å…¥çš„è¡¨ç¤ºå’Œä½¿ç”¨çš„Bertç‰¹å¾ä¹Ÿä¼šä¸ä¸€æ ·ã€‚</li>
</ul>
<h2 id="è‡ªç„¶è¯­è¨€æ¨ç†æ•°æ®é›†"><a href="#è‡ªç„¶è¯­è¨€æ¨ç†æ•°æ®é›†" class="headerlink" title="è‡ªç„¶è¯­è¨€æ¨ç†æ•°æ®é›†"></a>è‡ªç„¶è¯­è¨€æ¨ç†æ•°æ®é›†</h2><ul>
<li>Dependencies</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>d2l.DATA_HUB[<span class="hljs-string">'SNLI'</span>] = (<br>    <span class="hljs-string">'https://nlp.stanford.edu/projects/snli/snli_1.0.zip'</span>,<br>    <span class="hljs-string">'9fcde07509c7e87ec61c640c1b2753d9041758e4'</span>)<br><br>data_dir = d2l.download_extract(<span class="hljs-string">'SNLI'</span>)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>Reading the Dataset</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_snli</span>(<span class="hljs-params">data_dir, is_train</span>):<br>    <span class="hljs-string">"""Read the SNLI dataset into premises, hypotheses, and labels."""</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">extract_text</span>(<span class="hljs-params">s</span>):<br>        s = re.sub(<span class="hljs-string">'\\('</span>, <span class="hljs-string">''</span>, s)<br>        s = re.sub(<span class="hljs-string">'\\)'</span>, <span class="hljs-string">''</span>, s)<br>        s = re.sub(<span class="hljs-string">'\\s{2,}'</span>, <span class="hljs-string">' '</span>, s)<br>        <span class="hljs-keyword">return</span> s.strip()<br>    label_set = {<span class="hljs-string">'entailment'</span>: <span class="hljs-number">0</span>, <span class="hljs-string">'contradiction'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'neutral'</span>: <span class="hljs-number">2</span>}<br>    file_name = os.path.join(data_dir, <span class="hljs-string">'snli_1.0_train.txt'</span><br>                             <span class="hljs-keyword">if</span> is_train <span class="hljs-keyword">else</span> <span class="hljs-string">'snli_1.0_test.txt'</span>)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_name, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:<br>        rows = [row.split(<span class="hljs-string">'\t'</span>) <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> f.readlines()[<span class="hljs-number">1</span>:]]<br>    premises = [extract_text(row[<span class="hljs-number">1</span>]) <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> rows <span class="hljs-keyword">if</span> row[<span class="hljs-number">0</span>] <span class="hljs-keyword">in</span> label_set]<br>    hypotheses = [extract_text(row[<span class="hljs-number">2</span>]) <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> rows <span class="hljs-keyword">if</span> row[<span class="hljs-number">0</span>] <span class="hljs-keyword">in</span> label_set]<br>    labels = [label_set[row[<span class="hljs-number">0</span>]] <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> rows <span class="hljs-keyword">if</span> row[<span class="hljs-number">0</span>] <span class="hljs-keyword">in</span> label_set]<br>    <span class="hljs-keyword">return</span> premises, hypotheses, labels<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>Print the first 3 pairs</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">train_data = read_snli(data_dir, is_train=<span class="hljs-literal">True</span>)<br><span class="hljs-keyword">for</span> x0, x1, y <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(train_data[<span class="hljs-number">0</span>][:<span class="hljs-number">3</span>], train_data[<span class="hljs-number">1</span>][:<span class="hljs-number">3</span>], train_data[<span class="hljs-number">2</span>][:<span class="hljs-number">3</span>]):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">'premise:'</span>, x0)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">'hypothesis:'</span>, x1)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">'label:'</span>, y)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>Labels â€œentailmentâ€, â€œcontradictionâ€, and â€œneutralâ€ are balanced</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">test_data = read_snli(data_dir, is_train=<span class="hljs-literal">False</span>)<br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> [train_data, test_data]:<br>    <span class="hljs-built_in">print</span>([[row <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> data[<span class="hljs-number">2</span>]].count(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)])<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>Defining a Class for Loading the Dataset</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SNLIDataset</span>(torch.utils.data.Dataset):<br>    <span class="hljs-string">"""A customized dataset to load the SNLI dataset."""</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dataset, num_steps, vocab=<span class="hljs-literal">None</span></span>):<br>        self.num_steps = num_steps<br>        all_premise_tokens = d2l.tokenize(dataset[<span class="hljs-number">0</span>])<br>        all_hypothesis_tokens = d2l.tokenize(dataset[<span class="hljs-number">1</span>])<br>        <span class="hljs-keyword">if</span> vocab <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            self.vocab = d2l.Vocab(all_premise_tokens + all_hypothesis_tokens,<br>                                   min_freq=<span class="hljs-number">5</span>, reserved_tokens=[<span class="hljs-string">'&lt;pad&gt;'</span>])<br>        <span class="hljs-keyword">else</span>:<br>            self.vocab = vocab<br>        self.premises = self._pad(all_premise_tokens)<br>        self.hypotheses = self._pad(all_hypothesis_tokens)<br>        self.labels = torch.tensor(dataset[<span class="hljs-number">2</span>])<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">'read '</span> + <span class="hljs-built_in">str</span>(<span class="hljs-built_in">len</span>(self.premises)) + <span class="hljs-string">' examples'</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_pad</span>(<span class="hljs-params">self, lines</span>):<br>        <span class="hljs-keyword">return</span> torch.tensor([d2l.truncate_pad(<br>            self.vocab[line], self.num_steps, self.vocab[<span class="hljs-string">'&lt;pad&gt;'</span>])<br>                         <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> (self.premises[idx], self.hypotheses[idx]), self.labels[idx]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.premises)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>Putting All Things Together</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data_snli</span>(<span class="hljs-params">batch_size, num_steps=<span class="hljs-number">50</span></span>):<br>    <span class="hljs-string">"""Download the SNLI dataset and return data iterators and vocabulary."""</span><br>    num_workers = d2l.get_dataloader_workers()<br>    data_dir = d2l.download_extract(<span class="hljs-string">'SNLI'</span>)<br>    train_data = read_snli(data_dir, <span class="hljs-literal">True</span>)<br>    test_data = read_snli(data_dir, <span class="hljs-literal">False</span>)<br>    train_set = SNLIDataset(train_data, num_steps)<br>    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)<br>    train_iter = torch.utils.data.DataLoader(train_set, batch_size,<br>                                             shuffle=<span class="hljs-literal">True</span>,<br>                                             num_workers=num_workers)<br>    test_iter = torch.utils.data.DataLoader(test_set, batch_size,<br>                                            shuffle=<span class="hljs-literal">False</span>,<br>                                            num_workers=num_workers)<br>    <span class="hljs-keyword">return</span> train_iter, test_iter, train_set.vocab<br><br>train_iter, test_iter, vocab = load_data_snli(<span class="hljs-number">128</span>, <span class="hljs-number">50</span>)<br><span class="hljs-built_in">len</span>(vocab)<br></code></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> X, Y <span class="hljs-keyword">in</span> train_iter:<br>    <span class="hljs-built_in">print</span>(X[<span class="hljs-number">0</span>].shape)<br>    <span class="hljs-built_in">print</span>(X[<span class="hljs-number">1</span>].shape)<br>    <span class="hljs-built_in">print</span>(Y.shape)<br>    <span class="hljs-keyword">break</span><br></code></pre></td></tr></tbody></table></figure>



<h2 id="Bertå¾®è°ƒä»£ç "><a href="#Bertå¾®è°ƒä»£ç " class="headerlink" title="Bertå¾®è°ƒä»£ç "></a>Bertå¾®è°ƒä»£ç </h2><ul>
<li>Dependencies</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> multiprocessing<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>Loading Pretrained BERT</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">d2l.DATA_HUB[<span class="hljs-string">'bert.base'</span>] = (d2l.DATA_URL + <span class="hljs-string">'bert.base.torch.zip'</span>,<br>                             <span class="hljs-string">'225d66f04cae318b841a13d32af3acc165f253ac'</span>)<br>d2l.DATA_HUB[<span class="hljs-string">'bert.small'</span>] = (d2l.DATA_URL + <span class="hljs-string">'bert.small.torch.zip'</span>,<br>                              <span class="hljs-string">'c72329e68a732bef0452e4b96a1c341c8910f81f'</span>)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>Load pretrained BERT parameters</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_pretrained_model</span>(<span class="hljs-params">pretrained_model, num_hiddens, ffn_num_hiddens,</span><br><span class="hljs-params">                          num_heads, num_layers, dropout, max_len, devices</span>):<br>    data_dir = d2l.download_extract(pretrained_model)<br>    vocab = d2l.Vocab()<br>    vocab.idx_to_token = json.load(<span class="hljs-built_in">open</span>(os.path.join(data_dir, <span class="hljs-string">'vocab.json'</span>)))<br>    vocab.token_to_idx = {token: idx <span class="hljs-keyword">for</span> idx, token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<br>        vocab.idx_to_token)}<br>    bert = d2l.BERTModel(<span class="hljs-built_in">len</span>(vocab), num_hiddens, norm_shape=[<span class="hljs-number">256</span>],<br>                         ffn_num_input=<span class="hljs-number">256</span>, ffn_num_hiddens=ffn_num_hiddens,<br>                         num_heads=<span class="hljs-number">4</span>, num_layers=<span class="hljs-number">2</span>, dropout=<span class="hljs-number">0.2</span>,<br>                         max_len=max_len, key_size=<span class="hljs-number">256</span>, query_size=<span class="hljs-number">256</span>,<br>                         value_size=<span class="hljs-number">256</span>, hid_in_features=<span class="hljs-number">256</span>,<br>                         mlm_in_features=<span class="hljs-number">256</span>, nsp_in_features=<span class="hljs-number">256</span>)<br>    bert.load_state_dict(torch.load(os.path.join(data_dir,<br>                                                 <span class="hljs-string">'pretrained.params'</span>)))<br>    <span class="hljs-keyword">return</span> bert, vocab<br><br>devices = d2l.try_all_gpus()<br>bert, vocab = load_pretrained_model(<br>    <span class="hljs-string">'bert.small'</span>, num_hiddens=<span class="hljs-number">256</span>, ffn_num_hiddens=<span class="hljs-number">512</span>, num_heads=<span class="hljs-number">4</span>,<br>    num_layers=<span class="hljs-number">2</span>, dropout=<span class="hljs-number">0.1</span>, max_len=<span class="hljs-number">512</span>, devices=devices)<br></code></pre></td></tr></tbody></table></figure>

<ul>
<li>The Dataset for Fine-Tuning BERT</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SNLIBERTDataset</span>(torch.utils.data.Dataset):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dataset, max_len, vocab=<span class="hljs-literal">None</span></span>):<br>        all_premise_hypothesis_tokens = [[<br>            p_tokens, h_tokens] <span class="hljs-keyword">for</span> p_tokens, h_tokens <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<br>            *[d2l.tokenize([s.lower() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> sentences])<br>              <span class="hljs-keyword">for</span> sentences <span class="hljs-keyword">in</span> dataset[:<span class="hljs-number">2</span>]])]<br><br>        self.labels = torch.tensor(dataset[<span class="hljs-number">2</span>])<br>        self.vocab = vocab<br>        self.max_len = max_len<br>        (self.all_token_ids, self.all_segments,<br>         self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">'read '</span> + <span class="hljs-built_in">str</span>(<span class="hljs-built_in">len</span>(self.all_token_ids)) + <span class="hljs-string">' examples'</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_preprocess</span>(<span class="hljs-params">self, all_premise_hypothesis_tokens</span>):<br>        pool = multiprocessing.Pool(<span class="hljs-number">4</span>)<br>        out = pool.<span class="hljs-built_in">map</span>(self._mp_worker, all_premise_hypothesis_tokens)<br>        all_token_ids = [<br>            token_ids <span class="hljs-keyword">for</span> token_ids, segments, valid_len <span class="hljs-keyword">in</span> out]<br>        all_segments = [segments <span class="hljs-keyword">for</span> token_ids, segments, valid_len <span class="hljs-keyword">in</span> out]<br>        valid_lens = [valid_len <span class="hljs-keyword">for</span> token_ids, segments, valid_len <span class="hljs-keyword">in</span> out]<br>        <span class="hljs-keyword">return</span> (torch.tensor(all_token_ids, dtype=torch.long),<br>                torch.tensor(all_segments, dtype=torch.long),<br>                torch.tensor(valid_lens))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_mp_worker</span>(<span class="hljs-params">self, premise_hypothesis_tokens</span>):<br>        p_tokens, h_tokens = premise_hypothesis_tokens<br>        self._truncate_pair_of_tokens(p_tokens, h_tokens)<br>        tokens, segments = d2l.get_tokens_and_segments(p_tokens, h_tokens)<br>        token_ids = self.vocab[tokens] + [self.vocab[<span class="hljs-string">'&lt;pad&gt;'</span>]] \<br>                             * (self.max_len - <span class="hljs-built_in">len</span>(tokens))<br>        segments = segments + [<span class="hljs-number">0</span>] * (self.max_len - <span class="hljs-built_in">len</span>(segments))<br>        valid_len = <span class="hljs-built_in">len</span>(tokens)<br>        <span class="hljs-keyword">return</span> token_ids, segments, valid_len<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_truncate_pair_of_tokens</span>(<span class="hljs-params">self, p_tokens, h_tokens</span>):<br>        <span class="hljs-keyword">while</span> <span class="hljs-built_in">len</span>(p_tokens) + <span class="hljs-built_in">len</span>(h_tokens) &gt; self.max_len - <span class="hljs-number">3</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(p_tokens) &gt; <span class="hljs-built_in">len</span>(h_tokens):<br>                p_tokens.pop()<br>            <span class="hljs-keyword">else</span>:<br>                h_tokens.pop()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> (self.all_token_ids[idx], self.all_segments[idx],<br>                self.valid_lens[idx]), self.labels[idx]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.all_token_ids)<br><br></code></pre></td></tr></tbody></table></figure>

<ul>
<li><p>Generate training and testing examples</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size, max_len, num_workers = <span class="hljs-number">512</span>, <span class="hljs-number">128</span>, d2l.get_dataloader_workers()<br>data_dir = d2l.download_extract(<span class="hljs-string">'SNLI'</span>)<br>train_set = SNLIBERTDataset(d2l.read_snli(data_dir, <span class="hljs-literal">True</span>), max_len, vocab)<br>test_set = SNLIBERTDataset(d2l.read_snli(data_dir, <span class="hljs-literal">False</span>), max_len, vocab)<br>train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=<span class="hljs-literal">True</span>,<br>                                   num_workers=num_workers)<br>test_iter = torch.utils.data.DataLoader(test_set, batch_size,<br>                                  num_workers=num_workers)<br><br><span class="hljs-comment"># result</span><br>read <span class="hljs-number">549367</span> examples<br>read <span class="hljs-number">9824</span> examples<br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>This MLP transforms the BERT representation of the special â€œ<cls>â€ token into three outputs of natural language inference</cls></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BERTClassifier</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, bert</span>):<br>        <span class="hljs-built_in">super</span>(BERTClassifier, self).__init__()<br>        self.encoder = bert.encoder<br>        self.hidden = bert.hidden<br>        self.output = nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">3</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        tokens_X, segments_X, valid_lens_x = inputs<br>        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)<br>        <span class="hljs-keyword">return</span> self.output(self.hidden(encoded_X[:, <span class="hljs-number">0</span>, :]))<br><br>net = BERTClassifier(bert)<br><br></code></pre></td></tr></tbody></table></figure>
</li>
<li><p>The training</p>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs = <span class="hljs-number">1e-4</span>, <span class="hljs-number">5</span><br>trainer = torch.optim.Adam(net.parameters(), lr=lr)<br>loss = nn.CrossEntropyLoss(reduction=<span class="hljs-string">'none'</span>)<br>d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)<br></code></pre></td></tr></tbody></table></figure>











<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol>
<li><a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_attention-mechanisms/index.html">Attention D2L</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1v3411r78R?p=1&amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd">Self Attention &amp; Transformer æå®æ¯…</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1fL4y1z7Pi/?p=2&amp;spm_id_from=pageDriver&amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd">Self-supervised Learing BERT GPT æå®æ¯…</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Tb4y167rb?p=3&amp;spm_id_from=pageDriver&amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd">Attention Q&amp;A</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1v44y1C7Tg?p=3&amp;spm_id_from=pageDriver&amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd">Attention Seq2Seq Q&amp;A</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Kq4y1H7FL/?p=4&amp;spm_id_from=pageDriver&amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd">Transformer Q&amp;A</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1yU4y1E7Ns/?p=5&amp;spm_id_from=pageDriver&amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd">Bert Q&amp;A</a></li>
<li><a target="_blank" rel="noopener" href="https://zh-v2.d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html">å¾®è°ƒBert D2L æ•™ç¨‹</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV15L4y1v7ts/?p=4&amp;spm_id_from=pageDriver&amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd">Bertå¾®è°ƒ Q&amp;A</a></li>
</ol>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/AI/" class="category-chain-item">AI</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/">#ç ”0è‡ªå­¦</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>D2L-11-Attention Mechanisms and Transformers</div>
      <div>http://example.com/2023/08/16/d2l-11-attention-mechanisms-and-transformers/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>ä½œè€…</div>
          <div>Alexander Liu</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>å‘å¸ƒäº</div>
          <div>2023å¹´8æœˆ16æ—¥</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>è®¸å¯åè®®</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - ç½²å">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/08/18/d2l-kaggle-mu-biao-jian-ce-niu-zi-chuan-dai/" title="D2L-Kaggle-ç›®æ ‡æ£€æµ‹ï¼ˆç‰›ä»”ç©¿æˆ´ï¼‰">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">D2L-Kaggle-ç›®æ ‡æ£€æµ‹ï¼ˆç‰›ä»”ç©¿æˆ´ï¼‰</span>
                        <span class="visible-mobile">ä¸Šä¸€ç¯‡</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/08/14/d2l-10-modern-recurrent-neural-networks/" title="D2L-10-Modern Recurrent Neural Networks">
                        <span class="hidden-mobile">D2L-10-Modern Recurrent Neural Networks</span>
                        <span class="visible-mobile">ä¸‹ä¸€ç¯‡</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;ç›®å½•</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.events.registerRefreshCallback(function() {
      if ('mermaid' in window) {
        mermaid.init();
      }
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">æœç´¢</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">å…³é”®è¯</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        æ€»è®¿é—®é‡ 
        <span id="busuanzi_value_site_pv"></span>
         æ¬¡
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        æ€»è®¿å®¢æ•° 
        <span id="busuanzi_value_site_uv"></span>
         äºº
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- ä¸»é¢˜çš„å¯åŠ¨é¡¹ï¼Œå°†å®ƒä¿æŒåœ¨æœ€åº•éƒ¨ -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">åšå®¢åœ¨å…è®¸ JavaScript è¿è¡Œçš„ç¯å¢ƒä¸‹æµè§ˆæ•ˆæœæ›´ä½³</div>
  </noscript>
</body>
</html>
