<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>兔の博客</title>
  
  <subtitle>兔子的小窝</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2023-08-23T02:01:59.530Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Alexander Liu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>D2L-Paper Reading</title>
    <link href="http://example.com/2023/08/23/d2l-paper-reading/"/>
    <id>http://example.com/2023/08/23/d2l-paper-reading/</id>
    <published>2023-08-23T02:01:59.000Z</published>
    <updated>2023-08-23T02:01:59.530Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>D2L-Summary of D2L Lession</title>
    <link href="http://example.com/2023/08/20/d2l-summary-of-d2l-lession/"/>
    <id>http://example.com/2023/08/20/d2l-summary-of-d2l-lession/</id>
    <published>2023-08-20T12:12:18.000Z</published>
    <updated>2023-08-20T13:14:24.211Z</updated>
    
    
    <summary type="html">&lt;p&gt;D2L的课程结束啦，接下来是斯坦福的课程：&lt;a href=&quot;https://c.d2l.ai/stanford-cs329p/&quot;&gt;Stanford Lession&lt;/a&gt;，结尾课程链接在这里：&lt;a href=&quot;https://www.bilibili.com/video/BV1AL4y1Y7gu/?spm_id_from=333.999.0.0&amp;amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd&quot;&gt;D2L 课程总结和进阶学习&lt;/a&gt;。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>D2L-12-Optimization Algorithms</title>
    <link href="http://example.com/2023/08/18/d2l-12-optimization-algorithms/"/>
    <id>http://example.com/2023/08/18/d2l-12-optimization-algorithms/</id>
    <published>2023-08-18T12:53:14.000Z</published>
    <updated>2023-08-21T07:51:28.501Z</updated>
    
    
    <summary type="html">&lt;p&gt;对于深度学习问题，我们通常会先定义&lt;em&gt;损失函数&lt;/em&gt;。一旦我们有了损失函数，我们就可以使用优化算法来尝试最小化损失。在优化中，损失函数通常被称为优化问题的&lt;em&gt;目标函数&lt;/em&gt;。按照传统惯例，大多数优化算法都关注的是&lt;em&gt;最小化&lt;/em&gt;。如果我们需要最大化目标，那么有一个简单的解决方案：在目标函数前加负号即可。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>D2L-Kaggle-目标检测（牛仔穿戴）</title>
    <link href="http://example.com/2023/08/18/d2l-kaggle-mu-biao-jian-ce-niu-zi-chuan-dai/"/>
    <id>http://example.com/2023/08/18/d2l-kaggle-mu-biao-jian-ce-niu-zi-chuan-dai/</id>
    <published>2023-08-18T12:09:30.000Z</published>
    <updated>2023-08-18T12:50:15.397Z</updated>
    
    
      
      
        
        
    <summary type="html">&lt;p&gt;这个Competition中，我们遇到的主要问题，是数据的不平衡。数据不平衡使得某些类别的样本过少，我们需要处理这个问题。&lt;/p&gt;
&lt;h1 id=&quot;实战Kaggle比赛&quot;&gt;&lt;a href=&quot;#实战Kaggle比赛&quot; class=&quot;headerlink&quot;</summary>
        
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>D2L-11-Attention Mechanisms and Transformers</title>
    <link href="http://example.com/2023/08/16/d2l-11-attention-mechanisms-and-transformers/"/>
    <id>http://example.com/2023/08/16/d2l-11-attention-mechanisms-and-transformers/</id>
    <published>2023-08-16T09:06:57.000Z</published>
    <updated>2023-08-18T11:31:35.036Z</updated>
    
    
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1v3411r78R?p=1&amp;amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd&quot;&gt;Self Attention &amp;amp; Transformer 李宏毅&lt;/a&gt;，&lt;a href=&quot;https://www.bilibili.com/video/BV1fL4y1z7Pi/?p=2&amp;amp;spm_id_from=pageDriver&amp;amp;vd_source=ff957cd8fbaeb55d52afc75fbcc87dfd&quot;&gt;Self-supervised Learing BERT GPT 李宏毅&lt;/a&gt;。先看这些！这个对于原理讲的非常清楚，了解了这个，再去学李沐的课程，就十分清晰了昂！！！&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>D2L-10-Modern Recurrent Neural Networks</title>
    <link href="http://example.com/2023/08/14/d2l-10-modern-recurrent-neural-networks/"/>
    <id>http://example.com/2023/08/14/d2l-10-modern-recurrent-neural-networks/</id>
    <published>2023-08-14T12:31:54.000Z</published>
    <updated>2023-08-15T13:57:49.432Z</updated>
    
    
    <summary type="html">&lt;p&gt;循环神经网络在实践中一个常见问题是数值不稳定性。 尽管我们已经应用了梯度裁剪等技巧来缓解这个问题， 但是仍需要通过设计更复杂的序列模型来进一步处理它。 具体来说，我们将引入两个广泛使用的网络， 即&lt;em&gt;门控循环单元&lt;/em&gt;（gated recurrent units，GRU）和 &lt;em&gt;长短期记忆网络&lt;/em&gt;（long short-term memory，LSTM）。 然后，我们将基于一个单向隐藏层来扩展循环神经网络架构。 我们将描述具有多个隐藏层的深层架构， 并讨论基于前向和后向循环计算的双向设计。 现代循环网络经常采用这种扩展。 在解释这些循环神经网络的变体时， 我们将继续考虑 &lt;a href=&quot;https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/index.html#chap-rnn&quot;&gt;8节&lt;/a&gt;中的语言建模问题。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>D2L-Kaggle-狗的品种识别(ImageNet Dogs)</title>
    <link href="http://example.com/2023/08/10/d2l-kaggle-gou-de-pin-chong-shi-bie-imagenet-dogs/"/>
    <id>http://example.com/2023/08/10/d2l-kaggle-gou-de-pin-chong-shi-bie-imagenet-dogs/</id>
    <published>2023-08-10T09:02:13.000Z</published>
    <updated>2023-08-13T08:22:29.486Z</updated>
    
    
      
      
        
        
    <summary type="html">&lt;h1 id=&quot;实战Kaggle比赛：狗的品种识别-ImageNet-Dogs&quot;&gt;&lt;a href=&quot;#实战Kaggle比赛：狗的品种识别-ImageNet-Dogs&quot; class=&quot;headerlink&quot; title=&quot;实战Kaggle比赛：狗的品种识别(ImageNet</summary>
        
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>D2L-Kaggle-图像分类 (CIFAR-10)</title>
    <link href="http://example.com/2023/08/10/d2l-kaggle-tu-xiang-fen-lei-cifar-10/"/>
    <id>http://example.com/2023/08/10/d2l-kaggle-tu-xiang-fen-lei-cifar-10/</id>
    <published>2023-08-10T08:17:40.000Z</published>
    <updated>2023-08-13T08:22:54.972Z</updated>
    
    
      
      
        
        
    <summary type="html">&lt;h1 id=&quot;实战Kaggle比赛：图像分类-CIFAR-10&quot;&gt;&lt;a href=&quot;#实战Kaggle比赛：图像分类-CIFAR-10&quot; class=&quot;headerlink&quot; title=&quot;实战Kaggle比赛：图像分类</summary>
        
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>D2L-Kaggle-树叶分类</title>
    <link href="http://example.com/2023/08/10/d2l-kaggle-shu-xie-fen-lei/"/>
    <id>http://example.com/2023/08/10/d2l-kaggle-shu-xie-fen-lei/</id>
    <published>2023-08-10T04:05:10.000Z</published>
    <updated>2023-08-12T01:41:20.360Z</updated>
    
    
      
      
        
        
    <summary type="html">&lt;h1 id=&quot;实战Kaggle比赛：树叶分类&quot;&gt;&lt;a href=&quot;#实战Kaggle比赛：树叶分类&quot; class=&quot;headerlink&quot; title=&quot;实战Kaggle比赛：树叶分类&quot;&gt;&lt;/a&gt;实战Kaggle比赛：树叶分类&lt;/h1&gt;&lt;h2 id=&quot;竞赛总结&quot;&gt;&lt;a</summary>
        
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>D2L-Kaggle-房价预测</title>
    <link href="http://example.com/2023/08/10/d2l-kaggle-fang-jie-yu-ce/"/>
    <id>http://example.com/2023/08/10/d2l-kaggle-fang-jie-yu-ce/</id>
    <published>2023-08-10T04:02:49.000Z</published>
    <updated>2023-08-13T08:23:49.368Z</updated>
    
    
      
      
        
        
    <summary type="html">&lt;h1 id=&quot;实战Kaggle比赛：预测房价&quot;&gt;&lt;a href=&quot;#实战Kaggle比赛：预测房价&quot; class=&quot;headerlink&quot;</summary>
        
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>D2L-8-Computer Vision</title>
    <link href="http://example.com/2023/08/07/d2l-8-computer-vision/"/>
    <id>http://example.com/2023/08/07/d2l-8-computer-vision/</id>
    <published>2023-08-07T14:13:39.000Z</published>
    <updated>2023-08-13T08:09:59.923Z</updated>
    
    
    <summary type="html">&lt;p&gt;这一章节本质上是几个章节的融合：&lt;a href=&quot;https://zh-v2.d2l.ai/chapter_computer-vision/index.html&quot;&gt;计算机视觉章节&lt;/a&gt;和&lt;a href=&quot;https://zh-v2.d2l.ai/chapter_computational-performance/index.html&quot;&gt;计算性能章节&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>D2L-9-Recurrent Neural Networks</title>
    <link href="http://example.com/2023/08/07/d2l-9-recurrent-neural-networks/"/>
    <id>http://example.com/2023/08/07/d2l-9-recurrent-neural-networks/</id>
    <published>2023-08-07T11:47:21.000Z</published>
    <updated>2023-08-14T11:43:40.928Z</updated>
    
    
    <summary type="html">&lt;p&gt;针对于有顺序的数据类型，我们需要设计特定模型。我们不仅仅可以接收一个序列作为输入，而是还可能期望继续猜测这个序列的后续。如果说卷积神经网络可以有效地处理空间信息， 那么本章的&lt;em&gt;循环神经网络&lt;/em&gt;（recurrent neural network，RNN）则可以更好地处理序列信息。 循环神经网络通过引入状态变量存储过去的信息和当前的输入，从而可以确定当前的输出。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;循环神经网络 (RNN) 是深度学习模型，通过循环&lt;/em&gt;连接捕获序列的动态，循环连接可以被视为节点网络中的循环。乍一看这似乎违反直觉。毕竟，正是神经网络的前馈性质使得计算顺序变得明确。然而，循环边缘以精确的方式定义，以确保不会出现此类歧义。循环神经网络 跨时间步长（或序列步长）&lt;em&gt;展开&lt;/em&gt;，每个步骤应用&lt;em&gt;相同的基础参数。**标准连接同步&lt;/em&gt;应用以&lt;em&gt;在同一时间步长&lt;/em&gt;将每一层的激活传播到后续层，循环连接是 &lt;em&gt;动态的&lt;/em&gt;，跨相邻时间步传递信息。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>D2L-7-Modern Convolutional Neural Networks</title>
    <link href="http://example.com/2023/08/06/d2l-7-modern-convolutional-neural-networks/"/>
    <id>http://example.com/2023/08/06/d2l-7-modern-convolutional-neural-networks/</id>
    <published>2023-08-06T02:47:43.000Z</published>
    <updated>2023-08-10T12:56:02.722Z</updated>
    
    
    <summary type="html">&lt;p&gt;网络包括：AlexNet、VGG、NiN、GoogLeNet、ResNet、DenseNet&lt;/p&gt;
&lt;p&gt;Pytorch手动计算维度太困难，英文版教材已经改为自动推导参数（Lazy）！！！可以看英文版的！！！&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>D2L-6-Convolutional Neural Networks</title>
    <link href="http://example.com/2023/08/03/d2l-6-convolutional-neural-networks/"/>
    <id>http://example.com/2023/08/03/d2l-6-convolutional-neural-networks/</id>
    <published>2023-08-03T11:47:04.000Z</published>
    <updated>2023-08-06T02:41:27.877Z</updated>
    
    
      
      
        
        
    <summary type="html">&lt;p&gt;图像数据的每个样本都由一个二维像素网格组成， 每个像素可能是一个或者多个数值，取决于是黑白还是彩色图像。 到目前为止，我们处理这类结构丰富的数据的方式还不够有效。 我们仅仅通过将图像数据展平成一维向量而忽略了每个图像的空间结构信息，再将数据送入一个全连接的多层感知机中。</summary>
        
      
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>D2L-5-Deep Learning Computing</title>
    <link href="http://example.com/2023/08/03/d2l-5-deep-learning-computing/"/>
    <id>http://example.com/2023/08/03/d2l-5-deep-learning-computing/</id>
    <published>2023-08-03T02:14:22.000Z</published>
    <updated>2023-08-03T09:31:12.614Z</updated>
    
    
    <summary type="html">&lt;p&gt;随着时间的推移，深度学习库已经演变成提供越来越粗糙的抽象。 就像半导体设计师从指定晶体管到逻辑电路再到编写代码一样， 神经网络研究人员已经从考虑单个人工神经元的行为转变为从层的角度构思网络， 通常在设计架构时考虑的是更粗糙的块（block）。本章中，我们将深入探索深度学习计算的关键组件， 即模型构建、参数访问与初始化、设计自定义层和块、将模型读写到磁盘， 以及利用GPU实现显著的加速。 &lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>D2L-4-Multilayer Perceptrons</title>
    <link href="http://example.com/2023/07/30/d2l-4-multilayer-perceptrons/"/>
    <id>http://example.com/2023/07/30/d2l-4-multilayer-perceptrons/</id>
    <published>2023-07-30T13:50:51.000Z</published>
    <updated>2023-08-10T04:04:45.380Z</updated>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;本章有很好的实践！！！最后一个大节，有精力请务必复现！！！&lt;/strong&gt; &lt;/p&gt;
&lt;p&gt;最简单的深度网络称为&lt;em&gt;多层感知机&lt;/em&gt;。多层感知机由多层神经元组成， 每一层与它的上一层相连，从中接收输入； 同时每一层也与它的下一层相连，影响当前层的神经元。 当我们训练容量较大的模型时，我们面临着&lt;em&gt;过拟合&lt;/em&gt;的风险。 因此，本章将从基本的概念介绍开始讲起，包括&lt;em&gt;过拟合&lt;/em&gt;、&lt;em&gt;欠拟合&lt;/em&gt;和模型选择。 为了解决这些问题，本章将介绍&lt;em&gt;权重衰减&lt;/em&gt;和&lt;em&gt;暂退法&lt;/em&gt;等正则化技术。 我们还将讨论数值稳定性和参数初始化相关的问题， 这些问题是成功训练深度网络的关键。 &lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>D2L-3-Linear Neural Networks</title>
    <link href="http://example.com/2023/07/25/d2l-3-linear-neural-networks/"/>
    <id>http://example.com/2023/07/25/d2l-3-linear-neural-networks/</id>
    <published>2023-07-25T09:01:02.000Z</published>
    <updated>2023-08-03T02:11:44.093Z</updated>
    
    
    <summary type="html">&lt;p&gt; 本章我们将介绍神经网络的整个训练过程， 包括：定义简单的神经网络架构、数据处理、指定损失函数和如何训练模型。 为了更容易学习，我们将从经典算法————&lt;em&gt;线性&lt;/em&gt;神经网络开始，介绍神经网络的基础知识。 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络， 这些知识将为本书其他部分中更复杂的技术奠定基础。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>D2L-2-Preliminaries</title>
    <link href="http://example.com/2023/07/23/d2l-2-preliminaries/"/>
    <id>http://example.com/2023/07/23/d2l-2-preliminaries/</id>
    <published>2023-07-23T12:08:28.000Z</published>
    <updated>2023-07-25T09:01:31.480Z</updated>
    
    
    <summary type="html">&lt;p&gt;要学习深度学习，首先需要先掌握一些基本技能。 所有机器学习方法都涉及从数据中提取信息。 因此，我们先学习一些关于数据的实用技能，包括存储、操作和预处理数据。机器学习通常需要处理大型数据集。 &lt;strong&gt;线性代数&lt;/strong&gt;为人们提供了一些用来处理表格数据的方法。 我们可以将某些数据集视为一个表，其中表的行对应样本，列对应属性。 深度学习是关于优化的学习。 对于一个带有参数的模型，我们想要找到其中能拟合数据的最好模型。 在算法的每个步骤中，决定以何种方式调整参数需要一点&lt;strong&gt;微积分&lt;/strong&gt;知识。机器学习还涉及如何做出预测：给定观察到的信息，某些未知属性可能的值是多少？ 要在不确定的情况下进行严格的推断，我们需要借用&lt;strong&gt;概率语言&lt;/strong&gt;。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>D2L-1-Introduction</title>
    <link href="http://example.com/2023/07/22/d2l-1-introduction/"/>
    <id>http://example.com/2023/07/22/d2l-1-introduction/</id>
    <published>2023-07-22T11:23:34.000Z</published>
    <updated>2023-07-23T12:05:02.034Z</updated>
    
    
    <summary type="html">&lt;p&gt;起因是研究生组内需要做一些大模型相关的工作，自己进行了一些AI相关的学习。主要采用的《动手学深度学习》，Dive Into Deep Learning，版本是第二版，所有的资源都在网上有。参考链接为：&lt;a href=&quot;https://zh-v2.d2l.ai/index.html%E3%80%82%E5%90%8E%E7%BB%AD%E5%8D%9A%E5%AE%A2%EF%BC%8C%E4%B8%BB%E8%A6%81%E8%AE%B0%E5%BD%95%E6%89%80%E6%9C%89%E7%9B%B8%E5%85%B3%E7%9A%84%E4%BB%A3%E7%A0%81%E5%92%8C%E9%85%8D%E7%BD%AE%E3%80%82%E4%B8%BA%E4%BA%86%E8%BF%90%E8%A1%8C%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%92%8CAI%E7%9B%B8%E5%85%B3%E7%9A%84%E7%8E%AF%E5%A2%83%EF%BC%8C%E7%AC%94%E8%80%85%E4%BD%BF%E7%94%A8%E7%A1%AC%E4%BB%B6%E8%AE%BE%E5%A4%87%E7%9A%84%E6%98%AFwin11%E7%9A%84%E7%B3%BB%E7%BB%9F%EF%BC%8C%E9%85%8D%E5%A4%87%E6%9C%894090%E6%98%BE%E5%8D%A1%E5%92%8C64G%E7%9A%84%E5%86%85%E5%AD%98%EF%BC%8C%E7%A1%AC%E7%9B%98%E4%B8%BA%E4%B8%A4%E4%B8%AAT%EF%BC%8C%E8%B7%91%E5%9F%BA%E6%9C%AC%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%98%AF%E6%B2%A1%E5%95%A5%E9%97%AE%E9%A2%98%E3%80%82%E6%AD%A4%E5%A4%96%E8%BF%98%E6%9C%89%E4%B8%80%E5%8F%B0%E5%86%99%E6%96%87%E6%A1%A3%E7%9A%84Mac%EF%BC%8C%E4%BD%86%E6%98%AF%E6%80%A7%E8%83%BD%E4%B8%8D%E6%98%AF%E5%BE%88%E8%A1%8C%EF%BC%8C%E4%B8%80%E9%83%A8%E5%88%86%E7%9A%84%E7%AE%80%E5%8D%95%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9%E5%92%8C%E4%BB%A3%E7%A0%81%E8%A1%A8%E7%8E%B0%EF%BC%8C%E6%98%AF%E5%9C%A8Mac%E4%B8%8A%E5%AE%8C%E6%88%90%E7%9A%84%E3%80%82&quot;&gt;https://zh-v2.d2l.ai/index.html。后续博客，主要记录所有相关的代码和配置。为了运行大模型和AI相关的环境，笔者使用硬件设备的是win11的系统，配备有4090显卡和64G的内存，硬盘为两个T，跑基本的模型是没啥问题。此外还有一台写文档的Mac，但是性能不是很行，一部分的简单学习内容和代码表现，是在Mac上完成的。&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;大模型估计只能fine tune了orz。先学点AI再说！！！由于之前用过一些pytorch，这里主要采用pytorch作为工具进行示例学习，尽量写的简洁，记录一下过程和结果就好。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://example.com/categories/AI/"/>
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>Latex学习</title>
    <link href="http://example.com/2023/07/19/latex-xue-xi/"/>
    <id>http://example.com/2023/07/19/latex-xue-xi/</id>
    <published>2023-07-19T14:46:40.000Z</published>
    <updated>2023-07-22T04:50:50.941Z</updated>
    
    
    <summary type="html">&lt;p&gt;这里是Latex相关的学习。近日进研究生组，需要使用Latex去做很多paper work，&lt;/p&gt;</summary>
    
    
    
    
    <category term="研0自学" scheme="http://example.com/tags/%E7%A0%940%E8%87%AA%E5%AD%A6/"/>
    
  </entry>
  
</feed>
